@article{Kumar2017,
abstract = {Performance of any search engine relies heavily on its Web crawler. Web crawlers are the programs that get webpages from the Web by following hyperlinks. These webpages are indexed by a search engine and can be retrieved by a user query. In the area of Web crawling, we still lack an exhaustive study that covers all crawling techniques. This study follows the guidelines of systematic literature review and applies it to the field of Web crawling. We used the standard procedure of carrying out a systematic literature review on 248 studies from a total of 1488 articles published in 12 leading journals and other premier conferences and workshops. Existing literature about the Web crawler is classified into different key subareas. Each subarea is further divided according to the techniques being used. We analyzed the distribution of various articles using multiple criteria and depicted conclusions. Various studies that use open source Web crawlers are also reported. We have highlighted future areas of research. We call for an increased awareness in various fields of the Web crawler and identify how techniques from other domains can be used for crawling the Web. Limitations and recommendations for future are also discussed.For further resources related to this article, please visit the WIREs website.},
author = {Kumar, Manish and Bhatia, Rajesh and Rattan, Dhavleesh},
doi = {10.1002/widm.1218},
file = {:S$\backslash$:/SciencePhdSorted/Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery/A survey of Web crawlers for information retrieval{\_}Kumar, Bhatia, Rattan{\_}2017.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {6},
title = {{A survey of Web crawlers for information retrieval}},
volume = {7},
year = {2017}
}
@article{Jaganathan2015,
author = {Jaganathan, P. and Karthikeyan, T.},
doi = {10.3844/jcssp.2015.120.126},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Computer Science/Highly efficient architecture for scalable focused crawling using incremental parallel web crawler{\_}Jaganathan, Karthikeyan{\_}2015.pdf:pdf},
issn = {15493636},
journal = {Journal of Computer Science},
keywords = {Focused crawler,Incremental web crawler,Load balance,Relevance,URL distribution issue},
number = {1},
pages = {120--126},
title = {{Highly efficient architecture for scalable focused crawling using incremental parallel web crawler}},
volume = {11},
year = {2015}
}
@article{Suryawanshi2015,
author = {Suryawanshi, Parigha and Patil, D V},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/An Overview of Approaches Used In Focused Crawlers{\_}Suryawanshi, Patil{\_}2015.pdf:pdf},
keywords = {crawler,focused crawler,search engine},
pages = {698--702},
title = {{An Overview of Approaches Used In Focused Crawlers}},
year = {2015}
}
@article{Kumar2016a,
author = {Kumar, Naresh},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/International Journal of Software and Web Sciences ( IJSWS ) An Improved Architecture for Parallel Focused Crawler Using Mobile Agents{\_}K.pdf:pdf},
keywords = {dynamic web pages,internet traffic,mobile crawler,parallel and focused,search engine,www},
pages = {12--15},
title = {{International Journal of Software and Web Sciences ( IJSWS ) An Improved Architecture for Parallel Focused Crawler Using Mobile Agents}},
year = {2016}
}
@misc{Grubic2017d,
abstract = {Research project SM01 (Parallel Semantic Crawler for manufacturing multilingual web...). The experiments QES15 and QES15 are performed over Sd sample subset using BF, PR, HITS and SM crawlers. The Sd includes 50 web sites with the most challenging multi-lingual content in domain of metal manufacturing business in Serbia. These two experiments have different Page Load limit (PL{\_}max) set to load 15 (QES15) and 30 pages (QES30) per domain.},
author = {Grubi{\'{c}}, Goran},
doi = {10.17632/yb4mph9wgv.1},
publisher = {Mendeley Data},
title = {{SM01: Web crawler benchmark QES15 and QES30 experiments results}},
url = {https://data.mendeley.com/datasets/yb4mph9wgv/1},
volume = {v1},
year = {2017}
}
@techreport{Grubic2017a,
abstract = {Research project SM01 (Parallel Semantic Crawler for manufacturing multilingual web...). The experiments QES15 and QES15 are performed over Sd sample subset using BF, PR, HITS and SM crawlers. The Sd includes 50 web sites with the most challenging multi-lingual content in domain of metal manufacturing business in Serbia. These two experiments have different Page Load limit (PL{\_}max) set to load 15 (QES15) and 30 pages (QES30) per domain. Please refer to the Crawl Report Content Guide to learn about files in the report archives.},
author = {Grubi{\'{c}}, Goran},
doi = {10.17632/yb4mph9wgv.1},
institution = {Mendeley Data},
title = {{SM01: Web crawler benchmark QES15 and QES30 experiments results}},
url = {http://dx.doi.org/10.17632/yb4mph9wgv.1},
year = {2017}
}
@article{Vieira2016,
author = {Vieira, Karane and Barbosa, Luciano and da Silva, Altigran Soares and Freire, Juliana and Moura, Edleno},
doi = {10.1007/s11280-015-0331-7},
file = {:S$\backslash$:/SciencePhdSorted/World Wide Web/Finding seeds to bootstrap focused crawlers{\_}Vieira et al.{\_}2016.pdf:pdf},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Focused crawling,Relevance feedback,Web crawling},
number = {3},
pages = {449--474},
title = {{Finding seeds to bootstrap focused crawlers}},
volume = {19},
year = {2016}
}
@article{Bai2014,
author = {Bai, Quan and Xiong, Gang and Zhao, Yong and He, Longtao},
doi = {10.1016/j.procs.2014.05.363},
file = {:S$\backslash$:/SciencePhd/Crawlers/1-s2.0-S1877050914005407-main.pdf:pdf},
issn = {1877-0509},
journal = {Procedia - Procedia Computer Science},
keywords = {bogus behavior,high speed network,measurement,traffic,web crawler},
pages = {1084--1091},
publisher = {Elsevier Masson SAS},
title = {{Analysis and Detection of Bogus Behavior in Web Crawler Measurement}},
url = {http://dx.doi.org/10.1016/j.procs.2014.05.363},
volume = {31},
year = {2014}
}
@article{Cho2002,
abstract = {In this paper we study how we can design an eﬀective parallel crawler. As the size of the Web grows, it becomes imperative to parallelize a crawling process, in order to ﬁnish downloading pages in a reasonable amount of time. We ﬁrst propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling. Based on this understanding, we then propose metrics to evaluate a parallel crawler, and compare the proposed architectures using 40 million pages collected from the Web. Our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture.},
author = {Cho, Junghoo and Garcia-Molina, Hector},
doi = {10.1145/511463.511464},
file = {:S$\backslash$:/SciencePhdSorted/WWW '02 Proceedings of the 11th international conference on World Wide Web/Parallel crawlers{\_}Cho, Garcia-Molina{\_}2002.pdf:pdf},
isbn = {1581134495},
journal = {WWW '02 Proceedings of the 11th international conference on World Wide Web},
keywords = {parallelization,web crawler,web spider},
pages = {124 -- 135},
title = {{Parallel crawlers}},
url = {http://portal.acm.org/citation.cfm?doid=511446.511464{\%}5Cnhttp://ilpubs.stanford.edu:8090/505/},
year = {2002}
}
@article{Yang2005,
abstract = {The focused crawler is a topic-driven document-collecting crawler that was suggested as a promising alternative of maintaining up-to-date Web document indices in search engines. A major problem inherent in previous focused crawlers is the liability of missing highly relevant documents that are linked from off-topic documents. This problem mainly originated from the lack of consideration of structural information in a document. Traditional weighting method such as TFIDF employed in document classification can lead to this problem. In order to improve the performance of focused crawlers, this paper proposes a scheme of locality-based document segmentation to determine the relevance of a document to a specific topic. We segment a document into a set of sub-documents using contextual features around the hyperlinks. This information is used to determine whether the crawler would fetch the documents that are linked from hyperlinks in an off-topic document.},
author = {Yang, Jaeyoung and Kang, Jinbeom and Choi, Joongmin},
doi = {10.1007/11508069_13},
file = {:S$\backslash$:/SciencePhdSorted/Text/A Focused Crawler with Document Segmentation{\_}Yang, Kang, Choi{\_}2005.pdf:pdf},
isbn = {978-3-540-26972-4, 978-3-540-31693-0},
issn = {03029743},
journal = {Text},
pages = {94--101},
title = {{A Focused Crawler with Document Segmentation}},
url = {http://link.springer.com/10.1007/11508069{\_}13},
year = {2005}
}
@article{Pant2002,
abstract = {The dynamic nature of the Web highlights the scalability limitations of universal search engines. Topic driven crawlers can address the problem by distributing the crawling process across users, queries, or even client computers. The context available to a topic driven crawler allows for informed decisions about how to prioritize the links to be visited. Here we focus on the balance between a crawler's need to exploit this information to focus on the most promising links, and the need to...},
author = {Pant, Gautam and Srinivasan, Padmini and Menczer, Filippo},
file = {:S$\backslash$:/SciencePhdSorted/CEUR Workshop Proceedings/Exploration versus exploitation in topic driven crawlers{\_}Pant, Srinivasan, Menczer{\_}2002.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {August},
pages = {88--97},
title = {{Exploration versus exploitation in topic driven crawlers}},
volume = {702},
year = {2002}
}
@article{Bedi2012,
abstract = {The Web comprises of voluminous rich learning content. The volume of ever growing learning resources however leads to the problem of information overload. A large number of irrelevant search results generated from search engines based on keyword matching techniques further augment the problem. A learner in such a scenario needs semantically matched learning resources as the search results. Keeping in view the volume of content and significance of semantic knowledge, our paper proposes a multi-threaded semantic focused crawler (SFC) specially designed and implemented to crawl on the WWW for educational learning content. The proposed SFC utilizes domain ontology to expand a topic term and a set of seed URLs to initiate the crawl. The results obtained by multiple iterations of the crawl on various topics are shown and compared with the results obtained by executing an open source crawler on the similar dataset. The results are evaluated using Semantic Similarity, a vector space model based metric, and the harvest ratio.},
author = {Bedi, Punam and Thukral, Anjali and Banati, Hema and Behl, Abhishek and Mendiratta, Varun},
doi = {10.1007/s11390-012-1299-8},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Computer Science and Technology/A Multi-Threaded Semantic Focused Crawler{\_}Bedi et al.{\_}2012.pdf:pdf},
issn = {1000-9000},
journal = {Journal of Computer Science and Technology},
keywords = {a learner in such,a scenario needs semantically,a set of,a topic term and,content,domain ontology to expand,keeping in view the,learning resources as the,matched,matching techniques further augment,multi-threaded semantic focused crawler,our paper proposes a,search results,sfc,significance of semantic knowledge,specially designed and implemented,the problem,the proposed sfc utilizes,to crawl on the,volume of content and,www for educational learning},
number = {6},
pages = {1233--1242},
title = {{A Multi-Threaded Semantic Focused Crawler}},
url = {http://link.springer.com/article/10.1007/s11390-012-1299-8{\%}5Cnhttp://link.springer.com/content/pdf/10.1007/s11390-012-1299-8.pdf{\%}5Cnhttp://link.springer.com/article/10.1007/s11390-012-1299-8{\%}5Cnhttp://link.springer.com/content/pdf/10.1007/s11390-012-1299-},
volume = {27},
year = {2012}
}
@article{Luo2006,
abstract = {Focused crawlers are programs designed to selectively retrieve Web pages relevant to a specific domain for the use of domain-specific search engines. Tunneling is a heuristic-based method that solves global optimization problem. In this paper we use content block algorithm to enhance focused crawler's ability of traversing tunnel. The novel Algorithm not only avoid granularity too coarse when evaluation on the whole page but also avoid granularity too fine based on link-context. A comprehensive experiment has been conducted, the result shows obviously that this approach outperforms BestFirst and Anchor text algorithm both in harvest ratio and efficiency.},
author = {Luo, Na and Zuo, Wanli and Yuan, Fuyu and Zhang, Changli},
doi = {10.1007/11795131_92},
file = {:S$\backslash$:/SciencePhdSorted/the First international conference on Rough Sets and Knowledge Technology/A New Method for Focused Crawler Cross Tunnel{\_}Luo et al.{\_}2006.pdf:pdf},
isbn = {978-3-540-36297-5, 978-3-540-36299-9},
issn = {16113349},
journal = {the First international conference on Rough Sets and Knowledge Technology},
keywords = {anchor text,content block,focused crawler,local relevance},
pages = {632--637},
title = {{A New Method for Focused Crawler Cross Tunnel}},
url = {http://link.springer.com/10.1007/11795131{\_}92},
year = {2006}
}
@article{Deng2013,
abstract = {It is important to research and find out opinion leader in time of increased huge number of Micro Blog users around our world and daily life. Our governments in different countries around the world have found that it is a vital fact to analysis and control message spreading in Micro Blog and other social network. For the reason that opinion leaders always play vital role in information spreading in Micro Blog and it is necessary to detect opinion leaders precisely. In this article, we has constructed a SINA Micro Blog APIs based Micro Blog crawling and analysis tool. Furthermore, a novel MapReduce based Micro blog crawler and node betweenness approximation computation method was proposed better accuracy and less running time to detect core opinion leaders in Micro Blog graphs.},
author = {Deng, Xiaolong and Li, Yuxiao and Lin, Shu},
doi = {10.1016/j.aasri.2013.10.074},
file = {:S$\backslash$:/SciencePhdSorted/AASRI Procedia/Parallel Micro Blog Crawler Construction for Effective Opinion Leader Approximation{\_}Deng, Li, Lin{\_}2013.pdf:pdf},
issn = {22126716},
journal = {AASRI Procedia},
pages = {170--176},
publisher = {Elsevier B.V.},
title = {{Parallel Micro Blog Crawler Construction for Effective Opinion Leader Approximation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212671613000759},
volume = {5},
year = {2013}
}
@article{Sey2016,
author = {Sey, Ali and Patel, Ahmed},
doi = {10.1016/j.csi.2015.07.001},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Computer Standards {\&} Interfaces A focused crawler combinatory link and content model based on T-Graph principles{\_}Sey, Patel{\_}2016.pdf:pdf;:S$\backslash$:/SciencePhd/Crawlers/boozeee.pdf:pdf},
pages = {1--11},
title = {{Computer Standards {\&} Interfaces A focused crawler combinatory link and content model based on T-Graph principles}},
volume = {43},
year = {2016}
}
@article{Nath2012,
abstract = {World Wide Web is a collection of hyperlinked documents available in HTML format. Due to the growing and dynamic nature of the web, it has become a challenge to traverse all the URLs available in the web documents. The list of URL is very huge and so it is difficult to refresh it quickly as 40{\%} of web pages change daily. Due to which more of the network resources specifically bandwidth are consumed by the web crawlers to keep the repository up to date. So, this paper proposes Parallel Domain Focused Crawler that searches and retrieves the Web pages from the Web, which are related to a specific domain only and skips irrelevant domains. It makes use of change in frequency of webpage change to fetch a web page from the web. It downloads only those pages that are changed and ignores those pages that are not modified since the last crawl. Experimentally, it has been found that the proposed crawler reduces the load on the network up to 40{\%}, which is considerable.},
author = {Nath, Rajender and Kumar, Naresh},
file = {:S$\backslash$:/SciencePhdSorted/International Journal Of Computational Engineering Research (ijceronline.com)/A Novel Parallel Domain Focused Crawler for Reduction in Load on the Network{\_}Nath, Kumar{\_}2012.pdf:pdf},
journal = {International Journal Of Computational Engineering Research (ijceronline.com)},
keywords = {Internet traffic,WWW,mobile crawler,network resources,search engine,web change behavior},
number = {7},
pages = {2250--3005},
title = {{A Novel Parallel Domain Focused Crawler for Reduction in Load on the Network}},
volume = {2},
year = {2012}
}
@article{Sey2016a,
author = {Sey, Ali and Patel, Ahmed and Celestino, Joaquim},
doi = {10.1016/j.csi.2015.09.007},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Computer Standards {\&} Interfaces Empirical evaluation of the link and content-based focused Treasure-Crawler{\_}Sey, Patel, Celestino{\_}2016.pdf:pdf},
pages = {54--62},
title = {{Computer Standards {\&} Interfaces Empirical evaluation of the link and content-based focused Treasure-Crawler}},
volume = {44},
year = {2016}
}
@article{Stevanovic2012,
author = {Stevanovic, Dusan and An, Aijun and Vlajic, Natalija},
doi = {10.1016/j.eswa.2012.01.210},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems With Applications/Expert Systems with Applications Feature evaluation for web crawler detection with data mining techniques{\_}Stevanovic, An, Vlajic{\_}2012.pdf:pdf},
issn = {0957-4174},
journal = {Expert Systems With Applications},
keywords = {web crawler detection,web server access logs},
number = {10},
pages = {8707--8717},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications Feature evaluation for web crawler detection with data mining techniques}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.01.210},
volume = {39},
year = {2012}
}
@article{Srinivasan2004,
author = {Srinivasan, P and Menczer, F and A, G Pant.},
file = {:S$\backslash$:/SciencePhdSorted/In Information Retrieval, Submitted, Accessed May/General Evaluation Framework for Topical Web Crawlers{\_}Srinivasan, Menczer, A{\_}2004.pdf:pdf;::},
journal = {In Information Retrieval, Submitted, Accessed May},
keywords = {efficiency,evaluation,precision,recall,tasks,topics,web crawlers},
pages = {417--447},
title = {{General Evaluation Framework for Topical Web Crawlers}},
year = {2004}
}
@article{Ferreira2013,
author = {Ferreira, Rafael and Freitas, Fred and Brito, Patrick and Melo, Jean and Lima, Rinaldo and Costa, Evandro},
doi = {10.1016/j.eswa.2012.08.020},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems With Applications/Expert Systems with Applications RetriBlog An architecture-centered framework for developing blog crawlers{\_}Ferreira et al.{\_}2013.pdf:pdf},
issn = {0957-4174},
journal = {Expert Systems With Applications},
number = {4},
pages = {1177--1195},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications RetriBlog : An architecture-centered framework for developing blog crawlers}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.08.020},
volume = {40},
year = {2013}
}
@article{Li2013,
abstract = {A key problem of retrieving, integrating and mining rich and high quality information from massive Deep Web Databases (WDBs) online is how to automatically and effectively discover and recognize domain-specific WDBs' entry points, i.e., forms, in the Web. It has been a challenging task because domain-specific WDBs' forms with dynamic and heterogeneous properties are very sparsely distributed over several trillion Web pages. Although significant efforts have been made to address the problem and its special cases, more effective solutions remain to be further explored towards achieving both the satisfactory harvest rate and coverage rate of domain-specific WDBs' forms simultaneously. In this paper, an Enhanced FormFocused Crawler for domain-specific WDBs (E-FFC) has been proposed as a novel framework to address existing solutions' limitations. The E-FFC, based on the divide and conquer strategy, employs a series of novel and effective strategies/algorithms, including a two-step page classifier, a link scoring strategy, classifiers for advanced searchable and domain-specific forms, crawling stopping criteria, etc. to its end achieving the optimized harvest rate and coverage rate of domain-specific WDBs' forms simultaneously. Experiments of the E-FFC over a number of real Web pages in a set of representative domains have been conducted and the results show that the E-FFC outperforms the existing domain-specific Deep Web Form-Focused Crawlers in terms of the harvest rate, coverage rate and crawling robustness.},
author = {Li, Yanni and Wang, Yuping and Du, Jintao},
doi = {10.1007/s10844-012-0221-8},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Intelligent Information Systems/E-FFC An enhanced form-focused crawler for domain-specific deep web databases{\_}Li, Wang, Du{\_}2013.pdf:pdf},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Coverage rate,Deep Web Databases (WDBs),Form-Focused Crawler (FFC),Harvest rate},
number = {1},
pages = {159--184},
title = {{E-FFC: An enhanced form-focused crawler for domain-specific deep web databases}},
volume = {40},
year = {2013}
}
@article{Dikaiakos2005,
abstract = {In this paper, we present a characterization study of search-engine crawlers. For the purposes of our work, we use Web-server access logs from five academic sites in three different countries. Based on these logs, we analyze the activity of different crawlers that belong to five search engines: Google, AltaVista, Inktomi, FastSearch and CiteSeer. We compare crawler behavior to the characteristics of the general World-Wide Web traffic and to general characterization studies. We analyze crawler requests to derive insights into the behavior and strategy of crawlers. We propose a set of simple metrics that describe qualitative characteristics of crawler behavior, vis-??-vis a crawler's preference on resources of a particular format, its frequency of visits on a Web site, and the pervasiveness of its visits to a particular site. To the best of our knowledge, this is the first extensive and in depth characterization of search-engine crawlers. Our results and observations provide useful insights into crawler behavior and serve as basis of our ongoing work on the automatic detection of Web crawlers. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Dikaiakos, Marios D. and Stassopoulou, Athena and Papageorgiou, Loizos},
doi = {10.1016/j.comcom.2005.01.003},
file = {:S$\backslash$:/SciencePhdSorted/Computer Communications/An investigation of web crawler behavior Characterization and metrics{\_}Dikaiakos, Stassopoulou, Papageorgiou{\_}2005.pdf:pdf},
issn = {01403664},
journal = {Computer Communications},
keywords = {Crawlers,Web characterization},
number = {8},
pages = {880--897},
title = {{An investigation of web crawler behavior: Characterization and metrics}},
volume = {28},
year = {2005}
}
@article{ALQARALEH2015,
author = {ALQARALEH, Saed and RAMADAN, Omar and SALAMAH, Muhammed},
doi = {10.1108/AJIM-02-2015-0019},
file = {:S$\backslash$:/SciencePhdSorted/Aslib Journal of Information Management/Efficient watcher based web crawler design{\_}ALQARALEH, RAMADAN, SALAMAH{\_}2015.pdf:pdf},
isbn = {0520150082},
issn = {2050-3806},
journal = {Aslib Journal of Information Management},
number = {6},
pages = {663--686},
title = {{Efficient watcher based web crawler design}},
url = {http://www.emeraldinsight.com/doi/10.1108/AJIM-02-2015-0019},
volume = {67},
year = {2015}
}
@article{Zhang2010,
author = {Zhang, Huaxiang and Lu, Jing},
doi = {10.1016/j.asoc.2009.08.017},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/SCTWC An online semi-supervised clustering approach to topical web crawlers{\_}Zhang, Lu{\_}2010.pdf:pdf},
keywords = {evolutionary programming},
pages = {490--495},
title = {{SCTWC : An online semi-supervised clustering approach to topical web crawlers}},
volume = {10},
year = {2010}
}
@article{Siddesh2017,
author = {Siddesh, G. M. and Suresh, Kavya and Madhuri, K. Y. and Nijagal, Madhushree and Rakshitha, B. R. and Srinivasa, K. G.},
doi = {10.1007/s40031-016-0267-z},
file = {:S$\backslash$:/SciencePhdSorted/Journal of The Institution of Engineers (India) Series B/Optimizing Crawler4j using MapReduce Programming Model{\_}Siddesh et al.{\_}2017.pdf:pdf},
issn = {2250-2106},
journal = {Journal of The Institution of Engineers (India): Series B},
keywords = {Web crawler,Crawler4j,Hadoop,MapReduce,Crawler4j w,hadoop,mapreduce {\'{a}} crawler4j with,web crawler {\'{a}} crawler4j,{\'{a}} hadoop {\'{a}}},
number = {3},
pages = {329--336},
publisher = {Springer India},
title = {{Optimizing Crawler4j using MapReduce Programming Model}},
url = {http://link.springer.com/10.1007/s40031-016-0267-z},
volume = {98},
year = {2017}
}
@article{Du2014,
author = {Du, Yajun and Hai, Yufeng and Xie, Chunzhi and Wang, Xiaoming},
doi = {10.1016/j.asoc.2013.09.007},
file = {:S$\backslash$:/SciencePhdSorted/Applied Soft Computing Journal/An approach for selecting seed URLs of focused crawler based on user-interest ontology{\_}Du et al.{\_}2014.pdf:pdf},
issn = {1568-4946},
journal = {Applied Soft Computing Journal},
number = {7224},
pages = {663--676},
publisher = {Elsevier B.V.},
title = {{An approach for selecting seed URLs of focused crawler based on user-interest ontology}},
url = {http://dx.doi.org/10.1016/j.asoc.2013.09.007},
volume = {14},
year = {2014}
}
@article{Du2015,
author = {Du, Yajun and Liu, Wenjun and Lv, Xianjing and Peng, Guoli},
doi = {10.1016/j.asoc.2015.07.026},
file = {:S$\backslash$:/SciencePhdSorted/Applied Soft Computing Journal/An improved focused crawler based on Semantic Similarity Vector Space Model{\_}Du et al.{\_}2015.pdf:pdf},
issn = {1568-4946},
journal = {Applied Soft Computing Journal},
pages = {392--407},
publisher = {Elsevier B.V.},
title = {{An improved focused crawler based on Semantic Similarity Vector Space Model}},
url = {http://dx.doi.org/10.1016/j.asoc.2015.07.026},
volume = {36},
year = {2015}
}
@article{Farag2017,
abstract = {{\textcopyright} 2017 Springer-Verlag Berlin HeidelbergThere is need for an Integrated Event Focused Crawling system to collect Web data about key events. When a disaster or other significant event occurs, many users try to locate the most up-to-date information about that event. Yet, there is little systematic collecting and archiving anywhere of event information. We propose intelligent event focused crawling for automatic event tracking and archiving, ultimately leading to effective access. We developed an event model that can capture key event information, and incorporated that model into a focused crawling algorithm. For the focused crawler to leverage the event model in predicting webpage relevance, we developed a function that measures the similarity between two event representations. We then conducted two series of experiments to evaluate our system about two recent events: California shooting and Brussels attack. The first experiment series evaluated the effectiveness of our proposed event model representation when assessing the relevance of webpages. Our event model-based representation outperformed the baseline method (topic-only); it showed better results in precision, recall, and F1-score with an improvement of 20{\%} in F1-score. The second experiment series evaluated the effectiveness of the event model-based focused crawler for collecting relevant webpages from the WWW. Our event model-based focused crawler outperformed the state-of-the-art baseline focused crawler (best-first); it showed better results in harvest ratio with an average improvement of 40{\%}.},
author = {Farag, Mohamed M G and Lee, Sunshin and Fox, Edward A.},
doi = {10.1007/s00799-016-0207-1},
file = {:S$\backslash$:/SciencePhdSorted/International Journal on Digital Libraries/Focused crawler for events{\_}Farag, Lee, Fox{\_}2017.pdf:pdf;:S$\backslash$:/SciencePhdSorted/International Journal on Digital Libraries/Focused crawler for events{\_}Farag, Lee, Fox{\_}2017(2).pdf:pdf},
isbn = {0079901602071},
issn = {14321300},
journal = {International Journal on Digital Libraries},
keywords = {Digital libraries,Event archiving,Event modeling,Focused crawling,Web archiving},
number = {December 2016},
pages = {1--17},
publisher = {Springer Berlin Heidelberg},
title = {{Focused crawler for events}},
year = {2017}
}
@article{Grubic2015,
author = {Grubi{\'{c}}, Goran},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Parallel Semantic Crawlers for Knowledge Extraction from manufacturing business multilingual web proposals and experimental evaluation{\_}.pdf:pdf},
keywords = {business intelligence,manufacturing,parallel crawler,semantic similarity,web content mining,web crawler},
title = {{Parallel Semantic Crawlers for Knowledge Extraction from manufacturing business multilingual web : proposals and experimental evaluation}},
year = {2015}
}
@article{Yadav2008,
author = {Yadav, Divakar and Sharma, A. K. and Gupta, J. P.},
file = {:S$\backslash$:/SciencePhdSorted/WSEAS Transactions on Computers/Parallel crawler architecture and web page change detection{\_}Yadav, Sharma, Gupta{\_}2008.pdf:pdf},
issn = {11092750},
journal = {WSEAS Transactions on Computers},
keywords = {Change detection,Client crawlers,Multi-threaded server,Parallel crawler,Structural and content changes},
number = {7},
pages = {929--940},
title = {{Parallel crawler architecture and web page change detection}},
volume = {7},
year = {2008}
}
@article{Yang2010,
abstract = {This paper proposed the use of ontology-supported website models to provide a semantic level solution for an information agent so that it can provide fast, precise, and stable query results. We have based on the technique to develop a focused crawler, namely, OntoCrawler which can benefit both user requests and domain semantics. The technique in this research has practically applied on Google and Yahoo searching engines to actively search for webpages of related information, and the experiment outcomes indicated that this technique could definitely up-rise precision rate and recall rate of webpage query. Equipped with this technique, we have developed an ontology-supported information agent shell in Scholar domain which manifests the following interesting features: ontology-supported construction of website models, website models-supported website model expansion, website models-supported webpage retrieval, high-level outcomes of information recommendation, and accordingly proved the feasibility of the related techniques proposed in this paper. ?? 2010 Elsevier Ltd. All rights reserved.},
author = {Yang, Sheng Yuan},
doi = {10.1007/978-3-642-13067-0_54},
file = {:S$\backslash$:/SciencePhdSorted/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/A focused crawler with ontology-supported website models for information agents{\_}Yang{\_}2010.pdf:pdf},
isbn = {3642130666},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Focused crawlers,Ontology,Website models},
number = {7},
pages = {522--532},
publisher = {Elsevier Ltd},
title = {{A focused crawler with ontology-supported website models for information agents}},
url = {http://dx.doi.org/10.1016/j.eswa.2010.01.018},
volume = {6104 LNCS},
year = {2010}
}
@article{Sirohi2013,
author = {Sirohi, Priyank and Goel, Abhinav and Singhal, Niraj},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Architecture of a Parallel Focused Crawler for Online Social Networks{\_}Sirohi, Goel, Singhal{\_}2013.pdf:pdf},
pages = {3--6},
title = {{Architecture of a Parallel Focused Crawler for Online Social Networks}},
volume = {8491},
year = {2013}
}
@article{B2015,
author = {B, Michal Blinkiewicz and Galler, Mariusz and Szwabe, Andrzej},
doi = {10.1007/978-3-319-15615-6},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Template-Driven Semantic Parsing for Focused Web Crawler{\_}B, Galler, Szwabe{\_}2015.pdf:pdf},
isbn = {9783319156156},
keywords = {expression language,parsing,template,web},
pages = {351--352},
title = {{Template-Driven Semantic Parsing for Focused Web Crawler}},
year = {2015}
}
@article{Ahmadi-abkenari2012,
author = {Ahmadi-abkenari, Fatemeh and Selamat, Ali},
doi = {10.1016/j.ins.2011.08.022},
file = {:S$\backslash$:/SciencePhdSorted/Information Sciences/An architecture for a focused trend parallel Web crawler with the application of clickstream analysis{\_}Ahmadi-abkenari, Selamat{\_}2012.pdf:pdf},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {clickstream analysis},
number = {1},
pages = {266--281},
publisher = {Elsevier Inc.},
title = {{An architecture for a focused trend parallel Web crawler with the application of clickstream analysis}},
url = {http://dx.doi.org/10.1016/j.ins.2011.08.022},
volume = {184},
year = {2012}
}
@article{Batsakis2009,
author = {Batsakis, Sotiris and Petrakis, Euripides G M and Milios, Evangelos},
doi = {10.1016/j.datak.2009.04.002},
file = {:S$\backslash$:/SciencePhdSorted/Data {\&} Knowledge Engineering/Data {\&} Knowledge Engineering Improving the performance of focused web crawlers{\_}Batsakis, Petrakis, Milios{\_}2009.pdf:pdf},
issn = {0169-023X},
journal = {Data {\&} Knowledge Engineering},
number = {10},
pages = {1001--1013},
publisher = {Elsevier B.V.},
title = {{Data {\&} Knowledge Engineering Improving the performance of focused web crawlers}},
url = {http://dx.doi.org/10.1016/j.datak.2009.04.002},
volume = {68},
year = {2009}
}
@article{Liu2014a,
abstract = {In many research works, topical priorities of unvisited hyperlinks are computed based on linearly integrating topic-relevant similarities of various texts and corresponding weighted factors. However, these weighted factors are determined based on the personal experience, so that these values may make topical priorities of unvisited hyperlinks serious deviations directly. To solve this problem, this paper proposes a novel focused crawler applying the cell-like membrane computing optimization algorithm (CMCFC). The CMCFC regards all weighted factors corresponding to contribution degrees of similarities of various texts as one object, and utilizes evolution regulars and communication regulars in membranes to achieve the optimal object corresponding to the optimal weighted factors, which make the root measure square error (RMS) of priorities of hyperlinks achieve the minimum. Then, it linearly integrates optimal weighted factors and corresponding topical similarities of various texts, which are computed by using a Vector Space Model (VSM), to compute priorities of unvisited hyperlinks. The CMCFC obtains more accurate unvisited URLs' priorities to guide crawlers to collect higher quality web pages. The experimental results indicate that the proposed method improves the performance of focused crawlers by intelligently determining weighted factors. In conclusion, the mentioned approach is effective and significant for focused crawlers. ?? 2013 Elsevier B.V.},
author = {Liu, Wen Jun and Du, Ya Jun},
doi = {10.1016/j.neucom.2013.06.039},
file = {:S$\backslash$:/SciencePhdSorted/Neurocomputing/A novel focused crawler based on cell-like membrane computing optimization algorithm{\_}Liu, Du{\_}2014.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Focused crawler,Membrane computing,Optimization algorithm,VSM},
pages = {266--280},
publisher = {Elsevier},
title = {{A novel focused crawler based on cell-like membrane computing optimization algorithm}},
url = {http://dx.doi.org/10.1016/j.neucom.2013.06.039},
volume = {123},
year = {2014}
}
@inproceedings{Papavassiliou2013,
abstract = {This paper discusses a modular and open- source focused crawler (ILSP-FC) for the automatic acquisition of domain-specific monolingual and bilingual corpora from the Web. Besides describing the main modules integrated in the crawler (dealing with page fetching, normalization, clean- ing, text classification, de-duplication and document pair detection), we evaluate sev- eral of the system functionalities in an ex- periment for the acquisition of pairs of par- allel documents in German and Italian for the "Health {\&} Safety at work" domain. 1},
address = {Sofia},
author = {Papavassiliou, Vassilis and Prokopidis, Prokopis and Thurmair, Gregor},
booktitle = {Proceedings of the 6th Workshop on Building and Using Comparable Corpora},
file = {:S$\backslash$:/SciencePhdSorted/Proceedings of the 6th Workshop on Building and Using Comparable Corpora/A modular open-source focused crawler mining monolingual and bilingual corpora from the web{\_}Papavassiliou, Prokopidis, Thurmair{\_}2013.pdf:pdf},
organization = {Association for Computational Linguistics},
pages = {43--51},
publisher = {Association for Computational Linguistics},
title = {{A modular open-source focused crawler mining monolingual and bilingual corpora from the web}},
year = {2013}
}
@misc{Grubic2017,
author = {Grubi{\'{c}}, Goran},
doi = {10.17632/b4cs4rky9s.1},
publisher = {Mendeley Data},
title = {{SM01: Research sample subsets - Parallel Semantic Crawler for manufacturing multilingual web}},
url = {http://dx.doi.org/10.17632/b4cs4rky9s.1},
volume = {v1},
year = {2017}
}
@misc{Crawler-Lib2014,
address = {Wal­ter Zirn},
author = {Crawler-Lib},
publisher = {Crawler-Lib},
title = {{NHunspell - .NET wrapper of native Hunspell libraries}},
url = {http://www.crawler-lib.net/nhunspell},
urldate = {2016-01-01},
year = {2014}
}
@article{Wang2015a,
abstract = {There's more and more precious content digitized in digital archives especially for cultural heritage. It could cost much effort in digitization and archiving. To meet the requirements in a digital archiving system, several issues must be addressed. First, it usually requires resources such as computation and storage for each individual digital archive to maintain its own service. Second, the archived content would be more useful if they can be easily utilized in providing services such as searching across multiple archives. Current approaches usually adopt metadata harvesting that would build a centralized index from separate digital libraries. They usually suffer from the problem of metadata inconsistency. In this paper, we propose a distributed indexing approach to collaborative content-based multimedia retrieval across digital archives. To reduce the loads in each archive, we dynamically distribute the tasks of crawling, indexing, and query processing depending on the response time. Distributed crawler-based approach can simplify the design of indexing and query processing steps by maintaining the data to be indexed local to the machine for crawling. It can facilitate efficient archiving and indexing by automatically following the link structure of contents published on the Web. Also, it enables simpler implementation and easier support for cross-archive applications such as search and copy detection. Experimental results show the potential of the proposed approach in load balancing with appropriate task distribution.},
author = {Wang, Jenq Haur and Chang, Hung Chi},
doi = {10.1007/s11042-013-1461-5},
file = {:S$\backslash$:/SciencePhdSorted/Multimedia Tools and Applications/CoBITs a distributed indexing approach to collaborative content-based multimedia retrieval across digital archives{\_}Wang, Chang{\_}2015.pdf:pdf},
isbn = {13807501 (ISSN)},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Collaborative repository,Content-based retrieval,Distributed archiving,Distributed crawler,Peer-to-peer computing},
number = {8},
pages = {2639--2658},
title = {{CoBITs: a distributed indexing approach to collaborative content-based multimedia retrieval across digital archives}},
volume = {74},
year = {2015}
}
@article{Cho2012,
abstract = {In this paper we study in what order a crawler should visit the URLs it has seen, in order to obtain more "important" pages first. Obtaining important pages rapidly can be very useful when a crawler cannot visit the entire Web in a reasonable amount of time. We define several importance metrics, ordering schemes, and performance evaluation measures for this problem. We also experimentally evaluate the ordering schemes on the Stanford University Web. Our results show that a crawler with a good ordering scheme can obtain important pages significantly faster than one without. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Cho, Junghoo and Garcia-Molina, Hector and Page, Lawrence},
doi = {10.1016/j.comnet.2012.10.006},
file = {:S$\backslash$:/SciencePhdSorted/Computer Networks/Reprint of Efficient crawling through URL ordering{\_}Cho, Garcia-Molina, Page{\_}2012.pdf:pdf},
issn = {13891286},
journal = {Computer Networks},
keywords = {Crawling,URL ordering},
number = {18},
pages = {3849--3858},
publisher = {Elsevier B.V.},
title = {{Reprint of: Efficient crawling through URL ordering}},
url = {http://dx.doi.org/10.1016/j.comnet.2012.10.006},
volume = {56},
year = {2012}
}
@article{Pappas2012,
abstract = {The discovery of web documents about certain topics is an important task for web-based applications including web document retrieval, opinion mining and knowledge extraction. In this paper, we propose an agent-based focused crawling framework able to retrieve topic- and genre-related web documents. Starting from a simple topic query, a set of focused crawler agents explore in parallel topic-specific web paths using dynamic seed URLs that belong to certain web genres and are collected from web search engines. The agents make use of an internal mechanism that weighs topic and genre relevance scores of unvisited web pages. They are able to adapt to the properties of a given topic by modifying their internal knowledge during search, handle ambiguous queries, ignore irrelevant pages with respect to the topic and retrieve collaboratively topic-relevant web pages. We performed an experimental study to evaluate the behavior of the agents for a variety of topic queries demonstrating the benefits and the capabilities of our framework. {\textcopyright} 2012 IEEE.},
author = {Pappas, Nikolaos and Katsimpras, Georgios and Stamatatos, Efstathios},
doi = {10.1109/ICTAI.2012.75},
file = {:S$\backslash$:/SciencePhdSorted/Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI/An agent-based focused crawling framework for topic- and genre-related web document discovery{\_}Pappas, Katsimpras, Stamatatos{\_}2012.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {focused crawling,genre-aware crawling,link analysis,utility-based agents,web document discovery},
pages = {508--515},
title = {{An agent-based focused crawling framework for topic- and genre-related web document discovery}},
volume = {1},
year = {2012}
}
@article{Elgin2015,
abstract = {Hyperlink network analysis, which utilizes the links between websites to map online communication structures, offers an emerging methodology for studying the networks of supporters and opponents of public policies. Reasons for the methodology's appeal include the ability to utilize web crawlers to collect large amounts of data and the ability to apply quantitative and qualitative methods to examine network interactions. While the methodology has been utilized by diverse disciplines it is relatively new to the fields of political science and public policy. Utilizing a mixed-methods research design, this article examines the applicability of using hyperlink networks to study opposing groups in the Colorado climate and energy policy subsystem while drawing increased attention to the methodology's strengths and weaknesses. The results demonstrate the methodology's considerable potential, but highlight the need for greater adoption of a collection of best practices designed to increase the validity, reliability, and generalizability of hyperlink research. KEY WORDS: climate change, Internet, hyperlinks, social networks, Advocacy Coalition Framework, Policy Analytical Capacity While},
author = {Elgin, Dallas J.},
doi = {10.1111/ropr.12118},
file = {:S$\backslash$:/SciencePhdSorted/Review of Policy Research/Utilizing hyperlink network analysis to examine climate change supporters and opponents{\_}Elgin{\_}2015.pdf:pdf},
issn = {15411338},
journal = {Review of Policy Research},
keywords = {Advocacy coalition framework,Climate change,Hyperlinks,Internet,Policy analytical capacity,Social networks},
number = {2},
pages = {226--245},
title = {{Utilizing hyperlink network analysis to examine climate change supporters and opponents}},
volume = {32},
year = {2015}
}
@article{Liakos2016,
abstract = {A constantly growing amount of high-quality information resides in databases and is guarded behind forms that users fill out and submit. The Hidden Web comprises all these information sources that conventional web crawlers are incapable of discovering. In order to excavate and make available meaningful data from the Hidden Web, previous work has focused on developing query generation techniques that aim at downloading all the content of a given Hidden Web site with the minimum cost. However, there are circumstances where only a specific part of such a site might be of interest. For example, a politics portal should not have to waste bandwidth or processing power to retrieve sports articles just because they are residing in databases also containing documents relevant to politics. In cases like this one, we need to make the best use of our resources in downloading only the portion of the Hidden Web site that we are interested in. We investigate how we can build a focused Hidden Web crawler that can autonomously extract topic-specific pages from the Hidden Web by searching only the subset that is related to the corresponding area. In this regard, we present an approach that progresses iteratively and analyzes the returned results in order to extract terms that capture the essence of the topic we are interested in. We propose a number of different crawling policies and we experimentally evaluate them with data from four popular sites. Our approach is able to download most of the content in search in all cases, using a significantly smaller number of queries compared to existing approaches. {\textcopyright} 2015 Springer Science+Business Media New York},
author = {Liakos, Panagiotis and Ntoulas, Alexandros and Labrinidis, Alexandros and Delis, Alex},
doi = {10.1007/s11280-015-0349-x},
file = {:S$\backslash$:/SciencePhdSorted/World Wide Web/Focused crawling for the hidden web{\_}Liakos et al.{\_}2016.pdf:pdf},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Crawling,Focused,Hidden Web,Query selection,Topic-sensitive},
number = {4},
pages = {605--631},
title = {{Focused crawling for the hidden web}},
volume = {19},
year = {2016}
}
@article{Kumar2012,
abstract = {General crawlers use a breath first search to download as many pages as possible. Focused crawler can help the search engine to index all documents present on the Web related to a specific domain which in turn provides the search engine's users complete and up-to-date contents. In this paper we present a focused crawler capable of learning. Crawling results for four consecutive crawls are shown. Results shows significant improvement in the precision value for the crawler with respect to the number of crawling attempts made.},
author = {Kumar, Mukesh and Vig, Renu},
doi = {10.1016/j.protcy.2012.10.073},
file = {:S$\backslash$:/SciencePhdSorted/Procedia Technology/Learnable Focused Meta Crawling Through Web{\_}Kumar, Vig{\_}2012.pdf:pdf},
issn = {22120173},
journal = {Procedia Technology},
keywords = {1,Focused Web Crawler,Internet,Retrieval,Search Engine,Web,{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_},a crawler is a,engine that retrieves web,focused web crawler,internet,introduction and related work,pages by wandering around,program used by search,retrieval,search engine,the internet,web},
number = {1994},
pages = {606--611},
title = {{Learnable Focused Meta Crawling Through Web}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212017312006184},
volume = {6},
year = {2012}
}
@article{AL-Ghuribi2015,
abstract = {Extracting useful Web content is a major step in data mining. The Web content extraction process is very important for many technologies or uses as a preprocessing of many systems such as crawlers and indexers. Additionally, the extracted content is needed by the end users especially for blind and visually impaired users. It aims to extract useful and meaningful data from Webpages that are surrounded with various clutters such as advertisements and navigation menus. Many extraction algorithms are designed for English Language and perform less efficient and less accurate in Arabic language. In this paper, a bi-languages mining algorithm for extracting Web contents called BiLEx is presented. It extracts useful Web content from Arabic and English Webpages in the approximately same level of efficiency and accuracy. An experiment is made for 600 Webpages which are chosen randomly from 30 different Websites to test the proposed algorithm performance and efficiency. Results prove that BiLEx algorithm gives high precision, recall, and F1-measure for both Arabic and English Webpages.},
author = {AL-Ghuribi, Sumaia Mohammed and Alshomrani, Saleh},
doi = {10.1007/s13369-014-1530-8},
file = {:S$\backslash$:/SciencePhdSorted/Arabian Journal for Science and Engineering/Bi-languages Mining Algorithm for Extraction Useful Web Contents (BiLEx){\_}AL-Ghuribi, Alshomrani{\_}2015.pdf:pdf},
isbn = {1336901415308},
issn = {21914281},
journal = {Arabian Journal for Science and Engineering},
keywords = {BiLEx,Clutters,Extraction algorithms,Useful content},
number = {2},
pages = {501--518},
title = {{Bi-languages Mining Algorithm for Extraction Useful Web Contents (BiLEx)}},
volume = {40},
year = {2015}
}
@article{Jung2012,
abstract = {Metadata about information sources (e.g., databases and repositories) can be collected by Query Sampling (QS). Such metadata can include topics and statistics (e.g., term frequencies) about the information sources. This provides important evidence for determining which sources in the distributed information space should be selected for a given user query. The aim of this paper is to find out the semantic relationships between the information sources in order to distribute user queries to a large number of sources. Thereby, we propose an evolutionary approach for automatically conducting QS using multiple crawlers and obtaining the optimized semantic network from the sources. The aim of combining QS and evolutionary methods is to collaboratively extract metadata about target sources and optimally integrate the metadata, respectively. For evaluating the performance of contextualized QS on 122 information sources, we have compared the ranking lists recommended by the proposed method with user feedback (i.e., ideal ranks), and also computed the precision of the discovered subsumptions in terms of the semantic relationships between the target sources. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Jung, Jason J.},
doi = {10.1016/j.ins.2010.08.042},
file = {:S$\backslash$:/SciencePhdSorted/Information Sciences/Evolutionary approach for semantic-based query sampling in large-scale information sources{\_}Jung{\_}2012.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Collective intelligence,Context,Evolutionary approach,Query sampling,System interoperability},
number = {1},
pages = {30--39},
publisher = {Elsevier Inc.},
title = {{Evolutionary approach for semantic-based query sampling in large-scale information sources}},
url = {http://dx.doi.org/10.1016/j.ins.2010.08.042},
volume = {182},
year = {2012}
}
@article{Peng2013,
abstract = {The complexity of Web information environments and multiple-topic Web pages are negative factors significantly affecting the performance of focused crawling. In a Web page, anchors or some link-contexts may misguide focused crawling, and a highly relevant region also may be obscured owing to the low overall relevance of that page. So, partitioning Web pages into smaller blocks will significantly improve the performance. In view of above, this paper presents a heuristic-based approach, CBP-SLC (Content Block Partition-Selective Link Context), which combines Web page partition algorithm and selectively uses link-context according to the relevance of content blocks, to enhance focused Web crawling. For guiding crawler, we build a weighted voting classifier by iteratively applying the SVM algorithm based on a novel TFIDF-improved feature weighting approach. During classifying, an improved 1-DNF algorithm, called 1-DNFC, is also proposed aimed at identifying more reliable negative documents from the unlabeled examples set. Experimental results show that the performance of the classifier using TFIPNDF outperforms TFIDF, and our crawler outperforms Breadth-First, Best-First, Anchor Text Only, Link-context, SLC and CBP both in Harvest rate and Target recall, which indicate our new techniques are efficient and feasible. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Peng, Tao and Liu, Lu},
doi = {10.1016/j.knosys.2013.06.008},
file = {:S$\backslash$:/SciencePhdSorted/Knowledge-Based Systems/Focused crawling enhanced by CBP-SLC{\_}Peng, Liu{\_}2013.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {CBP-SLC,DOM tree,Focused crawling,TFIPNDF,Tunneling,WVC},
pages = {15--26},
publisher = {Elsevier B.V.},
title = {{Focused crawling enhanced by CBP-SLC}},
url = {http://dx.doi.org/10.1016/j.knosys.2013.06.008},
volume = {51},
year = {2013}
}
@article{Zhang2017c,
abstract = {Whether industrial 4.0 nor Internet industry, for today's industrial manufacturing enterprises, it should be to make full use of information and communication technology to deal with the arrival of smart and effective large data, combining products, machinery and human resources into together, according to the unexpected speed about the mode of sales product, it can change the manufacturing enterprises to process innovation and reform. This paper takes the automobile manufacturing industry as an example, based on sale car large data analysis, using data mining technology, through the Java program to prepare web crawler program for data collection. To give some suggestions for the automobile manufacturing industry in the production of automobile, it reduces the inventory of automobile enterprises and the waste of resources.},
author = {Zhang, Qi and Zhan, Hongfei and Yu, Junhe},
doi = {10.1016/j.procs.2017.03.137},
file = {:S$\backslash$:/SciencePhdSorted/Procedia Computer Science/Car Sales Analysis Based on the Application of Big Data{\_}Zhang, Zhan, Yu{\_}2017.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {big data,car manufacturing,data mining technology,web crawler},
number = {Icict},
pages = {436--441},
publisher = {Elsevier Masson SAS},
title = {{Car Sales Analysis Based on the Application of Big Data}},
url = {http://dx.doi.org/10.1016/j.procs.2017.03.137},
volume = {107},
year = {2017}
}
@article{Isele2010,
abstract = {The Web of Linked Data is growing and currently consists of several hundred interconnected data sources altogether serving over 25 billion RDF triples to the Web. What has hampered the exploitation of this global dataspace up till now is the lack of an open-source Linked Data crawler which can be employed by Linked Data applications to localize (parts of) the dataspace for further processing. With LDSpider, we are closing this gap in the landscape of publicly available Linked Data tools. LDSpider traverses theWeb of Linked Data by following RDF links between data items, it supports di erent crawling strategies and allows crawled data to be stored either in les or in an RDF store.},
author = {Isele, Robert and Umbrich, J{\"{u}}rgen and Bizer, Christian and Harth, Andreas},
file = {:S$\backslash$:/SciencePhdSorted/CEUR Workshop Proceedings/LDSpider An open-source crawling framework for the web of linked data{\_}Isele et al.{\_}2010.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Crawler,Linked data,Linked data tools,Spider},
pages = {29--32},
title = {{LDSpider: An open-source crawling framework for the web of linked data}},
volume = {658},
year = {2010}
}
@inproceedings{5529547,
abstract = {Crawlers are software which can traverse the internet and retrieve web pages by hyperlinks. In the face of the large number of websites, traditional web crawlers cannot function well to get the relevant pages effectively. To solve these problems, focused crawlers utilize semantic web technologies to analyze the semantics of hyperlinks and web documents. The focused crawler is a special-purpose search engine which aims to selectively seek out pages that are relevant to a predefined set of topics, rather than to exploit all regions of the web. The main characteristic of focused crawling is that the crawler does not need to collect all web pages, but selects and retrieves only the relevant pages. So the major problem is how to retrieve the maximal set of relevant and quality pages. To address this problem, we have designed a focused crawler which calculates the relevancy of block in web page. The Block is partitioned by VIPS algorithm. Page relevancy is calculated by sum of all block relevancy scores in one page. It also calculates the URL score for identifying whether a URL is relevant or not for a specific topic.},
author = {Hati, D and Kumar, A},
booktitle = {2010 2nd International Conference on Education Technology and Computer},
doi = {10.1109/ICETC.2010.5529547},
issn = {2155-1812},
keywords = {Computer science education,Crawlers,Educational technology,Information retrieval,Internet,Partitioning algorithms,Search engines,URL score,Uniform resource locators,VIPS algorithm,Web crawlers,Web documents,Web page retrieval,Web pages,Web server,Web sites,block partitioning,block relevancy scores,focused crawler,improved focused crawling approach,information retrieval,relevant page retrieval,search engines,semantic Web,semantic Web technology,software crawlers,special-purpose search engine,vector space model},
month = {jun},
pages = {V3--269--V3--273},
title = {{Improved focused crawling approach for retrieving relevant pages based on block partitioning}},
volume = {3},
year = {2010}
}
@article{AkbariTorkestani2012,
abstract = {The recent years have witnessed the birth and explosive growth of the Web. The exponential growth of the Web has made it into a huge source of information wherein finding a document without an efficient search en- gine is unimaginable. Web crawling has become an impor- tant aspect of the Web search on which the performance of the search engines is strongly dependent. Focused Web crawlers try to focus the crawling process on the topic- relevantWeb documents. Topic oriented crawlers arewidely used in domain-specific Web search portals and personal- ized search tools. This paper designs a decentralized learn- ing automata-based focusedWeb crawler. Taking advantage of learning automata, the proposed crawler learns the most relevant URLs and the promising paths leading to the tar- get on-topic documents. It can effectively adapt its config- uration to the Web dynamics. This crawler is expected to have a higher precision rate because of construction a small Web graph of only on-topic documents. Based on the Mar- tingale theorem, the convergence of the proposed algorithm is proved. To showthe performance of the proposed crawler, extensive simulation experiments are conducted. The ob- tained results show the superiority of the proposed crawler over several existing methods in terms of precision, recall, and running time. The t-test is used to verify the statistical significance of the precision results of the proposed crawler.},
author = {{Akbari Torkestani}, Javad},
doi = {10.1007/s10489-012-0351-2},
file = {:S$\backslash$:/SciencePhdSorted/Applied Intelligence/An adaptive focused Web crawling algorithm based on learning automata{\_}Akbari Torkestani{\_}2012.pdf:pdf},
isbn = {1048901203},
issn = {0924669X},
journal = {Applied Intelligence},
keywords = {Focused Web crawler,Learning automata,Search engine,Web crawling},
pages = {1--16},
title = {{An adaptive focused Web crawling algorithm based on learning automata}},
year = {2012}
}
@article{Styperek2015,
abstract = {A regular user of a semantic search system frequently posses no knowledge about the SPARQL language nor about the ontology of a given knowledge base, especially when it provides domain-unspecific data obtained from heterogeneous sources. Nevertheless, he/she should be provided with tools enabling both intuitive and effective exploration of RDF-compliant knowledge bases. Natural language querying is one of the solutions that have been proposed so far as means for making knowledge bases more user-friendly. However, the results of natural language querying usually have lower precision and recall than analogical results of graph-based querying. In the paper, we introduce an evaluation methodology based on the 2011 QALD workshop queries that allows to measure the accuracy of a semantic search system as well as the complexity of the query formulation process. The obtained results confirm the intuition that graph-based querying, although assuring comparatively high accuracy of the results, is usually still too difficult for regular users. On the other hand, on the basis of results obtained for an experimental search system referred to as Semantic Focused Crawler, we claim that enhancing a SPARQL-compliant graph-based system by an entity-type recommendation feature may reduce the number of query elements necessary to formulate a query without compromising the quality of the results.},
author = {Styperek, Adam and Ciesielczyk, Michal and Szwabe, Andrzej and Misiorek, Pawel},
doi = {10.1007/s40595-015-0044-y},
file = {:S$\backslash$:/SciencePhdSorted/Vietnam Journal of Computer Science/Evaluation of SPARQL-compliant semantic search user interfaces{\_}Styperek et al.{\_}2015.pdf:pdf},
issn = {2196-8896},
journal = {Vietnam Journal of Computer Science},
keywords = {DBpedia,RDF,SPARQL,Search,Semantic Web},
number = {3},
pages = {191--199},
publisher = {Springer Berlin Heidelberg},
title = {{Evaluation of SPARQL-compliant semantic search user interfaces}},
url = {http://dx.doi.org/10.1007/s40595-015-0044-y},
volume = {2},
year = {2015}
}
@article{Du2013a,
abstract = {A web crawler is an important research component in a search engine. In this paper, a new method for measuring the similarity of formal concept analysis (FCA) concepts and a new notion of a web page's rank are proposed that use an information content approach based on users' web logs. First, an extension similarity and an intension similarity that analyze a user's browsing pattern and their hyperlinks are proposed. Second, the information content similarity between two nouns is computed automatically by examining their ISA and Part-Of hierarchy and using a user's web log. A method for computing the semantic similarity between two concepts in two different concept lattices (the base concept lattice and the current concept lattice) and finding the semantic ranking of web pages is proposed. Last, our experiment demonstrates that our crawler is more suitable for crawling focused web pages. It proves that the semantic ranking of web pages is useful and efficient for making a web crawler's choice of a web page for continuing work. ?? 2012 Elsevier Inc. All rights reserved.},
author = {Du, Yajun and Hai, Yufeng},
doi = {10.1016/j.jss.2012.07.040},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Systems and Software/Semantic ranking of web pages based on formal concept analysis{\_}Du, Hai{\_}2013.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Crawling direction,Formal concept analysis,Search engine,Web crawler},
number = {1},
pages = {187--197},
publisher = {Elsevier Inc.},
title = {{Semantic ranking of web pages based on formal concept analysis}},
url = {http://dx.doi.org/10.1016/j.jss.2012.07.040},
volume = {86},
year = {2013}
}
@misc{Grubic2017c,
abstract = {Research project SM01 (Parallel Semantic Crawler for manufacturing multilingual web...) In the DLC pattern multiple pages of the same web site are loaded and processed in parallel threads.},
author = {Grubi{\'{c}}, Goran},
doi = {10.17632/hzxkbhfw7z.1},
number = {v1},
publisher = {Mendeley Data},
title = {{SM01: Web Crawling with DLC parallel execution pattern - experiment reports}},
url = {https://data.mendeley.com/datasets/hzxkbhfw7z/1},
year = {2017}
}
@article{Liu2011,
abstract = {The key to Deep Web Crawling is to submit valid input values to a query form and retrieve Deep Web content efficiently. In the literature, related work focus only on generic text boxes or entire query forms, causing the problem of "data islands" or inferior validity of query submission. This paper proposes the concept of Minimum Executable Pattern (MEP), a minimal combination of elements in a query form that can conduct a successful query, and then presents a MEPGeneration method and a MEP-based Deep Web adaptive crawling method. The query form is parsed and partitioned into MEP set, and then local-optimal queries are generated by choosing a MEP in the MEP set and a keyword vector of the MEP. Furthermore, the crawler can make a decision on its termination to balance the trade-off between high coverage of the content and resource consumption. The adoption of MEP is expected to improve the validity of query submission, and adaptive selection of multiple MEPs shows good effect for overcoming the problem of "data islands". We present a set of experiments to validate the effectiveness of the proposed method. Experimental results show that our method outperforms the state of art methods in terms of query capability and applicability, and on average, it achieves good coverage by issuing only a few hundred queries. {\textcopyright} 2010 Springer Science+Business Media, LLC.},
author = {Liu, Jun and Jiang, Lu and Wu, Zhaohui and Zheng, Qinghua},
doi = {10.1007/s10844-010-0124-5},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Intelligent Information Systems/Deep Web adaptive crawling based on minimum executable pattern{\_}Liu et al.{\_}2011.pdf:pdf},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Adaptive crawling,Deep Web,Deep Web surfacing,Minimum executable pattern},
number = {2},
pages = {197--215},
title = {{Deep Web adaptive crawling based on minimum executable pattern}},
volume = {36},
year = {2011}
}
@misc{Grubic2017e,
abstract = {Research project SM01 (Parallel Semantic Crawler for manufacturing multilingual web...) Research sample sets mentioned in "Evaluation" section of the paper given in spreadsheet and plain text formats + including some extra information.. Origin of the initial research data set: the research sample set was extracted from CRM system of a company doing business in the domain of application},
author = {Grubi{\'{c}}, Goran},
doi = {10.17632/b4cs4rky9s.1},
publisher = {Mendeley Data},
title = {{SM01: Research sample subsets}},
url = {https://data.mendeley.com/datasets/b4cs4rky9s/1},
volume = {v1},
year = {2017}
}
@article{Kovacevic2008,
abstract = {Professionals and craftsmen in the construction sector make an intensive use of information in their decision-making processes but only make limited use of the abundant information that is potentially available to them, particularly on the web. Consequently, designs are impoverished, construction is defective, and innovation is delayed. To facilitate convivial access to focused information, we have developed a question-and-answer (Q-A) system (reported elsewhere). To support this system, we have developed an automated crawler that permits the establishment of a bank of relevant pages, adapted to the needs of this particular industry-user community. It is based on the machine-learning framework in which an intelligent decision unit is trained to distinguish between nontopic and informative pages. We show that standard approaches which use both positive and negative classes are sensitive to the noise in the negative class. We propose different techniques for learning without negative examples, since initially one only has limited, positive information labeled by human experts; they are evaluated. Our crawler that uses the positive examples-based learning (PEBL) framework is able to collect construction-oriented pages with high precision and discovery rate. It can also be used to build domain-specific collections of pages in different scientific or professional contexts. [ABSTRACT FROM AUTHOR]},
author = {Kova{\v{c}}evi{\'{c}}, Milos and Davidson, Colin H},
doi = {10.1080/08839510802028447},
file = {:S$\backslash$:/SciencePhdSorted/Applied Artificial Intelligence/Crawling the Construction Web - a Machine-Learning Approach Without Negative Examples.{\_}Kova{\v{c}}evi{\'{c}}, Davidson{\_}2008.pdf:pdf},
isbn = {0883-9514$\backslash$r1087-6545},
issn = {08839514},
journal = {Applied Artificial Intelligence},
keywords = {ARCHITECTURE,ARTIFICIAL intelligence,CIVIL engineering,COMPUTER networks,COMPUTER science,INFORMATION retrieval,RESEARCH},
number = {5},
pages = {459--482},
title = {{Crawling the Construction Web - a Machine-Learning Approach Without Negative Examples.}},
url = {10.1080/08839510802028447},
volume = {22},
year = {2008}
}
@article{Brunelle2016,
abstract = {As web technologies evolve, web archivists work to adapt so that digital history is preserved. Recent advances in web technologies have introduced client-side executed scripts (Ajax) that, for example, load data without a change in top level Universal Resource Identifier (URI) or require user interaction (e.g., content loading via Ajax when the page has scrolled). These advances have made automating methods for capturing web pages more difficult. In an effort to understand why mementos (archived versions of live resources) in today's archives vary in completeness and sometimes pull content from the live web, we present a study of web resources and archival tools. We used a collection of URIs shared over Twitter and a collection of URIs curated by Archive-It in our investigation. We created local archived versions of the URIs from the Twitter and Archive-It sets using WebCite, wget, and the Heritrix crawler. We found that only 4.2 {\%} of the Twitter collection is perfectly archived by all of these tools, while 34.2 {\%} of the Archive-It collection is perfectly archived. After studying the quality of these mementos, we identified the practice of loading resources via JavaScript (Ajax) as the source of archival difficulty. Further, we show that resources are increasing their use of JavaScript to load embedded resources. By 2012, over half (54.5 {\%}) of pages use JavaScript to load embedded resources. The number of embedded resources loaded via JavaScript has increased by 12.0 {\%} from 2005 to 2012. We also show that JavaScript is responsible for 33.2 {\%} more missing resources in 2012 than in 2005. This shows that JavaScript is responsible for an increasing proportion of the embedded resources unsuccessfully loaded by mementos. JavaScript is also responsible for 52.7 {\%} of all missing embedded resources in our study. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
author = {Brunelle, Justin F. and Kelly, Mat and Weigle, Michele C. and Nelson, Michael L.},
doi = {10.1007/s00799-015-0140-8},
file = {:S$\backslash$:/SciencePhdSorted/International Journal on Digital Libraries/The impact of JavaScript on archivability{\_}Brunelle et al.{\_}2016.pdf:pdf},
isbn = {14325012 (ISSN)},
issn = {14321300},
journal = {International Journal on Digital Libraries},
keywords = {Digital preservation,Web architecture,Web archiving},
number = {2},
pages = {95--117},
publisher = {Springer Berlin Heidelberg},
title = {{The impact of JavaScript on archivability}},
volume = {17},
year = {2016}
}
@article{Jung2009,
abstract = {Recently, many systems, the so-called open-networked systems, have been opened to the public in a variety of ways. Such openness is providing people with not only simple services (e.g., ODBC and open API) but also their local knowledge. In this paper, we are focusing on the local knowledge which is composed of two parts: (i) domain ontology and (ii) business rules. More importantly, the local knowledge is applicable to support logical inferences of decision supporting processes in other information systems. In this context, we propose a novel framework of open decision support system (ODSS) which is capable of gathering relevant knowledge from an open-networked environment. Thereby, we exploit two main methods: (i) context-based focused crawler architecture to discover local knowledge from interlinked systems, and (ii) knowledge alignment process to integrate the discovered local knowledge. As a conclusion, we demonstrate how the merged knowledge can be exploited to support decision making efficiently by conducting some experimentations. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Jung, Jason J.},
doi = {10.1016/j.eswa.2008.02.057},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/Towards open decision support systems based on semantic focused crawling{\_}Jung{\_}2009.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Focused crawler,Ontology alignment,Open decision support systems},
number = {2 PART 2},
pages = {3914--3922},
publisher = {Elsevier Ltd},
title = {{Towards open decision support systems based on semantic focused crawling}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.02.057},
volume = {36},
year = {2009}
}
@misc{Grubic2017b,
abstract = {Research project SM01 (Parallel Semantic Crawler for manufacturing multilingual web...) The JLC pattern is running parallel thread for each domain in the Crawl Job domain cue. Objective of the experiment was to find what number of parallel DLC threads (TCmax) results in the highest execution efficiency. The crawlers were run: 2, 4, 6, 8, 10, 12, 14 and 16 parallel DLC threads, against the Sc subset. Other configuration parameters: Load Take per iteration (LT) set to 1 and Page Loads limit (PLmax) set to 30. Sampling period of the resource utilization: 5 seconds.},
address = {Belgrade},
author = {Grubi{\'{c}}, Goran},
doi = {10.17632/5prx8vycr5.1},
edition = {v1},
institution = {University of Belgrade Faculty of Organizational Sciences},
publisher = {Mendeley Data},
title = {{SM01: Web Crawling with JLC parallel execution pattern - experiment reports}},
url = {https://data.mendeley.com/datasets/5prx8vycr5/1},
year = {2017}
}
@article{Toral2016,
abstract = {We present a widely applicable methodology to bring machine translation (MT) to under-resourced languages in a cost-effective and rapid manner. Our proposal relies on web crawling to automatically acquire parallel data to train statistical MT systems if any such data can be found for the language pair and domain of interest. If that is not the case, we resort to (1) crowdsourcing to translate small amounts of text (hundreds of sentences), which are then used to tune statistical MT models, and (2) web crawling of vast amounts of monolingual data (millions of sentences), which are then used to build language models for MT. We apply these to two respective use-cases for Croatian, an under-resourced language that has gained relevance since it recently attained official status in the European Union. The first use-case regards tourism, given the importance of this sector to Croatia's economy, while the second has to do with tweets, due to the growing importance of social media. For tourism, we crawl parallel data from 20 web domains using two state-of-the-art crawlers and explore how to combine the crawled data with bigger amounts of general-domain data. Our domain-adapted system is evaluated on a set of three additional tourism web domains and it outperforms the baseline in terms of automatic metrics and/or vocabulary coverage. In the social media use-case, we deal with tweets from the 2014 edition of the soccer World Cup. We build domain-adapted systems by (1) translating small amounts of tweets to be used for tuning by means of crowdsourcing and (2) crawling vast amounts of monolingual tweets. These systems outperform the baseline (Microsoft Bing) by 7.94 BLEU points (5.11 TER) for Croatian-to-English and by 2.17 points (1.94 TER) for English-to-Croatian on a test set translated by means of crowdsourcing. A complementary manual analysis sheds further light on these results.},
author = {Toral, Antonio and Espl{\'{a}}-Gomis, Miquel and Klubi{\v{c}}ka, Filip and Ljube{\v{s}}i{\'{c}}, Nikola and Papavassiliou, Vassilis and Prokopidis, Prokopis and Rubino, Raphael and Way, Andy},
doi = {10.1007/s10579-016-9363-6},
file = {:S$\backslash$:/SciencePhdSorted/Language Resources and Evaluation/Crawl and crowd to bring machine translation to under-resourced languages{\_}Toral et al.{\_}2016.pdf:pdf},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Crowdsourcing,Statistical machine translation,Web crawling},
pages = {1--33},
title = {{Crawl and crowd to bring machine translation to under-resourced languages}},
year = {2016}
}
@article{Shchekotykhin2010,
abstract = {Web mining systems exploit the redundancy of data published on the Web to automatically extract information from existing Web documents. The first step in the information extraction process is thus to locate within a limited period of time as many Web pages as possible that contain relevant information, a task which is commonly accomplished by applying focused crawling techniques. The performance of such a crawler can be measured by its "recall", i.e. the percentage of documents found and identified as relevant compared to the number of existing documents. A higher recall value implies that more redundant data is available, which in turn leads to better results in the subsequent fact extraction phase. In this paper, we propose xCrawl, a new focused crawling method which outperforms state-of-the-art approaches with respect to recall values achievable within a given period of time. This method is based on a new combination of ideas and techniques used to identify and exploit navigational structures of Websites, such as hierarchies, lists or maps. In addition, automatic query generation is applied to rapidly collect Web sources containing target documents. The proposed crawling technique was inspired by the requirements of a Web mining system developed to extract product and service descriptions and was evaluated in different application scenarios. Comparisons with existing focused crawling techniques reveal that the new crawling method leads to a significant increase in recall whilst maintaining precision.},
author = {Shchekotykhin, Kostyantyn and Jannach, Dietmar and Friedrich, Gerhard},
doi = {10.1007/s10115-009-0266-3},
file = {:S$\backslash$:/SciencePhdSorted/Knowledge and Information Systems/xCrawl A high-recall crawling method for Web mining{\_}Shchekotykhin, Jannach, Friedrich{\_}2010.pdf:pdf},
isbn = {9780769535029},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Information extraction,Information retrieval,Web crawling,Web mining},
number = {2},
pages = {303--326},
title = {{xCrawl: A high-recall crawling method for Web mining}},
volume = {25},
year = {2010}
}
@article{Achsan2014,
abstract = {Mining data from a web database becomes more challenging in recent years due to the exploding size of data, the rising of dynamic web, and the increasing performance of web security. Mining data from a web database differs from mining data from web sites because it is intended to collect specific data from a single web site. Collecting a very large data in a limited time tends to be detected as a cyber attack and will be banned from connecting into the web server. To avoid the problem, this paper proposes a crawling method to mine web database faster and cheaper than conventional web crawlers. The method used is to run hundreds of threads from a single web crawler in a single computer and to distribute the threads into hundreds or thousands publicly available proxy servers. This web crawler strategy highly increases the speed of mining and is more secure than using single thread of web crawler. ?? 2014 The Authors. Published by Elsevier Ltd.},
author = {Achsan, Harry T Yani and Wibowo, Wahyu Catur},
doi = {10.1016/j.proeng.2014.03.017},
file = {:S$\backslash$:/SciencePhdSorted/Procedia Engineering/A fast distributed focused-web crawling{\_}Achsan, Wibowo{\_}2014.pdf:pdf},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {Focused web crawler,Multi thread,Proxy,Web database},
pages = {492--499},
publisher = {Elsevier B.V.},
title = {{A fast distributed focused-web crawling}},
url = {http://dx.doi.org/10.1016/j.proeng.2014.03.017},
volume = {69},
year = {2014}
}
@article{Alam2012,
abstract = {Webcrawlers are essential tomanyWeb applications, such asWeb search engines, Web archives, andWeb directories, which maintainWeb pages in their local repositories. In this paper, we study the problem of crawl scheduling that biases crawl ordering toward important pages. We propose a set of crawling algorithms for effective and efficient crawl ordering by prioritizing important pages with the well-known PageRank as the importance metric. In order to score URLs, the proposed algorithms utilize various features, including partial link structure, inter-host links, page titles, and topic relevance. We conduct a large- scale experiment using publicly available data sets to examine the effect of each feature on crawl ordering and evaluate the performance of many algorithms. The experimental results verify the efficacy of our schemes. In particular, compared with the representative RankMass crawler, the FPR-title-host algorithm reduces computational overhead by a factor as great as three in running time while improving effectiveness by 5{\%} in cumulative PageRank.},
author = {Alam, Md Hijbul and Ha, Jong Woo and Lee, Sang Keun},
doi = {10.1007/s10115-012-0535-4},
file = {:S$\backslash$:/SciencePhdSorted/Knowledge and Information Systems/Novel approaches to crawling important pages early{\_}Alam, Ha, Lee{\_}2012.pdf:pdf;:S$\backslash$:/SciencePhdSorted/Knowledge and Information Systems/Novel approaches to crawling important pages early{\_}Alam, Ha, Lee{\_}2012(2).pdf:pdf},
isbn = {02191377 (ISSN)},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Crawl ordering,Fractional PageRank,PageRank,Web crawler},
number = {3},
pages = {707--734},
title = {{Novel approaches to crawling important pages early}},
volume = {33},
year = {2012}
}
@article{Du2013,
abstract = {With the Internet growing exponentially, search engines are encountering unprecedented challenges. A focused search engine selectively seeks out web pages that are relevant to user topics. Determining the best strategy to utilize a focused search is a crucial and popular research topic. At present, the rank values of unvisited web pages are computed by considering the hyperlinks (as in the PageRank algorithm), a Vector Space Model and a combination of them, and not by considering the semantic relations between the user topic and unvisited web pages. In this paper, we propose a concept context graph to store the knowledge context based on the user's history of clicked web pages and to guide a focused crawler for the next crawling. The concept context graph provides a novel semantic ranking to guide the web crawler in order to retrieve highly relevant web pages on the user's topic. By computing the concept distance and concept similarity among the concepts of the concept context graph and by matching unvisited web pages with the concept context graph, we compute the rank values of the unvisited web pages to pick out the relevant hyperlinks. Additionally, we constitute the focused crawling system, and we retrieve the precision, recall, average harvest rate, and F-measure of our proposed approach, using Breadth First, Cosine Similarity, the Link Context Graph and the Relevancy Context Graph. The results show that our proposed method outperforms other methods. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Du, Yajun and Pen, Qiangqiang and Gao, Zhaoqiong},
doi = {10.1016/j.datak.2013.09.003},
file = {:S$\backslash$:/SciencePhdSorted/Data and Knowledge Engineering/A topic-specific crawling strategy based on semantics similarity{\_}Du, Pen, Gao{\_}2013.pdf:pdf},
isbn = {1398009725},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Concept context graph,Focused crawling,Formal concept analysis,Information retrieval,Search engine,Web crawler,Web information systems},
pages = {75--93},
publisher = {Elsevier B.V.},
title = {{A topic-specific crawling strategy based on semantics similarity}},
url = {http://dx.doi.org/10.1016/j.datak.2013.09.003},
volume = {88},
year = {2013}
}
@article{Wang2009b,
abstract = {Focused crawling is an important technique for topical resource discovery on the Web. The key issue in focused crawling is to prioritize uncrawled uniform resource locators (URLs) in the frontier to focus the crawling on relevant pages. Traditional focused crawlers mainly rely on content analysis. Link-based techniques are not effectively exploited despite their usefulness. In this paper, we propose a new frontier prioritizing algorithm, namely the on-line topical importance estimation (OTIE) algorithm. OTIE combines link- and content-based analysis to evaluate the priority of an uncrawled URL in the frontier. We performed real crawling experiments over 30 topics selected from the Open Directory Project (ODP) and compared harvest rate and target recall of the four crawling algorithms: breadth-first, link-context-prediction, on-line page importance computation (OPIC) and our OTIE. Experimental results showed that OTIE significantly outperforms the other three algorithms on the average target recall while maintaining an acceptable harvest rate. Moreover, OTIE is much faster than the traditional focused crawling algorithm. {\textcopyright} 2009 Zhejiang University and Springer-Verlag GmbH.},
author = {Wang, Can and Guan, Zi-yu and Chen, Chun and Bu, Jia-jun and Wang, Jun-feng and Lin, Huai-zhong},
doi = {10.1631/jzus.A0820481},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Zhejiang University SCIENCE A/On-line topical importance estimation an effective focused crawling algorithm combining link and content analysis{\_}Wang et al.{\_}2009.pdf:pdf},
isbn = {1673-565X},
issn = {1673-565X},
journal = {Journal of Zhejiang University SCIENCE A},
keywords = {Classifiers,Focused crawlers,On-line topical importance estimation (OTIE) algor,PageRank,Topical crawlers},
number = {8},
pages = {1114--1124},
title = {{On-line topical importance estimation: an effective focused crawling algorithm combining link and content analysis}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-68849107224{\&}partnerID=tZOtx3y1},
volume = {10},
year = {2009}
}
@article{Insa2013,
abstract = {The main content in a webpage is usually centered and visible without the need to scroll. It is often rounded by the navigation menus of the website and it can include advertisements, panels, banners, and other not necessarily related information. The process to automatically extract the main content of a webpage is called content extraction. Content extraction is an area of research of widely interest due to its many applications. Concretely, it is useful not only for the final human user, but it is also frequently used as a preprocessing stage of different systems (i.e., robots, indexers, crawlers, etc.) that need to extract the main content of a web document to avoid the treatment and processing of other useless information. In this work we present a new technique for content extraction that is based on the information contained in the DOM tree. The technique analyzes the hierarchical relations of the elements in the webpage and the distribution of textual information in order to identify the main block of content. Thanks to the hierarchy imposed by the DOM tree the technique achieves a considerable recall and precision. Using the DOM structure for content extraction gives us the benefits of other approaches based on the syntax of the webpage (such as characters, words and tags), but it also gives us a very precise information regarding the related components in a block (not necessarily textual such as images or videos), thus, producing very cohesive blocks. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Insa, David and Silva, Josep and Tamarit, Salvador},
doi = {10.1016/j.jlap.2013.01.002},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Logic and Algebraic Programming/Using the wordsleafs ratio in the DOM tree for content extraction{\_}Insa, Silva, Tamarit{\_}2013.pdf:pdf},
issn = {15678326},
journal = {Journal of Logic and Algebraic Programming},
keywords = {Block detection,Content extraction,DOM,Information retrieval},
number = {8},
pages = {311--325},
publisher = {Elsevier Inc.},
title = {{Using the words/leafs ratio in the DOM tree for content extraction}},
url = {http://dx.doi.org/10.1016/j.jlap.2013.01.002},
volume = {82},
year = {2013}
}
@article{Batzios2012,
abstract = {This paper presents WebOWL, an experiment in using the latest technologies to develop a Semantic Web search engine. WebOWL consists of a community of intelligent agents, acting as crawlers, that are able to discover and learn the locations of Semantic Web neighborhoods on the Web, a semantic database to store data from different ontologies, a query mechanism that supports semantic queries in OWL, and a ranking algorithm that determines the order of the returned results based on the semantic relationships of classes and individuals. The system has been implemented using Jade, Jena and the db4o object database engine and has successfully stored over one million OWL classes, individuals and properties. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Batzios, Alexandros and Mitkas, Pericles A.},
doi = {10.1016/j.eswa.2011.11.034},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/WebOWL A Semantic Web search engine development experiment{\_}Batzios, Mitkas{\_}2012.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {OWL database,OWL querying,Semantic Web,Semantic querying,Semantic search},
number = {5},
pages = {5052--5060},
publisher = {Elsevier Ltd},
title = {{WebOWL: A Semantic Web search engine development experiment}},
url = {http://dx.doi.org/10.1016/j.eswa.2011.11.034},
volume = {39},
year = {2012}
}
@article{Cafarella2011,
abstract = {Google's Web Tables and Deep Web Crawler identify and deliver this otherwise inaccessible resource directly to end users.},
author = {Cafarella, Michael J and Halevy, Alon and Madhavan, Jayant},
doi = {10.1145/1897816.1897839},
file = {:S$\backslash$:/SciencePhdSorted/Communications of the ACM/Structured data on the web{\_}Cafarella, Halevy, Madhavan{\_}2011.pdf:pdf},
isbn = {9781617290398},
issn = {00010782},
journal = {Communications of the ACM},
number = {2},
pages = {72},
title = {{Structured data on the web}},
url = {http://portal.acm.org/citation.cfm?doid=1897816.1897839},
volume = {54},
year = {2011}
}
@article{Rogers2015,
author = {Rogers, Anat Ben-david Richard A},
doi = {10.1007/s00799-015-0153-3},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Lost but not forgotten finding pages on the unarchived web{\_}Rogers{\_}2015.pdf:pdf},
keywords = {anchor text,information retrieval,link evidence,web archives,web archiving,web crawlers},
pages = {247--265},
title = {{Lost but not forgotten : finding pages on the unarchived web}},
year = {2015}
}
@article{,
file = {:S$\backslash$:/SciencePhdSorted/Unknown/No Title{\_}Unknown{\_}Unknown(5).pdf:pdf},
keywords = {business intelligence,corre sponding author,corresponding author,crawler,faculty of organizational sciences,goran bo{\v{z}}o grubi{\'{c}},manufacturing industry,master,mr,parallel,s institution,semantic similarity,web content mining,web crawler},
title = {{No Title}}
}
@article{Du2017,
author = {Du, YaJun and Li, ChenXing and Hu, Qiang and Li, XiaoLei and Chen, XiaoLiang},
doi = {10.1016/j.neucom.2016.08.142},
file = {:S$\backslash$:/SciencePhdSorted/Neurocomputing/Ranking Webpages Using a Path Trust Knowledge Graph{\_}Du et al.{\_}2017.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Context graph,Knowledge graph,Language model,Path trust degree,Topic-specific crawlers,context graph,knowledge graph,language model,path trust degree,topic-specific crawlers},
publisher = {Elsevier B.V.},
title = {{Ranking Webpages Using a Path Trust Knowledge Graph}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217309669},
year = {2017}
}
@article{Iu2012,
author = {Iu, H Ongyu L and Ilios, E Vangelos M},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Probabilistic models for focused web crawling{\_}Iu, Ilios{\_}2012.pdf:pdf},
keywords = {focused crawlers,machine learning,text and link analysis,topical crawlers,web mining},
number = {3},
title = {{Probabilistic models for focused web crawling}},
volume = {28},
year = {2012}
}
@article{Shmueli2017,
author = {Shmueli, Edi and Zaides, Ilya},
doi = {10.1007/s10723-017-9400-8},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Framework for Scalable File System Metadata Crawling and Differencing{\_}Shmueli, Zaides{\_}2017.pdf:pdf},
issn = {1570-7873},
keywords = {File system,Metadata,Crawler,Scheduler,crawler,file system,metadata},
publisher = {Journal of Grid Computing},
title = {{Framework for Scalable File System Metadata Crawling and Differencing}},
year = {2017}
}
@article{Nigam2014,
author = {Nigam, Aviral},
doi = {10.5963/IJCSAI0403001},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Web Crawling Algorithms{\_}Nigam{\_}2014.pdf:pdf},
keywords = {- web crawler,a,adaptive a},
pages = {63--67},
title = {{Web Crawling Algorithms}},
volume = {4},
year = {2014}
}
@article{Ruzic2011,
author = {Ruzic, D and Andrlic, B and Ruzic, I},
doi = {10.5848/APBJ.2011.00141},
file = {:S$\backslash$:/SciencePhdSorted/International Journal of {\ldots}/Web 2.0 Promotion Techniques in Hospitality Industry.{\_}Ruzic, Andrlic, Ruzic{\_}2011.pdf:pdf},
journal = {International Journal of {\ldots}},
pages = {310--320},
title = {{Web 2.0 Promotion Techniques in Hospitality Industry.}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=17416264{\&}AN=66508364{\&}h=Z7/P5YksuHB4oJEc6itOAt5ZdktPtGgKGJYYiQVUoalicb861A0GbbCC9qz1yIhL2yJE31yN3/Ix5hxGhbEj+w=={\&}crl=c},
year = {2011}
}
@article{Lv2016,
abstract = {There is a need for enhanced context-based document relevance assessment and ranking to facilitate the retrieval of more relevant information for supporting environmental decision making. This paper proposes a new context-based relevance assessment method, which allows for enhanced context representation and context-based document relevance recognition through: (1) a context-aware and deep semantic concept indexing approach, and (2) a deep and semantically-sensitive relevance estimation approach. The proposed relevance assessment method was integrated into two widely-used document ranking models [vector space model (VSM) and statistical language model (SLM)], resulting in two improved ranking methods: (1) a context-enhanced VSM-based method, and (2) a context-enhanced SLM-based method. The two context-enhanced document ranking methods were evaluated in retrieving webpages that are relevant to transportation project environmental review. The two context-enhanced methods were compared with each other and with their provenance methods (i.e., original VSM and SLM) in terms of mean precision (MP) and mean average precision (MAP). The context-enhanced VSM-based method outperformed the context-enhanced SLM-based method on every metric. It achieved 48{\%} MAP, 79{\%} MP at the top 10 retrieved documents, and over 65{\%} MP at the top 50 retrieved documents, on the testing data. It also showed significant improvement over the state-of-the-art keyword-based VSM method.},
author = {Lv, Xuan and El-Gohary, Nora M.},
doi = {10.1016/j.aei.2016.08.004},
file = {:S$\backslash$:/SciencePhdSorted/Advanced Engineering Informatics/Enhanced context-based document relevance assessment and ranking for improved information retrieval to support environmental decision ma.pdf:pdf},
isbn = {1474-0346},
issn = {14740346},
journal = {Advanced Engineering Informatics},
keywords = {Context-based relevance assessment,Context-enhanced document ranking,Information retrieval,Project environmental review,Statistical language model,Vector space model},
number = {4},
pages = {737--750},
publisher = {Elsevier Ltd},
title = {{Enhanced context-based document relevance assessment and ranking for improved information retrieval to support environmental decision making}},
url = {http://dx.doi.org/10.1016/j.aei.2016.08.004},
volume = {30},
year = {2016}
}
@article{Saleiro2017,
author = {Saleiro, Pedro and Rodrigues, Eduarda M and Soares, Carlos},
doi = {10.1007/s00354-017-0021-3},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/TexRep A Text Mining Framework for Online Reputation Monitoring{\_}Saleiro, Rodrigues, Soares{\_}2017.pdf:pdf},
issn = {0288-3635},
title = {{TexRep: A Text Mining Framework for Online Reputation Monitoring}},
year = {2017}
}
@article{Bozkir2018,
abstract = {Abstract In this paper, we propose a ranking approach which considers visual similarities among web pages by using structure and vision-based features. Throughout the study, we aim to understand and represent the web page visual structure as in the way people do by focusing on the layout similarity through the wireframe design. The conducted study is composed of two parts. In the first part, structural similarities are analyzed with the proposed concept of “layout components” along with visual inspection of {\{}DOM{\}} trees. In this way, five types of structural layout components are proposed and revealed. Moreover, whitespaces are also utilized since they are important visual cues in the visual perception of web pages. In the second part, a computer-vision based method named histogram of oriented gradients (HOG) is employed to reveal local visual cues in terms of edge orientations. Following the feature extraction phases, extracted feature histograms are mapped on spatial information preserving multilevel and multi-resolution bag of features representation method named spatial pyramid matching. In this way, three goals were achieved: (1) the visual layout of web pages were mapped and compared in a multi-resolution schema; (2) the intermediate process of visual segmentation was removed; and (3) efficient and easily comparable web page layout signatures were generated. We also conducted a questionnaire study covering 312 subjects. This helped us to create a benchmark dataset involving similarity scores collected from individuals. So far, there exists no web page layout similarity ranking oriented corpus in the literature. Our suggested approach achieved a remarkable ranking performance at top-5 and top-10 retrieval results. According to the findings of the comparative study, our approach outperforms some structure and vision-based studies in the literature. With this achievement, web pages could be employed as a query item to find other, similar web pages by taking into consideration that they are web pages, instead of images or anything else. },
author = {Bozkir, Ahmet Selman and {Akcapinar Sezer}, Ebru},
doi = {10.1016/j.ijhcs.2017.10.008},
file = {:S$\backslash$:/SciencePhdSorted/International Journal of Human-Computer Studies/Layout-based computation of web page similarity ranks{\_}Bozkir, Akcapinar Sezer{\_}2018.pdf:pdf},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {Web page layout,Layout similarity,Similarity ranki},
number = {September 2016},
pages = {95--114},
publisher = {Elsevier Ltd},
title = {{Layout-based computation of web page similarity ranks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1071581917301465},
volume = {110},
year = {2018}
}
@article{Yetgin2015,
abstract = {The paper introduces the concept of product identity clustering based on new similarity metrics and new performance metrics for web crawled products. Product identity clustering is defined as the clustering of identical products here, e.g. for price comparison purpose. Products blindly crawled over the web sources, e.g. online marketplaces, have different description format where the features describing the same products differ in both number and representation format. This problem causes imperfect feature vectors where the vectors are considered to be not uniform in both length and structure, with the features of various data types (numeric, categorical) and unknown vector structures. Furthermore, the product information usually contains redundant, missing, or faulty data, regarded as noise here. The product identity clustering becomes a challenge when the vectors‘ metadata is unknown prior and the imperfect nature of feature vectors is considered with the occurrence of noises. In this paper, the product identity clustering concept is introduced as new mining metric in e-commerce. Then novel similarity metrics are introduced to improve the product identity clustering performance of legacy metrics. Finally, novel performance metrics are proposed to measure the performance of the identity clustering algorithms. Using these metrics, a comparison of the legacy based similarity metrics (euclidian, cosine,etc) and the proposed similarity metrics is given. The results show that legacy metrics are not successful in discriminating the identical web-crawled products and the proposed metrics enables better achievement in the product identity clustering problem},
author = {Yetgin, Zeki and G??z??kara, Furkan},
doi = {10.3906/elk-1307-127},
file = {:S$\backslash$:/SciencePhdSorted/Turkish Journal of Electrical Engineering and Computer Sciences/New metrics for clustering of identical products over imperfect data{\_}Yetgin, Gzkara{\_}2015.pdf:pdf},
issn = {13036203},
journal = {Turkish Journal of Electrical Engineering and Computer Sciences},
keywords = {Identity clustering,Performance metrics,Product clustering,Similarity metrics,Web mining},
number = {4},
pages = {1195--1208},
title = {{New metrics for clustering of identical products over imperfect data}},
volume = {23},
year = {2015}
}
@article{Baggio2007,
abstract = {The website network of a tourism destination is examined. Network theoretic metrics are used to gauge the static and dynamic characteristics of the webspace. The topology of the network is found partly similar to the one exhibited by similar systems. However, some differences are found, mainly due to the relatively poor connectivity and clusterisation of the network. These results are interpreted by considering the formation mechanisms and the connotation of the linkages between websites. Clustering and assortativity coefficients are proposed as quantitative estimations of the degree of collaboration and cooperation among destination stakeholders. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {physics/0606043},
author = {Baggio, Rodolfo},
doi = {10.1016/j.physa.2007.01.008},
eprint = {0606043},
file = {:S$\backslash$:/SciencePhdSorted/Physica A Statistical Mechanics and its Applications/The web graph of a tourism system{\_}Baggio{\_}2007.pdf:pdf},
isbn = {03784371},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Complex networks,Internet,Tourism systems,Web},
number = {2},
pages = {727--734},
primaryClass = {physics},
title = {{The web graph of a tourism system}},
volume = {379},
year = {2007}
}
@article{Louvan2009,
author = {Louvan, Samuel},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/EXTRACTING THE MAIN CONTENT By Table of Contents{\_}Louvan{\_}2009.pdf:pdf},
number = {August},
title = {{EXTRACTING THE MAIN CONTENT By Table of Contents}},
year = {2009}
}
@article{Henzinger2002a,
author = {Henzinger, M.R. and Motwani, R. and Silverstein, C.},
doi = {10.1145/792550.792553},
file = {:S$\backslash$:/SciencePhdSorted/ACM SIGIR Forum/Challenges in web search engines{\_}Henzinger, Motwani, Silverstein{\_}2002.pdf:pdf;:S$\backslash$:/SciencePhdSorted/ACM SIGIR Forum/Challenges in web search engines{\_}Henzinger, Motwani, Silverstein{\_}2002(2).pdf:pdf},
isbn = {0163-5840},
issn = {01635840},
journal = {ACM SIGIR Forum},
number = {2},
pages = {11--22},
title = {{Challenges in web search engines}},
url = {http://coblitz.codeen.org:3125/citeseer.ist.psu.edu/cache/papers/cs/27191/http:zSzzSzwww.henzinger.comzSzmonikazSzmpaperszSzsigirforum.pdf/henzinger02challenges.pdf{\%}5Cnhttp://dl.acm.org/citation.cfm?id=792553},
volume = {36},
year = {2002}
}
@article{Shindikar2012,
author = {Shindikar, Shubhangi and Nimbalkar, Mv and Deshpande, Anand},
file = {:S$\backslash$:/SciencePhdSorted/Ijser.Org/A Personalized Ontology Model for Web Information Gathering by Domain Specific Search{\_}Shindikar, Nimbalkar, Deshpande{\_}2012.pdf:pdf},
journal = {Ijser.Org},
number = {7},
title = {{A Personalized Ontology Model for Web Information Gathering by Domain Specific Search}},
url = {http://www.ijser.org/researchpaper/A-Personalised-Ontology-Model-for-Web-Information-Gathering-by-Domain-Specific-Search.pdf},
volume = {3},
year = {2012}
}
@article{Korfiatis2012,
abstract = {Online reviews have received much attention recently in the literature, as their visibility has been proven to play an important role during the purchase process. Furthermore, recent theoretical insight argue that the votes casted on how helpful an online review is (review helpfulness) are of particular importance, since they constitute a focal point for examining consumer decision making during the purchase process. In this paper, we explore the interplay between online review helpfulness, rating score and the qualitative characteristics of the review text as measured by readability tests. We construct a theoretical model based on three elements: conformity, understandability and expressiveness and we investigate the directional relationship between the qualitative characteristics of the review text, review helpfulness and the impact of review helpfulness on the review score. Furthermore, we examine whether this relation holds for extreme and moderate review scores. To validate this model we applied four basic readability measures to a dataset containing 37,221 reviews collected from Amazon UK, in order to determine the relationship between the percentage of helpful votes awarded to a review and the review text's stylistic elements. We also investigated the interrelationships between extremely helpful and unhelpful reviews, as well as absolutely positive and negative reviews using intergroup comparisons. We found that review readability had a greater effect on the helpfulness ratio of a review than its length; in addition, extremely helpful reviews received a higher score than those considered less helpful. The present study contributes to the ever growing literature on on-line reviews by showing that readability tests demonstrate a directional relationship with average length reviews and their helpfulness and that this relationship holds both for moderate and extreme review scores. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Korfiatis, Nikolaos and Garc{\'{i}}a-Bariocanal, Elena and S{\'{a}}nchez-Alonso, Salvador},
doi = {10.1016/j.elerap.2011.10.003},
file = {:S$\backslash$:/SciencePhdSorted/Electronic Commerce Research and Applications/Evaluating content quality and helpfulness of online product reviews The interplay of review helpfulness vs. review content{\_}Korfiatis, G.pdf:pdf},
isbn = {1567-4223},
issn = {15674223},
journal = {Electronic Commerce Research and Applications},
keywords = {Product reviews,Readability tests,Review helpfulness,Word of mouth},
number = {3},
pages = {205--217},
publisher = {Elsevier B.V.},
title = {{Evaluating content quality and helpfulness of online product reviews: The interplay of review helpfulness vs. review content}},
url = {http://dx.doi.org/10.1016/j.elerap.2011.10.003},
volume = {11},
year = {2012}
}
@article{Lau2004,
author = {Lau, Kin-nam and Lee, Kam-hon and Ho, Ying and Lam, Pong-yuen},
doi = {10.1057/palgrave.dbm.3240241},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Database Marketing {\&} Customer Strategy Management/Mining the web for business intelligence Homepage analysis in the internet era{\_}Lau et al.{\_}2004.pdf:pdf},
issn = {1741-2439},
journal = {Journal of Database Marketing {\&} Customer Strategy Management},
number = {1},
pages = {32--54},
title = {{Mining the web for business intelligence: Homepage analysis in the internet era}},
url = {http://www.palgrave-journals.com/doifinder/10.1057/palgrave.dbm.3240241},
volume = {12},
year = {2004}
}
@article{Ananieva2016,
author = {Ananieva, Anastasia and Onykiy, Boris and Artamonov, Alexey},
doi = {10.1016/j.procs.2016.07.470},
file = {:S$\backslash$:/SciencePhdSorted/Procedia - Procedia Computer Science/Thematic Thesauruses in Agent Technologies for Scientific and Technical Information Search{\_}Ananieva, Onykiy, Artamonov{\_}2016.pdf:pdf},
issn = {1877-0509},
journal = {Procedia - Procedia Computer Science},
keywords = {agent-based technologies,databases,information analysis systems,table and graph models,thesaurus},
pages = {493--498},
publisher = {The Author(s)},
title = {{Thematic Thesauruses in Agent Technologies for Scientific and Technical Information Search}},
url = {http://dx.doi.org/10.1016/j.procs.2016.07.470},
volume = {88},
year = {2016}
}
@article{Li2012,
abstract = {Use of links to enhance page ranking has been widely studied. The underlying assumption is that links convey recommendations. Although this technique has been used successfully in global web search, it produces poor results for website search, because the majority of the links in a website are used to organize information and convey no recommendations. By distinguishing these two kinds of links, respectively for recommendation and information organization, this paper describes a path-based method for web page ranking. We define the Hierarchical Navigation Path (HNP) as a new resource for improving web search. HNP is composed of multi-step navigation information in visitors' website browsing. It provides indications of the content of the destination page. We first classify the links inside a website. Then, the links for web page organization are exploited to construct the HNPs for each page. Finally, the PathRank algorithm is described for web page retrieval. The experiments show that our approach results in significant improvements over existing solutions. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Li, Jian Qiang and Zhao, Yu and Garcia-Molina, Hector},
doi = {10.1007/s11280-011-0133-5},
file = {:S$\backslash$:/SciencePhdSorted/World Wide Web/A path-based approach for web page retrieval{\_}Li, Zhao, Garcia-Molina{\_}2012.pdf:pdf},
isbn = {1128001101},
issn = {1386145X},
journal = {World Wide Web},
keywords = {navigation path,web information retrieval,web search},
number = {3},
pages = {257--283},
title = {{A path-based approach for web page retrieval}},
volume = {15},
year = {2012}
}
@article{Massimino2016,
abstract = {There is a growing interest in leveraging alternate sources of empirical data, with an increasing emphasis being placed on the Internet. This paper serves as a primer for supply chain management (SCM) researchers that may be interested in leveraging Internet-based sources for their own research, but perhaps not familiar with how to begin. Here, definitions and concepts critical to successful implementation in practice are provided. In addition, concrete, discipline-relevant examples accompany the discussion, and are aided by a fully detailed online code supplement. Performance enhancements are discussed, as well as associated caveats and limitations. Additionally, insights and guidance are offered on the unique responsibilities for researchers to uphold the ethical spirit of scientific research when continuing along these paths. Pragmatic issues related to the application of these techniques are presented for consideration of individual researchers and the SCM community as a whole.},
author = {Massimino, Brett},
doi = {10.1111/jbl.12120},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Business Logistics/Accessing Online Data Web-Crawling and Information-Scraping Techniques to Automate the Assembly of Research Data{\_}Massimino{\_}2016.pdf:pdf},
isbn = {0735-3766},
issn = {21581592},
journal = {Journal of Business Logistics},
keywords = {Internet data,information scraping,programming etiquette,social responsibility,web crawling},
number = {1},
pages = {34--42},
title = {{Accessing Online Data: Web-Crawling and Information-Scraping Techniques to Automate the Assembly of Research Data}},
volume = {37},
year = {2016}
}
@article{Zhang2018,
abstract = {To deal with the turbulent environments, firms have endeavored to achieve greater supply chain collaboration. In researching the antecedents or the conditions that lead to or affect supply chain collaboration, prior studies focus on the use of interorganizational systems (IOS) but simplify or ignore its culture context. Although IOS use is necessary for supply chain collaboration to succeed, organizational culture must be taken into consideration simultaneously. Many supply chain collaborations fail due to incompatible corporate culture and the complexities involved. The objective of the study is to explore the impact of collaborative culture and IOS use on supply chain collaboration by examining a moderated mediation model. Data was collected through a Web survey of U.S. manufacturing firms. Structural equation modeling (LISREL) was used to analyze the data. Following the steps and procedures for implementing the latent variable interaction by orthogonalizing via residual-centered approach, the model was tested. The results indicate that collaborative culture enhances supply chain collaboration directly as well as indirectly by facilitating IOS use, which in turn improves supply chain collaboration. Thus, IOS use partially mediates the relationship between collaborative culture and supply chain collaboration. Surprisingly, the moderating effect of collaborative culture on the relationship between IOS appropriation and supply chain collaboration is not supported.},
author = {Zhang, Qingyu and Cao, Mei},
doi = {10.1016/j.ijpe.2017.10.014},
file = {:S$\backslash$:/SciencePhdSorted/International Journal of Production Economics/Exploring antecedents of supply chain collaboration Effects of culture and interorganizational system appropriation{\_}Zhang, Cao{\_}2018.pdf:pdf},
issn = {09255273},
journal = {International Journal of Production Economics},
keywords = {Culture,Interorganizational systems,Structural equation modeling,Supply chain collaboration},
number = {October 2017},
pages = {146--157},
publisher = {Elsevier Ltd},
title = {{Exploring antecedents of supply chain collaboration: Effects of culture and interorganizational system appropriation}},
url = {https://doi.org/10.1016/j.ijpe.2017.10.014},
volume = {195},
year = {2018}
}
@article{Wang2014,
abstract = {This paper proposes to use random walk (RW) to discover the properties of the deep web data sources that are hidden behind searchable interfaces. The properties, such as the average degree and population size of both documents and terms, are of interests to general public, and find their applications in business intelligence, data integration and deep web crawling. We show that simple RW can outperform the uniform random (UR) samples disregarding the high cost of UR sampling. We prove that in the idealized case when the degrees follow Zipf's law, the sample size of UR sampling needs to grow in the order of O(N/ln 2 N) with the corpus size N, while the sample size of RW sampling grows logarithmically. Reuters corpus is used to demonstrate that the term degrees resemble power law distribution, thus RW is better than UR sampling. On the other hand, document degrees have lognormal distribution and exhibit a smaller variance, therefore UR sampling is slightly better. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Wang, Yan and Liang, Jie and Lu, Jianguo},
doi = {10.1007/s10791-013-9230-7},
file = {:S$\backslash$:/SciencePhdSorted/Information Retrieval/Discover hidden web properties by random walk on bipartite graph{\_}Wang, Liang, Lu{\_}2014.pdf:pdf},
issn = {15737659},
journal = {Information Retrieval},
keywords = {Deep web,Estimator,Graph sampling,Hidden data source,Random walk,Zipf's law},
number = {3},
pages = {203--228},
title = {{Discover hidden web properties by random walk on bipartite graph}},
volume = {17},
year = {2014}
}
@article{Kara2012,
abstract = {In this paper, we present an ontology-based information extraction and retrieval system and its application in the soccer domain. In general, we deal with three issues in semantic search, namely, usability, scalability and retrieval performance. We propose a keyword-based semantic retrieval approach. The performance of the system is improved considerably using domain-specific information extraction, inferencing and rules. Scalability is achieved by adapting a semantic indexing approach and representing the whole world as small independent models. The system is implemented using the state-of-the-art technologies in Semantic Web and its performance is evaluated against traditional systems as well as the query expansion methods. Furthermore, a detailed evaluation is provided to observe the performance gain due to domain-specific information extraction and inferencing. Finally, we show how we use semantic indexing to solve simple structural ambiguities. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Kara, Soner and Alan, ??zg??r and Sabuncu, Orkunt and Akpnar, Samet and Cicekli, Nihan K. and Alpaslan, Ferda N.},
doi = {10.1016/j.is.2011.09.004},
file = {:S$\backslash$:/SciencePhdSorted/Information Systems/An ontology-based retrieval system using semantic indexing{\_}Kara et al.{\_}2012.pdf:pdf},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Keyword-based,Ontology,Query,Semantic indexing,Semantic web},
number = {4},
pages = {294--305},
pmid = {70390772},
title = {{An ontology-based retrieval system using semantic indexing}},
volume = {37},
year = {2012}
}
@article{Saha2010,
author = {Saha, Suman and Murthy, C A and Pal, Sankar K},
doi = {10.3233/WIA-2010-0186},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Application of rough ensemble classifier to web services categorization and focused crawling{\_}Saha, Murthy, Pal{\_}2010.pdf:pdf},
keywords = {focused crawling,rough ensemble classifier,url prediction,web service categorization,wsdl tag structure},
pages = {181--202},
title = {{Application of rough ensemble classifier to web services categorization and focused crawling}},
volume = {8},
year = {2010}
}
@article{Bischof2018,
abstract = {Several institutions collect statistical data about cities, regions, and countries for various purposes. Yet, while access to high quality and recent such data is both crucial for decision makers and a means for achieving transparency to the public, all too often such collections of data remain isolated and not re-useable, let alone comparable or properly integrated. In this paper we present the Open City Data Pipeline, a focused attempt to collect, integrate, and enrich statistical data collected at city level worldwide, and re-publish the resulting dataset in a re-useable manner as Linked Data. The main features of the Open City Data Pipeline are: (i) we integrate and cleanse data from several sources in a modular and extensible, always up-to-date fashion; (ii) we use both Machine Learning techniques and reasoning over equational background knowledge to enrich the data by imputing missing values, (iii) we assess the estimated accuracy of such imputations per indicator. Additionally, (iv) we make the integrated and enriched data, including links to external data sources, such as DBpedia, available both in a web browser interface and as machine-readable Linked Data, using standard vocabularies such as QB and PROV. Apart from providing a contribution to the growing collection of data available as Linked Data, our enrichment process for missing values also contributes a novel methodology for combining rule-based inference about equational knowledge with inferences obtained from statistical Machine Learning approaches. While most existing works about inference in Linked Data have focused on ontological reasoning in RDFS and OWL, we believe that these complementary methods and particularly their combination could be fruitfully applied also in many other domains for integrating Statistical Linked Data, independent from our concrete use case of integrating city data.},
author = {Bischof, Stefan and Harth, Andreas and K{\"{a}}mpgen, Benedikt and Polleres, Axel and Schneider, Patrik},
doi = {10.1016/j.websem.2017.09.003},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Web Semantics/Enriching integrated statistical open city data by combining equational knowledge and missing value imputation{\_}Bischof et al.{\_}2018.pdf:pdf},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Data cleaning,Data integration,Linked Data,Open data},
pages = {1--21},
publisher = {Elsevier B.V.},
title = {{Enriching integrated statistical open city data by combining equational knowledge and missing value imputation}},
url = {https://doi.org/10.1016/j.websem.2017.09.003},
volume = {48},
year = {2018}
}
@article{Garriga2017,
author = {Garriga, Martin and Flores, Andres and Zunino, Alejandro and Cechich, Alejandra and Mateos, Cristian},
doi = {10.1016/j.csi.2016.09.005},
file = {:S$\backslash$:/SciencePhdSorted/Computer Standards {\&} Interfaces/A domain independent readability metric for web service descriptions{\_}Garriga et al.{\_}2017.pdf:pdf},
issn = {0920-5489},
journal = {Computer Standards {\&} Interfaces},
keywords = {Domain independent,Readability,Web service descriptions,WordNet},
number = {June 2016},
pages = {124--141},
publisher = {Elsevier},
title = {{A domain independent readability metric for web service descriptions}},
url = {http://dx.doi.org/10.1016/j.csi.2016.09.005},
volume = {50},
year = {2017}
}
@article{Bermejo2009,
abstract = {SEO optimization},
author = {Bermejo, F.},
doi = {10.1177/1461444808099579},
file = {:S$\backslash$:/SciencePhdSorted/New Media {\&} Society/Audience manufacture in historical perspective from broadcasting to Google{\_}Bermejo{\_}2009.pdf:pdf},
isbn = {1461-4448},
issn = {1461-4448},
journal = {New Media {\&} Society},
number = {1-2},
pages = {133--154},
title = {{Audience manufacture in historical perspective: from broadcasting to Google}},
url = {http://nms.sagepub.com/cgi/doi/10.1177/1461444808099579},
volume = {11},
year = {2009}
}
@article{Domingue2016,
abstract = {The value of big data is predicated on the ability to detect trends and patterns and more generally to make sense of the large volumes of data that is often comprised of a heterogeneous mix of format, structure, and semantics. Big data analysis is the component of the big data value chain that focuses on transforming raw acquired data into a coherent usable resource suitable for analysis. Using a range of interviews with key stakeholders in small and large companies and academia, this chapter outlines key insights, state of the art, emerging trends, future requirements, and sectorial case studies for data analysis.},
author = {Domingue, John and Lasierra, Nelia and Fensel, Anna and Kasteren, Tim Van and Strohbach, Martin and Thalhammer, Andreas},
doi = {10.1007/978-3-319-21569-3},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/New Horizons for a Data-Driven Economy{\_}Domingue et al.{\_}2016.pdf:pdf},
isbn = {978-3-319-21568-6},
title = {{New Horizons for a Data-Driven Economy}},
url = {http://link.springer.com/10.1007/978-3-319-21569-3},
year = {2016}
}
@article{Gori2005,
author = {Gori, Marco and Witten, Ian},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/The Bubble of Web Visibility{\_}Gori, Witten{\_}2005.pdf:pdf},
number = {3},
pages = {115--118},
title = {{The Bubble of Web Visibility}},
volume = {48},
year = {2005}
}
@article{Wang2017,
author = {Wang, Ning and Zeng, Jianping and Ye, Maozhi and Chen, Mingming},
doi = {10.1007/s10586-017-0909-1},
file = {:S$\backslash$:/SciencePhdSorted/Cluster Computing/Text mining and sustainable clusters from unstructured data in cloud computing{\_}Wang et al.{\_}2017.pdf:pdf},
isbn = {1058601709},
issn = {1573-7543},
journal = {Cluster Computing},
keywords = {Infrastructure-as-a-Service (IaaS),Platform-as-a-S,as-a-service,iaas,infrastructure-as-a-service,mining,paas,platform-,saas,security applications,software for text,software-as-a-service,text,tm},
pages = {3--12},
publisher = {Springer US},
title = {{Text mining and sustainable clusters from unstructured data in cloud computing}},
year = {2017}
}
@article{Chau2008,
abstract = {As the Web continues to grow, it has become increasingly difficult to search for relevant information using traditional search engines. Topic-specific search engines provide an alternative way to support efficient information retrieval on the Web by providing more precise and customized searching in various domains. However, developers of topic-specific search engines need to address two issues: how to locate relevant documents (URLs) on the Web and how to filter out irrelevant documents from a set of documents collected from the Web. This paper reports our research in addressing the second issue. We propose a machine-learning-based approach that combines Web content analysis and Web structure analysis. We represent each Web page by a set of content-based and link-based features, which can be used as the input for various machine learning algorithms. The proposed approach was implemented using both a feedforward/backpropagation neural network and a support vector machine. Two experiments were designed and conducted to compare the proposed Web-feature approach with two existing Web page filtering methods - a keyword-based approach and a lexicon-based approach. The experimental results showed that the proposed approach in general performed better than the benchmark approaches, especially when the number of training documents was small. The proposed approaches can be applied in topic-specific search engine development and other Web applications such as Web content management. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Chau, Michael and Chen, Hsinchun},
doi = {10.1016/j.dss.2007.06.002},
file = {:S$\backslash$:/SciencePhdSorted/Decision Support Systems/A machine learning approach to web page filtering using content and structure analysis{\_}Chau, Chen{\_}2008.pdf:pdf},
isbn = {0167-9236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Link analysis,Machine learning,Web mining,Web page classification},
number = {2},
pages = {482--494},
title = {{A machine learning approach to web page filtering using content and structure analysis}},
volume = {44},
year = {2008}
}
@article{Rafiei2015,
abstract = {An online community is a virtual community where people can express their opinions and their knowledge freely. There are a great deal of information in online communities, however there is no way to determine its authenticity. Thus the knowledge which has been shared in online communities is not reliable. By determining expertise level of users and finding experts in online communities the accuracy of posted comments can be evaluated. In this study, a hybrid method for expert finding in online communities is presented which is based on content analysis and social network analysis. The content analysis is based on concept map and the social network analysis is based on PageRank algorithm. To evaluate the proposed method java online community was selected and then correlation between our results and scores prepared by java online community was calculated. Based on obtained results Spearman correlation for 11 subcategories of java online community using this method is 0.904, which is highly an acceptable value.},
author = {Rafiei, Majid and Kardan, Ahmad A},
doi = {10.1186/s13673-015-0030-5},
file = {:S$\backslash$:/SciencePhdSorted/Human-centric Computing and Information Sciences/A novel method for expert finding in online communities based on concept map and PageRank{\_}Rafiei, Kardan{\_}2015.pdf:pdf},
isbn = {2192-1962},
issn = {2192-1962},
journal = {Human-centric Computing and Information Sciences},
keywords = {Expert finding,Online community,Concept map,Dijkst,concept map,dijkstra,expert finding,knowledge sharing,online community,pagerank algorithm,s algorithm},
number = {1},
pages = {10},
title = {{A novel method for expert finding in online communities based on concept map and PageRank}},
url = {http://www.hcis-journal.com/content/5/1/10},
volume = {5},
year = {2015}
}
@article{Isik2011,
abstract = {Business intelligence (BI) has become the top priority for many organizations who have implemented BI solutions to improve their decision-making process. Yet, not all BI initiatives have fulfilled the expectations.We suggest that one of the reasons for failure is the lack of an understanding of the critical factors that define the success of BI applications, and that BI capabilities are among those critical factors. We present findings from a survey of 116 BI professionals that provides a snapshot of user satisfaction with various BI capabilities and the relationship be- tween these capabilities and user satisfaction with BI. Our findings suggest that users are generally satisfied with BI overall and with BI capabilities. However, the BI capabilities with which they are most satisfied are not necessarily the ones that are the most strongly related to BI success. Of the five capabilities that were the most highly correlated with overall satisfaction with BI, only one was specifically related to data. Another interesting finding implies that, although users are not highly satisfied with the level of interaction of BI with other systems, this capability is highly correlated with BI success. Implications of these findings for the successful use and management of BI are dis- cussed. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd. Keywords:},
archivePrefix = {arXiv},
arxivId = {9782951740884},
author = {Isik, Oyku and Jones, Mc and Sidorova, Anna},
doi = {10.1002/isaf},
eprint = {9782951740884},
file = {:S$\backslash$:/SciencePhdSorted/Intelligent systems in accounting, finance and management/Business intelligence (BI) success and the role of BI capabilities{\_}Isik, Jones, Sidorova{\_}2011.pdf:pdf},
isbn = {1550-1949},
issn = {1055615X},
journal = {Intelligent systems in accounting, finance and management},
keywords = {bi,bi capabilities,bi satisfaction,bi success,business intelligence},
number = {January},
pages = {161--176},
pmid = {1321731390},
title = {{Business intelligence (BI) success and the role of BI capabilities}},
volume = {176},
year = {2011}
}
@article{Duan2012,
abstract = {Abstract—Business intelligence (BI) is the process of transforming raw data into useful information for more effective strategic, operational insights, and decision-making purposes so that it yields real business benefits. This new emerging technique can not only improve applications in enterprise systems and industrial informatics, respectively, but also play a very important role to bridge the connection between enterprise systems and industrial informatics. This paper was intended as a short introduction to BI with the emphasis on the fundamental algorithms and recent progress. In addition, we point out the challenges and opportunities to smoothly connect industrial informatics to enterprise systems for BI research.},
author = {Duan, Lian and Xu, Li Da},
doi = {10.1109/TII.2012.2188804},
file = {:S$\backslash$:/SciencePhdSorted/IEEE Transactions on Industrial Informatics/Business intelligence for enterprise systems A survey{\_}Duan, Xu{\_}2012.pdf:pdf},
isbn = {1551-3203},
issn = {15513203},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {Business intelligence (BI),data mining,enterprise systems,industrial informatics},
number = {3},
pages = {679--687},
title = {{Business intelligence for enterprise systems: A survey}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6156777{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6156777},
volume = {8},
year = {2012}
}
@article{Marrese-Taylor2013,
abstract = {In this study we extend Bing Liu's aspect-based opinion mining technique to apply it to the tourism domain. Using this extension, we also offer an approach for considering a new alternative to discover consumer preferences about tourism products, particularly hotels and restaurants, using opinions available on the Web as reviews. An experiment is also conducted, using hotel and restaurant reviews obtained from TripAdvisor, to evaluate our proposals. Results showed that tourism product reviews available on web sites contain valuable information about customer preferences that can be extracted using an aspect-based opinion mining approach. The proposed approach proved to be very effective in determining the sentiment orientation of opinions, achieving a precision and recall of 90{\%}. However, on average, the algorithms were only capable of extracting 35{\%} of the explicit aspect expressions. {\textcopyright} 2013 The Authors.},
author = {Marrese-Taylor, Edison and Vel{\'{a}}squez, Juan D. and Bravo-Marquez, Felipe and Matsuo, Yutaka},
doi = {10.1016/j.procs.2013.09.094},
file = {:S$\backslash$:/SciencePhdSorted/Procedia Computer Science/Identifying customer preferences about tourism products using an aspect-based opinion mining approach{\_}Marrese-Taylor et al.{\_}2013.pdf:pdf},
isbn = {1877-0509},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Aspect-based,Customer preferences,Natural language processing,Opinion mining,Tourism,Web mining},
pages = {182--191},
title = {{Identifying customer preferences about tourism products using an aspect-based opinion mining approach}},
volume = {22},
year = {2013}
}
@article{Spangler2009,
author = {Spangler, Scott and Chen, Ying and Proctor, Larry and Lelescu, Ana and Behal, Amit and He, Bin},
doi = {10.3233/WIA-2009-0166},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/COBRA – mining web for COrporate Brand and Reputation Analysis{\_}Spangler et al.{\_}2009.pdf:pdf},
keywords = {amount of information for,at the overall stack,called cobra,cally integrated brand and,classification instead of looking,corporate,exist for mining the,holisti-,however,insights,mines cgm contents for,narrow problems such as,of technologies required for,often result in excessive,reputation analysis solution that,search based technologies that,some,text mining based technologies,they often target very,this paper describes a,users to digest manually,web,web mining,web page},
pages = {243--254},
title = {{COBRA – mining web for COrporate Brand and Reputation Analysis}},
volume = {7},
year = {2009}
}
@article{Rasekh2015,
author = {Rasekh, Iman},
doi = {10.1016/j.procs.2015.08.505},
file = {:S$\backslash$:/SciencePhdSorted/Procedia - Procedia Computer Science/A New Competitive Intelligence-Based Strategy for Web Page Search{\_}Rasekh{\_}2015.pdf:pdf},
issn = {1877-0509},
journal = {Procedia - Procedia Computer Science},
keywords = {folksonomy,ica,linked based ica algorithm,linked based page ranking,semantic webs},
number = {Scse},
pages = {450--456},
publisher = {Elsevier Masson SAS},
title = {{A New Competitive Intelligence-Based Strategy for Web Page Search}},
url = {http://dx.doi.org/10.1016/j.procs.2015.08.505},
volume = {62},
year = {2015}
}
@article{Edwards2015,
author = {Edwards, M and Rashid, A and Rayson, P},
file = {:S$\backslash$:/SciencePhdSorted/ACM Computing Surveys (CSUR)/A systematic survey of online data mining technology intended for law enforcement{\_}Edwards, Rashid, Rayson{\_}2015.pdf:pdf},
journal = {ACM Computing Surveys (CSUR)},
number = {1},
title = {{A systematic survey of online data mining technology intended for law enforcement}},
url = {http://dl.acm.org/citation.cfm?id=2811403},
volume = {48},
year = {2015}
}
@article{Goldenberg2012,
abstract = {Online content and products are presented as product networks, in which nodes are product pages linked by hyperlinks. These links are typically algorithmically induced recommendations based on aggregated data. Recently, websites have begun to offer social networks and user-generated links alongside the product network, creating a dual-network structure. The authors investigate the role of this dual-network structure in facilitating content exploration. They analyze YouTube's dual network and show that user pages have unique structural properties and act as content brokers. Next, the authors show that random rewiring of the product network cannot replicate this brokering effect. They present seven Internet studies in which participants browsing a YouTube-based website are exposed to different conditions of recommendations. The first set of studies shows that exposure to the dual network results in a more efficient (time to desirable outcome) and more effective (average product rating, overall satisfaction) exploration process. The next set of studies extends the previous ones to include dynamic structures, in which the network changes as a function of time or in response to participants' satisfaction. Furthermore, the results are replicated using data from another content site. [ABSTRACT FROM AUTHOR]$\backslash$nCopyright of Journal of Marketing Research (JMR) is the property of American Marketing Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Goldenberg, Jacob and Oestreicher-Singer, Gal and Reichman, Shachar},
doi = {10.1509/jmr.11.0091},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Marketing Research (JMR)/The Quest for Content How User-Generated Links Can Facilitate Online Exploration{\_}Goldenberg, Oestreicher-Singer, Reichman{\_}2012.pdf:pdf},
isbn = {00222437},
issn = {00222437},
journal = {Journal of Marketing Research (JMR)},
keywords = {and participants at the,and the,barak libai,brynjolfsson,dina mayzlin and eitan,e-commerce,generated content conference,helpful discussions,information systems conferences,marketing science conference,muller for many,nicholas economides,of user-,product network,social networks,the authors thank erik,the emergence and impact,the international conference on,user generated content},
number = {4},
pages = {452--468},
pmid = {78191683},
title = {{The Quest for Content: How User-Generated Links Can Facilitate Online Exploration}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=bth{\&}AN=78191683{\&}lang=zh-cn{\&}site=ehost-live},
volume = {49},
year = {2012}
}
@article{Martinez-Torres2011,
abstract = {Purpose – Web sites are typically designed attending to a variety of criteria. However, web site structure determines browsing behavior and way-finding results. The aim of this study is to identify the main profiles of web sites' organizational structure by modeling them as graphs and considering several social network analysis features. Design/methodology/approach – Acase study based on 80 institutional Spanish universities' web sites has been used for this purpose. For each root domain, two different networks have been considered: the first is the domain network, and the second is the page network. In both cases, several indicators related to social network analysis have been evaluated to characterize the web site structure. Factor analysis provides the statistical methodology to adequately extract the main web site profiles in terms of their internal structure. Findings – This paper allows the categorization of web site design styles and provides general guidelines to assist designers to better identify areas for creating and improving institutional web sites. The findings of this study offer practical implications to web site designers for creating and maintaining an effective web presence, and for improving usability. Research limitations/implications – The research is limited to 80 institutional Spanish universities' web sites. Other institutional university web sites from different countries can be analyzed, and the conclusions could be compared or enlarged. Originality/value – This paper highlights the importance of the internal web sites structure, and their implications on usability and way-finding results. As a difference to previous research, the paper is focused on the comparison of internal structure of institutional web sites, rather than analyzing the web as a whole or the interrelations among web sites.},
author = {Mart{\'{i}}nez-Torres, M.R. and Toral, SL},
doi = {10.1108/10662241111123711},
file = {:S$\backslash$:/SciencePhdSorted/Internet {\ldots}/Web site structure mining using social network analysis{\_}Mart{\'{i}}nez-Torres, Toral{\_}2011.pdf:pdf},
isbn = {1066224111112},
issn = {1066-2243},
journal = {Internet {\ldots}},
number = {2},
pages = {104--123},
title = {{Web site structure mining using social network analysis}},
url = {http://www.emeraldinsight.com/10.1108/10662241111123711{\%}5Cnhttp://www.emeraldinsight.com/journals.htm?articleid=1915571{\&}show=abstract},
volume = {21},
year = {2011}
}
@article{Nogales2017,
author = {Nogales, Alberto and Sicilia-urban, Miguel Angel and Garc{\'{i}}a-barriocanal, Elena and Nogales, Alberto and Sicilia-urban, Miguel Angel and Garc{\'{i}}a-barriocanal, Elena and Nogales, Alberto and Sicilia-urban, Miguel Angel and Garc{\'{i}}a-barriocanal, Elena},
doi = {10.1108/OIR-06-2015-0183},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Measuring vocabulary use in the Linked Data Cloud{\_}Nogales et al.{\_}2017.pdf:pdf},
title = {{Measuring vocabulary use in the Linked Data Cloud}},
year = {2017}
}
@article{Ali2015a,
abstract = {The volume of traveling websites is rapidly increasing. This makes relevant information extraction more challenging. Several fuzzy ontology-based systems have been proposed to decrease the manual work of a full-text query search engine and opinion mining. However, most search engines are keyword-based, and available full-text search engine systems are still imperfect at extracting precise information using different types of user queries. In opinion mining, travelers do not declare their hotel opinions entirely but express individual feature opinions in reviews. Hotel reviews have numerous uncertainties, and most featured opinions are based on complex linguistic wording (small, big, very good and very bad). Available ontology-based systems cannot extract blurred information from reviews to provide better solutions. To solve these problems, this paper proposes a new extraction and opinion mining system based on a type-2 fuzzy ontology called T2FOBOMIE. The system reformulates the user's full-text query to extract the user requirement and convert it into the format of a proper classical full-text search engine query. The proposed system retrieves targeted hotel reviews and extracts feature opinions from reviews using a fuzzy domain ontology. The fuzzy domain ontology, user information and hotel information are integrated to form a type-2 fuzzy merged ontology for the retrieving of feature polarity and individual hotel polarity. The Prot{\'{e}}g{\'{e}} OWL-2 (Ontology Web Language) tool is used to develop the type-2 fuzzy ontology. A series of experiments were designed and demonstrated that T2FOBOMIE performance is highly productive for analyzing reviews and accurate opinion mining.},
author = {Ali, Farman and Kim, Eun Kyoung and Kim, Yong Gi},
doi = {10.1007/s10489-014-0609-y},
file = {:S$\backslash$:/SciencePhdSorted/Applied Intelligence/Type-2 fuzzy ontology-based opinion mining and information extraction A proposal to automate the hotel reservation system{\_}Ali, Kim, Kim{\_}.pdf:pdf},
isbn = {doi:10.1007/s10489-014-0609-y},
issn = {0924669X},
journal = {Applied Intelligence},
keywords = {Information extraction,Ontology merging and ontology evaluation,Opinion mining,Type-2 fuzzy ontology},
number = {3},
pages = {481--500},
title = {{Type-2 fuzzy ontology-based opinion mining and information extraction: A proposal to automate the hotel reservation system}},
volume = {42},
year = {2015}
}
@article{Varlamov2016,
author = {Varlamov, M. I. and Turdakov, D. Yu.},
doi = {10.1134/S0361768816050078},
file = {:S$\backslash$:/SciencePhdSorted/Programming and Computer Software/A survey of methods for the extraction of information from Web resources{\_}Varlamov, Turdakov{\_}2016.pdf:pdf},
issn = {0361-7688},
journal = {Programming and Computer Software},
number = {5},
pages = {279--291},
title = {{A survey of methods for the extraction of information from Web resources}},
url = {http://link.springer.com/10.1134/S0361768816050078},
volume = {42},
year = {2016}
}
@article{Qiu2007,
abstract = {As the enterprise Informatization process goes through deep, an increasing number of enterprises deploy their web sites on the internet, making the web sites become a brand of the enterprise. Thus, it is very necessary to evaluate the quality for enterprise web sites. However, almost the total current website evaluation methods have ignored the external web influence factors of quantitative evaluation and can't work effectively. In this paper, we employ the weblink analysis method to evaluate the web influence for enterprise web sites, combing 17 link analysis indicators obtained by Google and AltaVista. We then explored the relationship between the web sites type and web influence and discussed the performance of the link indicators and link analysis tools. The whole analysis is based on our experiments, using the data accepted from 2006 Top 500 companies list of China and 2006 Fortune 500 companies list in the world.},
author = {Qiu, J and Ren, Q},
file = {:S$\backslash$:/SciencePhdSorted/IFIP Advances in Information and Communication Technology/Link analysis and web influence evaluation for enterprise websites{\_}Qiu, Ren{\_}2007.pdf:pdf},
isbn = {9780387754659},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
pages = {300--308},
title = {{Link analysis and web influence evaluation for enterprise websites}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84901761812{\&}partnerID=40{\&}md5=2f56e0bab8b70451cc48865d2cbb7d44},
volume = {251 VOLUME},
year = {2007}
}
@article{Sieloff2003,
author = {Sieloff, Steve and Corporation, Acxiom and Rock, Little},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/DATA EXTRACTION FROM REPOSITORIES ON THE WEB A SEMI-AUTOMATIC APPROACH{\_}Sieloff, Corporation, Rock{\_}2003.pdf:pdf},
keywords = {data extraction,web mining},
number = {4},
pages = {13--23},
title = {{DATA EXTRACTION FROM REPOSITORIES ON THE WEB : A SEMI-AUTOMATIC APPROACH}},
volume = {7},
year = {2003}
}
@article{Saleh2017,
author = {Saleh, Ahmed I and Abulwafa, Arwa E and Al, Mohammed F},
doi = {10.1016/j.asoc.2016.12.028},
file = {:S$\backslash$:/SciencePhdSorted/Applied Soft Computing Journal/A web page distillation strategy for efficient focused crawling based on optimized Na{\"{i}}ve bayes ( ONB ) classifier{\_}Saleh, Abulwafa, Al{\_}20.pdf:pdf},
issn = {1568-4946},
journal = {Applied Soft Computing Journal},
keywords = {wen page classification},
pages = {181--204},
publisher = {Elsevier B.V.},
title = {{A web page distillation strategy for efficient focused crawling based on optimized Na{\"{i}}ve bayes ( ONB ) classifier}},
url = {http://dx.doi.org/10.1016/j.asoc.2016.12.028},
volume = {53},
year = {2017}
}
@article{Papagiannidis2017,
abstract = {In this paper we propose using a novel big-data-mining methodology and the Internet as a new source of useful meta-data for industry classification. The proposed methodology can be utilised as a decision support system for identifying industrial clusters in almost real time in a specific geographic region, contributing to strategic co-operation and policy development for operations and supply chain management across organisational boundaries through big data analytics. Our theoretical discussion on discerning industrial activity of firms in geographical regions starts by highlighting the limitations of the Standard Industrial Classification (SIC) codes. This discussion is followed by the proposed methodology, which has three main steps revolving around web-based data collection, pre-processing and analysis, and reporting of clusters. We discuss each step in detail, presenting the experimental approaches tested. We apply our methodology to a regional case, in the North East of England, in order to demonstrate how such a big data decision support system/analytics can work in practice. Implications for theory, policy and practice are discussed, as well as potential avenues for further research.},
author = {Papagiannidis, Savvas and See-To, Eric W.K. and Assimakopoulos, Dimitris G. and Yang, Yang},
doi = {10.1016/j.cor.2017.06.010},
file = {:S$\backslash$:/SciencePhdSorted/Computers and Operations Research/Identifying industrial clusters with a novel big-data methodology Are SIC codes (not) fit for purpose in the Internet age{\_}Papagiannidis.pdf:pdf},
isbn = {0305-0548},
issn = {03050548},
journal = {Computers and Operations Research},
keywords = {Big data analytics,Clusters,Industry classification,North East of England,Operations,Regional policy,SIC codes,Strategic co-operation},
pages = {1--12},
publisher = {Elsevier Ltd},
title = {{Identifying industrial clusters with a novel big-data methodology: Are SIC codes (not) fit for purpose in the Internet age?}},
url = {http://dx.doi.org/10.1016/j.cor.2017.06.010},
volume = {0},
year = {2017}
}
@article{Vieira2009,
abstract = {Templates are pieces of HTML code common to a set of web pages usually adopted by content providers to enhance the uniformity of layout and navigation of theirs Web sites. They are usually generated using authoring/publishing tools or by programs that build HTML pages to publish content from a database. In spite of their usefulness, the content of templates can negatively affect the quality of results produced by systems that automatically process information available in web sites, such as search engines, clustering and automatic categorization programs. Further, the information available in templates is redundant and thus processing and storing such information just once for a set of pages may save computational resources. In this paper, we present and evaluate methods for detecting templates considering a scenario where multiple templates can be found in a collection of Web pages. Most of previous work have studied template detection algorithms in a scenario where the collection has just a single template. The scenario with multiple templates is more realistic and, as it is discussed here, it raises important questions that may require extensions and adjustments in previously proposed template detection algorithms. We show how to apply and evaluate two template detection algorithms in this scenario, creating solutions for detecting multiple templates. The methods studied partitions the input collection into clusters that contain common HTML paths and share a high number of HTML nodes and then apply a single-template detection procedure over each cluster. We also propose a new algorithm for single template detection based on a restricted form of bottom-up tree-mapping that requires only small set of pages to correctly identify a template and which has a worst-case linear complexity. Our experimental results over a representative set of Web pages show that our approach is efficient and scalable while obtaining accurate results.},
author = {Vieira, Karane and {da Costa Carvalho}, Andr{\'{e}} Luiz and Berlt, Klessius and de Moura, Edleno S. and da Silva, Altigran S. and Freire, Juliana},
doi = {10.1007/s11280-009-0059-3},
file = {:S$\backslash$:/SciencePhdSorted/World Wide Web/On finding templates on web collections{\_}Vieira et al.{\_}2009.pdf:pdf},
isbn = {1386-145X},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Tree-mapping,Web IR,Web template detection},
number = {2},
pages = {171--211},
title = {{On finding templates on web collections}},
volume = {12},
year = {2009}
}
@article{Ye2017,
author = {Ye, Minjian and Li, Guangzhong},
doi = {10.1186/s40854-017-0056-y},
file = {:S$\backslash$:/SciencePhdSorted/Financial Innovation/Internet big data and capital markets a literature review{\_}Ye, Li{\_}2017.pdf:pdf},
issn = {2199-4730},
journal = {Financial Innovation},
keywords = {Internet big data,Financial studies,Capital market,capital markets,financial studies,internet big data},
number = {1},
pages = {6},
publisher = {Financial Innovation},
title = {{Internet big data and capital markets: a literature review}},
url = {http://jfin-swufe.springeropen.com/articles/10.1186/s40854-017-0056-y},
volume = {3},
year = {2017}
}
@article{Kumar2015a,
author = {Kumar, Sarowar and Abhishek, Kumar and Singh, M P},
doi = {10.1016/j.procs.2015.06.052},
file = {:S$\backslash$:/SciencePhdSorted/Procedia - Procedia Computer Science/Accessing Relevant and Accurate Information using Entropy{\_}Kumar, Abhishek, Singh{\_}2015.pdf:pdf},
issn = {1877-0509},
journal = {Procedia - Procedia Computer Science},
keywords = {a large number of,algorithm,and compares the result,any given query,based search engine generally,collection of web,finding accurate results for,for a given set,hypertext document,mostly irrelevant,of query with less,provides result set with,response time,semantic-synaptic web mining algorithm,so effort is required,the algorithm focuses on,the world wide,this paper implements the,to provide relevant data,use of entropy for,web is a large,web pages,with other existing},
pages = {449--455},
publisher = {Elsevier Masson SAS},
title = {{Accessing Relevant and Accurate Information using Entropy}},
url = {http://dx.doi.org/10.1016/j.procs.2015.06.052},
volume = {54},
year = {2015}
}
@article{Brunelle2015,
author = {Brunelle, Justin F and Kelly, Mat and Salaheldeen, Hany and Weigle, Michele C and Nelson, Michael L},
doi = {10.1007/s00799-015-0150-6},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Not all mementos are created equal measuring the impact of missing resources{\_}Brunelle et al.{\_}2015.pdf:pdf},
keywords = {digital preservation,memento damage,web architecture,web archiving},
pages = {283--301},
title = {{Not all mementos are created equal : measuring the impact of missing resources}},
year = {2015}
}
@article{Wang2013,
abstract = {In this paper, we develop SumView, a Web-based review summarization system, to automatically extract the most representative expressions and customer opinions in the reviews on various product features. Different from existing review analysis which makes more efforts on sentiment classification and opinion mining, our system mainly focuses on summarization, i.e., delivering the majority of information contained in the review documents by selecting the most representative review sentences for each extracted product feature. Comprehensive case studies and experiments demonstrate the effectiveness of our system, and the user study shows users' satisfaction. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Wang, Dingding and Zhu, Shenghuo and Li, Tao},
doi = {10.1016/j.eswa.2012.05.070},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/SumView A Web-based engine for summarizing product reviews and customer opinions{\_}Wang, Zhu, Li{\_}2013.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Matrix factorization,Opinion mining,Product review,Summarization},
number = {1},
pages = {27--33},
publisher = {Elsevier Ltd},
title = {{SumView: A Web-based engine for summarizing product reviews and customer opinions}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.05.070},
volume = {40},
year = {2013}
}
@article{Olbrich2012,
author = {Olbrich, Rainer and Holsing, Christian},
doi = {10.2753/JEC1086-4415160202},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Modeling Consumer Purchasing Behavior in Social Shopping Communities with Clickstream Data{\_}Olbrich, Holsing{\_}2012.pdf:pdf},
isbn = {1086441516},
keywords = {0 is rapidly moving,2,30,77,a,and phrases,as a shopping channel,clickstream data,importance of the internet,online consumer purchasing behavior,parallel to the increasing,social shopping,the advent of web,the online landscape into,user-generated content,virtual community},
number = {2},
pages = {15--40},
title = {{Modeling Consumer Purchasing Behavior in Social Shopping Communities with Clickstream Data}},
volume = {16},
year = {2012}
}
@article{Yang2014a,
abstract = {ABSTRACT Mergers and acquisitions (M{\&}A) play increasingly important roles for contemporary business, especially in high-tech industries that conduct M{\&}As to pursue complementarity from other companies and thereby preserve or extend their competitive advantages. The appropriate selection (prediction) of M{\&}A targets for a given bidder company constitutes a critical first step for an effective technology M{\&}A activity. Yet existing studies only employ financial and managerial indicators when constructing M{\&}A prediction models, and select candidate target companies without considering the profile of the bidder company or its technological compatibility with candidate target companies. Such limitations greatly restrict the applicability of existing studies to supporting technology M{\&}A predictions. To address these limitations, we propose a technology M{\&}A prediction technique that encompasses technological indicators as independent variables and accounts for the technological profiles of both bidder and candidate target companies. Forty-three technological indicators are derived from patent documents and an ensemble learning method is developed for our proposed technology M{\&}A prediction technique. Our evaluation results, on the basis of the M{\&}A cases between January 1997 and May 2008 that involve companies in Japan and Taiwan, confirm the viability and applicability of the proposed technology M{\&}A prediction technique. In addition, our evaluation also suggests that the incorporation of the technological profiles and compatibility of both bidder and candidate target companies as predictors significantly improves the effectiveness of relevant predictions. [ABSTRACT FROM AUTHOR]},
author = {Yang, Chin Sheng and Wei, Chih Ping and Chiang, Yu Hsun},
doi = {10.1111/deci.12062},
file = {:S$\backslash$:/SciencePhdSorted/Decision Sciences/Exploiting technological indicators for effective technology merger and acquisition (M{\&}A) predictions{\_}Yang, Wei, Chiang{\_}2014.pdf:pdf},
isbn = {0011-7315},
issn = {00117315},
journal = {Decision Sciences},
keywords = {Data Mining,Ensemble Learning,M{\&}A Prediction,Mergers and Acquisitions (M{\&}A),Patent Mining,Technological Indicators,Technology M{\&}A},
number = {1},
pages = {147--174},
title = {{Exploiting technological indicators for effective technology merger and acquisition (M{\&}A) predictions}},
volume = {45},
year = {2014}
}
@article{Ferrara2014,
abstract = {Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of applications. Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. This survey aims at providing a structured and comprehensive overview of the literature in the field of Web Data Extraction. We provided a simple classification framework in which existing Web Data Extraction applications are grouped into two main classes, namely applications at the Enterprise level and at the Social Web level. At the Enterprise level, Web Data Extraction techniques emerge as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. At the Social Web level, Web Data Extraction techniques allow to gather a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities to analyze human behavior at a very large scale. We discuss also the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains.},
archivePrefix = {arXiv},
arxivId = {1207.0246},
author = {Ferrara, Emilio and {De Meo}, Pasquale and Fiumara, Giacomo and Baumgartner, Robert},
doi = {10.1016/j.knosys.2014.07.007},
eprint = {1207.0246},
file = {:S$\backslash$:/SciencePhdSorted/Knowledge-Based Systems/Web data extraction, applications and techniques A survey{\_}Ferrara et al.{\_}2014.pdf:pdf},
isbn = {978-1-4244-5678-9},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Business intelligence,Information retrieval,Knowledge engineering,Knowledge-based systems,Web data mining,Web information extraction},
pages = {301--323},
pmid = {18244404},
publisher = {Elsevier B.V.},
title = {{Web data extraction, applications and techniques: A survey}},
url = {http://dx.doi.org/10.1016/j.knosys.2014.07.007},
volume = {70},
year = {2014}
}
@article{Opalinski2015,
abstract = {The paper contains the concept of agent-based search system and monitoring of Web pages. It is oriented at the exploration of limited problem area, covering a given sector of industry or economy. The proposal of agent-based (modular) structure of the system is due to the desire to ease the introduction of modifications or enrichment of its functionality. Commonly used search engines do not offer such a feature. The second part of the article presents a pilot version of the WEB mining system, represent- ing a simplified implementation of the previously presented concept. Testing of the implemented application was executed by referring to the problem area of foundry industry.},
author = {Opalinski, Andrzej and Nawarecki, Edward and Kluska-Nawarecka, Stanislawa},
doi = {10.1016/j.procs.2015.05.263},
file = {:S$\backslash$:/SciencePhdSorted/Procedia Computer Science/Agent-based Approach to WEB Exploration Process{\_}Opalinski, Nawarecki, Kluska-Nawarecka{\_}2015.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Agent system,Assessment of the adequacy,Monitoring,Patterns of web pages,Web mining},
pages = {1052--1061},
publisher = {Elsevier Masson SAS},
title = {{Agent-based Approach to WEB Exploration Process}},
url = {http://dx.doi.org/10.1016/j.procs.2015.05.263 http://www.sciencedirect.com/science/article/pii/S1877050915010716},
volume = {51},
year = {2015}
}
@article{Kovacevic2008a,
author = {Kovacevic, Milos and Nie, Jian-yun and Davidson, Colin},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Providing Answers to Questions from Automatically Collected Web Pages for Intelligent Decision Making in the Construction Sector{\_}Kovacev.pdf:pdf},
number = {February},
pages = {3--14},
title = {{Providing Answers to Questions from Automatically Collected Web Pages for Intelligent Decision Making in the Construction Sector}},
year = {2008}
}
@article{Peng2006,
author = {Peng, Tao and He, Fengling and Zuo, Wanli and Zhang, Changli},
file = {:S$\backslash$:/SciencePhdSorted/Advances in Artificial Intelligence/Adaptive Topical Web Crawling for Domain-Specific Resource Discovery Guided by Link-Context{\_}Peng et al.{\_}2006.pdf:pdf},
isbn = {3540490264},
issn = {03029743},
journal = {Advances in Artificial Intelligence},
pages = {963--973},
title = {{Adaptive Topical Web Crawling for Domain-Specific Resource Discovery Guided by Link-Context}},
year = {2006}
}
@article{FossoWamba2017,
author = {{Fosso Wamba}, Samuel},
doi = {10.1108/BPMJ-02-2017-0046},
file = {:S$\backslash$:/SciencePhdSorted/Business Process Management Journal/Big data analytics and business process innovation{\_}Fosso Wamba{\_}2017.pdf:pdf},
issn = {1463-7154},
journal = {Business Process Management Journal},
number = {3},
pages = {BPMJ--02--2017--0046},
title = {{Big data analytics and business process innovation}},
url = {http://www.emeraldinsight.com/doi/10.1108/BPMJ-02-2017-0046},
volume = {23},
year = {2017}
}
@article{Chen2014,
abstract = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
file = {:S$\backslash$:/SciencePhdSorted/Mobile Networks and Applications/Big data A survey{\_}Chen, Mao, Liu{\_}2014.pdf:pdf},
isbn = {1383-469X},
issn = {1383469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
number = {2},
pages = {171--209},
pmid = {1511170304},
title = {{Big data: A survey}},
volume = {19},
year = {2014}
}
@article{Bukhari2012,
abstract = {As the internet grows rapidly, millions of web pages are being added on a daily basis. The extraction of precise information is becoming more and more difficult as the volume of data on the internet increases. Several search engines and information fetching tools are available on the internet, all of which claim to provide the best crawling facilities. For the most part, these search engines are keyword based. This poses a problem for visually impaired people who want to get the full use from online resources available to other users. Visually impaired users require special aid to get along with any given computer system. Interface and content management are no exception, and special tools are required to facilitate the extraction of relevant information from the internet for visually impaired users. The {\{}HOIEV{\}} {\{}(Heavyweight{\}} Ontology Based Information Extraction for Visually impaired User) architecture provides a mechanism for highly precise information extraction using heavyweight ontology and built-in vocal command system for visually impaired internet users. Our prototype intelligent system not only integrates and communicates among different tools, such as voice command parsers, domain ontology extractors and short message engines, but also introduces an autonomous mechanism of information extraction {\{}(IE){\}} using heavyweight ontology. In this research we designed domain specific heavyweight ontology using {\{}OWL{\}} 2 {\{}(Web{\}} Ontology Language 2) and for axiom writing we used {\{}PAL{\}} {\{}(Prot{\}},g, Axiom Language). We introduced a novel autonomous mechanism for {\{}IE{\}} by developing prototype software. A series of experiments were designed for the testing and analysis of the performance of heavyweight ontology in general, and our information extraction prototype specifically.},
author = {Bukhari, Ahmad C. and Kim, Yong Gi},
doi = {10.1007/s10462-011-9238-6},
file = {:S$\backslash$:/SciencePhdSorted/Artificial Intelligence Review/Ontology-assisted automatic precise information extractor for visually impaired inhabitants{\_}Bukhari, Kim{\_}2012.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {E-Store ontology,Heavyweight ontology for IE,Hybrid technique for ontology modeling,IE for blind user,Intelligent information extraction},
number = {1},
pages = {9--24},
title = {{Ontology-assisted automatic precise information extractor for visually impaired inhabitants}},
volume = {38},
year = {2012}
}
@article{Gu2016,
author = {Gu, Zhenyu and Lou, Jian},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Computer-Aided Design Data driven webpage color design{\_}Gu, Lou{\_}2016.pdf:pdf},
pages = {46--59},
title = {{Computer-Aided Design Data driven webpage color design}},
volume = {77},
year = {2016}
}
@article{Uzun2013,
abstract = {Eliminating noisy information and extracting informative content have become important issues for web mining, search and accessibility. This extraction process can employ automatic techniques and hand-crafted rules. Automatic extraction techniques focus on various machine learning methods, but implementing these techniques increases time complexity of the extraction process. Conversely, extraction through hand-crafted rules is an efficient technique that uses string manipulation functions, but preparing these rules is difficult and cumbersome for users. In this paper, we present a hybrid approach that contains two steps that can invoke each other. The first step discovers informative content using Decision Tree Learning as an appropriate machine learning method and creates rules from the results of this learning method. The second step extracts informative content using rules obtained from the first step. However, if the second step does not return an extraction result, the first step gets invoked. In our experiments, the first step achieves high accuracy with 95.76{\%} in extraction of the informative content. Moreover, 71.92{\%} of the rules can be used in the extraction process, and it is approximately 240 times faster than the first step. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Uzun, Erdin?? and Agun, Hayri Volkan and Yerlikaya, Tarik},
doi = {10.1016/j.ipm.2013.02.005},
file = {:S$\backslash$:/SciencePhdSorted/Information Processing and Management/A hybrid approach for extracting informative content from web pages{\_}Uzun, Agun, Yerlikaya{\_}2013.pdf:pdf},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Template Detection,Web Cleaning,Web Content Extraction,Web Learning Modeling},
number = {4},
pages = {928--944},
publisher = {Elsevier Ltd},
title = {{A hybrid approach for extracting informative content from web pages}},
url = {http://dx.doi.org/10.1016/j.ipm.2013.02.005},
volume = {49},
year = {2013}
}
@article{Guo2017,
author = {Guo, Liang and Sharma, Ruchi and Yin, Lei and Lu, Ruodan and Rong, Ke},
doi = {10.1108/BPMJ-05-2015-0065},
file = {:S$\backslash$:/SciencePhdSorted/Business Process Management Journal/Automated competitor analysis using big data analytics evidence from the fitness mobile app business{\_}Guo et al.{\_}2017.pdf:pdf},
issn = {1463-7154},
journal = {Business Process Management Journal},
number = {3},
pages = {BPMJ--05--2015--0065},
title = {{Automated competitor analysis using big data analytics: evidence from the fitness mobile app business}},
url = {http://www.emeraldinsight.com/doi/10.1108/BPMJ-05-2015-0065},
volume = {23},
year = {2017}
}
@article{Holzinger2006,
abstract = {In this paper, we show how to use ontologies to bootstrap a knowledge acquisition process that extracts product information from tabular data on Web pages. Furthermore, we use logical rules to reason about product specific properties and to derive higher-order knowledge about product features. We will also explain the knowledge acquisition process, covering both ontological and procedural aspects. Finally, we will give an qualitative and quantitative evaluation of our results.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Holzinger, Wolfgang and Kr{\"{u}}pl, Bernhard and Herzog, Marcus},
doi = {10.1007/11926078_21},
eprint = {9780201398298},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Using Ontologies for Extracting Product Features from Web Pages{\_}Holzinger, Kr{\"{u}}pl, Herzog{\_}2006.pdf:pdf},
isbn = {978-3-540-49029-6},
issn = {00219258},
pages = {286--299},
pmid = {15326190},
title = {{Using Ontologies for Extracting Product Features from Web Pages}},
url = {http://link.springer.com/10.1007/11926078{\_}21},
year = {2006}
}
@article{Oren2008,
author = {Oren, Eyal and Heitmann, Benjamin and Decker, Stefan},
doi = {10.1016/j.websem.2008.04.003},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Web Semantics Science , Services and Agents on the World Wide Web ActiveRDF Embedding Semantic Web data into object-oriented languages.pdf:pdf},
pages = {191--202},
title = {{Web Semantics : Science , Services and Agents on the World Wide Web ActiveRDF : Embedding Semantic Web data into object-oriented languages ଝ}},
volume = {6},
year = {2008}
}
@article{Devi2016,
abstract = {This paper studies efficient processing of durable top-k queries on historical time series databases. Durable top-k queries, obtained as an extension of snapshot top-k queries during a certain time period, play a key role in finding objects with durable quality and predicting the status of these objects for successive time intervals by updating the query interval at all timestamps. Web crawling and indexing are tremendously significant in recent times, especially in terms of achieving efficient durable top-k queries from vast quantum of web documents. Existing algorithms that have been employed throw up results that are less than applicable to analyzers. This paper chiefly focuses on web crawling and indexing query terms under their respective categories and updating rank changes at every time interval. Links are crawled using the modified depth-first search (MDFS) algorithm, accessed, and metadata such as the title, keywords, and descriptions extracted. To handle query indexing, novel indexing techniques are proposed to yield efficient results. This study is invaluable for analysts working on large data obtained as a result of crawling and indexing, effectively decreasing their workload.},
author = {Devi, R. Suganya and Manjula, D. and Sugumaran, Vijayan},
doi = {10.1007/s10586-016-0595-4},
file = {:S$\backslash$:/SciencePhdSorted/Cluster Computing/Efficient indexing structure to handle durable queries through web crawling{\_}Devi, Manjula, Sugumaran{\_}2016.pdf:pdf},
issn = {15737543},
journal = {Cluster Computing},
keywords = {Temporal queries,Web search,Durable queries,Time series,Top-k,Tempo},
number = {3},
pages = {1347--1358},
publisher = {Springer US},
title = {{Efficient indexing structure to handle durable queries through web crawling}},
volume = {19},
year = {2016}
}
@book{Richey2016,
abstract = {Journals in business logistics, operations management, supply chain management, and business strategy have initiated ongoing calls for Big Data research and its impact on research and practice. Currently, no extant research has defined the concept fully. The purpose of this paper is to develop an industry grounded definition of Big Data by canvassing supply chain managers across six nations. The supply chain setting defines Big Data as inclusive of four dimensions: volume, velocity, variety, and veracity. The study further extracts multiple concepts that are important to the future of supply chain relationship strategy and performance. These outcomes provide a starting point and extend a call for theoretically grounded and paradigm-breaking research on managing business-to- business relationships in the age of Big Data.},
author = {Richey, Robert Glenn and Morgan, Tyler R. and Lindsey-Hall, Kristina and Adams, Frank G.},
booktitle = {International Journal of Physical Distribution {\&} Logistics Management},
doi = {10.1108/IJPDLM-05-2016-0134},
file = {:S$\backslash$:/SciencePhdSorted/International Journal of Physical Distribution {\&} Logistics Management/A global exploration of Big Data in the supply chain{\_}Richey et al.{\_}2016.pdf:pdf},
isbn = {0520130138},
issn = {0960-0035},
number = {8},
pages = {710--739},
title = {{A global exploration of Big Data in the supply chain}},
url = {http://www.emeraldinsight.com/doi/10.1108/IJPDLM-05-2016-0134},
volume = {46},
year = {2016}
}
@article{A2009,
author = {{\~{A}}, Olena Medelyan and Milne, David and Legg, Catherine and Witten, Ian H},
doi = {10.1016/j.ijhcs.2009.05.004},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Human Computer Studies/Mining meaning from Wikipedia{\_}{\~{A}} et al.{\_}2009.pdf:pdf},
issn = {1071-5819},
journal = {Journal of Human Computer Studies},
keywords = {Information extraction,Information retrieval,NLP,Ontologies,Semantic web,Text mining,Wikipedia,Wikipedia mining,information extraction,information retrieval,nlp,ontologies,semantic web,text mining,wikipedia,wikipedia mining},
number = {9},
pages = {716--754},
publisher = {Elsevier},
title = {{Mining meaning from Wikipedia}},
url = {http://dx.doi.org/10.1016/j.ijhcs.2009.05.004},
volume = {67},
year = {2009}
}
@article{Science2012,
author = {Science, Computer},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Web page content block partitioning for Focussed Crawling{\_}Science{\_}2012.pdf:pdf},
number = {June},
title = {{Web page content block partitioning for Focussed Crawling}},
year = {2012}
}
@article{Derhami2013,
abstract = {Ranking web pages for presenting the most relevant web pages to user's queries is one of the main issues in any search engine. In this paper, two new ranking algorithms are offered, using Reinforcement Learning (RL) concepts. RL is a powerful technique of modern artificial intelligence that tunes agent's parameters, interactively. In the first step, with formulation of ranking as an RL problem, a new connectivity-based ranking algorithm, called RL-Rank, is proposed. In RL-Rank, agent is considered as a surfer who travels between web pages by clicking randomly on a link in the current page. Each web page is considered as a state and value function of state is used to determine the score of that state (page). Reward is corresponded to number of out links from the current page. Rank scores in RL-Rank are computed in a recursive way. Convergence of these scores is proved. In the next step, we introduce a new hybrid approach using combination of BM25 as a content-based algorithm and RL-Rank. Both proposed algorithms are evaluated by well known benchmark datasets and analyzed according to concerning criteria. Experimental results show using RL concepts leads significant improvements in raking algorithms. ?? 2013 Elsevier B.V.},
author = {Derhami, Vali and Khodadadian, Elahe and Ghasemzadeh, Mohammad and {Zareh Bidoki}, Ali Mohammad},
doi = {10.1016/j.asoc.2012.12.023},
file = {:S$\backslash$:/SciencePhdSorted/Applied Soft Computing Journal/Applying reinforcement learning for web pages ranking algorithms{\_}Derhami et al.{\_}2013.pdf:pdf},
isbn = {1568-4946},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Agent,Artificial intelligence,Ranking,Reinforcement Learning,Search engine,Value function},
number = {4},
pages = {1686--1692},
publisher = {Elsevier B.V.},
title = {{Applying reinforcement learning for web pages ranking algorithms}},
url = {http://dx.doi.org/10.1016/j.asoc.2012.12.023},
volume = {13},
year = {2013}
}
@article{Thorleuchter2013,
abstract = {The internet is a valuable source of information where many ideas can be found dealing with different topics. A few numbers of ideas might be able to solve an existing problem. However, it is time-consuming to identify these ideas within the large amount of textual information in the internet. This paper introduces a new web mining approach that enables an automated identification of new technological ideas extracted from internet sources that are able to solve a given problem. It adapts and combines several existing approaches from literature: approaches that extract new technological ideas from a user given text, approaches that investigate the different idea characteristics in different technical domains, and multi-language web mining approaches. In contrast to previous work, the proposed approach enables the identification of problem solution ideas in the internet considering domain dependencies and language aspects. In a case study, new ideas are identified to solve existing technological problems as occurred in research and development (R{\&}D) projects. This supports the process of research planning and technology development. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Thorleuchter, D. and {Van Den Poel}, D.},
doi = {10.1016/j.eswa.2013.01.013},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/Web mining based extraction of problem solution ideas{\_}Thorleuchter, Van Den Poel{\_}2013.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Idea mining,R{\&}D planning,Text mining,Web mining},
number = {10},
pages = {3961--3969},
title = {{Web mining based extraction of problem solution ideas}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.01.013},
volume = {40},
year = {2013}
}
@article{Hogan2014,
author = {Hogan, Aidan and Arenas, Marcelo and Mallea, Alejandro and Polleres, Axel},
doi = {10.1016/j.websem.2014.06.004},
file = {:S$\backslash$:/SciencePhdSorted/Web Semantics Science, Services and Agents on the World Wide Web/Web Semantics Science , Services and Agents on the World Wide Web Everything you always wanted to know about blank nodes ✩{\_}Hogan et al..pdf:pdf;:S$\backslash$:/SciencePhdSorted/Web Semantics Science, Services and Agents on the World Wide Web/Web Semantics Science , Services and Agents on the World Wide Web Everything you always wanted to know about blank nodes ✩{\_}Hogan et a(2).pdf:pdf},
issn = {1570-8268},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
pages = {42--69},
publisher = {Elsevier B.V.},
title = {{Web Semantics : Science , Services and Agents on the World Wide Web Everything you always wanted to know about blank nodes ✩}},
url = {http://dx.doi.org/10.1016/j.websem.2014.06.004},
volume = {28},
year = {2014}
}
@article{Menczer,
author = {Menczer, By Filippo},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/8 Web Crawling{\_}Menczer{\_}Unknown.pdf:pdf},
title = {{8 Web Crawling}}
}
@article{Green2000,
abstract = {The interrelation between Web publishing and information retrieval technologies is explored. The different elements of the Web have implications for indexing and searching Web pages. There are two main platforms used for searching the Web – directories and search engines – which later became combined to create one-stop search sites, resulting in the Web business model known as portals. Portalisation gave rise to a second-generation of firms delivering innovative search technology. Various new approaches to Web indexing and information retrieval are listed. PC-based search tools incorporate intelligent agents to allow greater manipulation of search strategies and results. Current trends are discussed, in particular the rise of XML, and their implications for the future. It is concluded that the Web is emerging from a nascent stage and is evolving into a more complex, diverse and structured environment.},
author = {Green, David},
doi = {10.1108/14684520010330283},
file = {:S$\backslash$:/SciencePhdSorted/Online Information Review/The evolution of Web searching{\_}Green{\_}2000.pdf:pdf},
isbn = {1468-4527},
issn = {1468-4527},
journal = {Online Information Review},
keywords = {electronic publishing,information retrieval,internet},
number = {2},
pages = {124--137},
title = {{The evolution of Web searching}},
url = {http://dx.doi.org/10.1108/14684520010330283{\%}5Cnhttp://www.emeraldinsight.com/10.1108/14684520010330283},
volume = {24},
year = {2000}
}
@article{Pecina2014,
abstract = {In this paper, we tackle the problem of domain adaptation of statistical machine translation (SMT) by exploiting domain-specific data acquired by domain-focused crawling of text from the World Wide Web. We design and empirically evaluate a procedure for automatic acquisition of monolingual and parallel text and their exploitation for system training, tuning, and testing in a phrase-based SMT framework. We present a strategy for using such resources depending on their availability and quantity supported by results of a large-scale evaluation carried out for the domains of environment and labour legislation, two language pairs (English–French and English–Greek) and in both directions: into and from English. In general, machine translation systems trained and tuned on a general domain perform poorly on specific domains and we show that such systems can be adapted successfully by retuning model parameters using small amounts of parallel in-domain data, and may be further improved by using additional monolingual and parallel training data for adaptation of language and translation models. The average observed improvement in BLEU achieved is substantial at 15.30 points absolute.},
author = {Pecina, Pavel and Toral, Antonio and Papavassiliou, Vassilis and Prokopidis, Prokopis and Tamchyna, Ale?? and Way, Andy and van Genabith, Josef},
doi = {10.1007/s10579-014-9282-3},
file = {:S$\backslash$:/SciencePhdSorted/Language Resources and Evaluation/Domain adaptation of statistical machine translation with domain-focused web crawling{\_}Pecina et al.{\_}2014.pdf:pdf},
isbn = {1574-020X; 1574-0218},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Domain adaptation,Optimisation,Statistical machine translation,Web crawling},
number = {1},
pages = {147--193},
pmid = {26120290},
title = {{Domain adaptation of statistical machine translation with domain-focused web crawling}},
volume = {49},
year = {2014}
}
@article{Bontcheva2003,
author = {Bontcheva, DK and Kiryakov, MA},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Semantic web enabled, open source language technology{\_}Bontcheva, Kiryakov{\_}2003.pdf:pdf},
number = {ii},
title = {{Semantic web enabled, open source language technology}},
url = {http://eprints.aktors.org/250/},
year = {2003}
}
@article{Moreno2013,
author = {Moreno, Antonio and Vicient, Carlos and Sa, David},
doi = {10.1016/j.engappai.2012.08.002},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Engineering Applications of Artificial Intelligence An automatic approach for ontology-based feature extraction from heterogeneous textu.pdf:pdf},
keywords = {information extraction},
pages = {1092--1106},
title = {{Engineering Applications of Artificial Intelligence An automatic approach for ontology-based feature extraction from heterogeneous textual resources}},
volume = {26},
year = {2013}
}
@article{Moreno2013a,
author = {Moreno, Antonio and Isern, David and L{\'{o}}pez, Alejandra C},
doi = {10.1016/j.eswa.2012.12.090},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems With Applications/Expert Systems with Applications Ontology-based information extraction of regulatory networks from scientific articles with case studies.pdf:pdf},
issn = {0957-4174},
journal = {Expert Systems With Applications},
keywords = {knowledge representation,ontology-based information extraction},
number = {8},
pages = {3266--3281},
title = {{Expert Systems with Applications Ontology-based information extraction of regulatory networks from scientific articles with case studies for Escherichia coli}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.12.090},
volume = {40},
year = {2013}
}
@article{G??k2015,
abstract = {As enterprises expand and post increasing information about their$\backslash$nbusiness activities on their websites, website data promises to be a$\backslash$nvaluable source for investigating innovation. This article examines the$\backslash$npracticalities and effectiveness of web mining as a research method for$\backslash$ninnovation studies. We use web mining to explore the R{\&}D activities of$\backslash$n296 UK-based green goods small and mid-size enterprises. We find that$\backslash$nwebsite data offers additional insights when compared with other$\backslash$ntraditional unobtrusive research methods, such as patent and publication$\backslash$nanalysis. We examine the strengths and limitations of enterprise$\backslash$ninnovation web mining in terms of a wide range of data quality$\backslash$ndimensions, including accuracy, completeness, currency, quantity,$\backslash$nflexibility and accessibility. We observe that far more companies in our$\backslash$nsample report undertaking R{\&}D activities on their web sites than would$\backslash$nbe suggested by looking only at conventional data sources. While$\backslash$ntraditional methods offer information about the early phases of R{\&}D and$\backslash$ninvention through publications and patents, web mining offers insights$\backslash$nthat are more downstream in the innovation process. Handling website$\backslash$ndata is not as easy as alternative data sources, and care needs to be$\backslash$ntaken in executing search strategies. Website information is also$\backslash$nself-reported and companies may vary in their motivations for posting$\backslash$n(or not posting) information about their activities on websites.$\backslash$nNonetheless, we find that web mining is a significant and useful$\backslash$ncomplement to current methods, as well as offering novel insights not$\backslash$neasily obtained from other unobtrusive sources.},
author = {G??k, Abdullah and Waterworth, Alec and Shapira, Philip},
doi = {10.1007/s11192-014-1434-0},
file = {:S$\backslash$:/SciencePhdSorted/Scientometrics/Use of web mining in studying innovation{\_}Gk, Waterworth, Shapira{\_}2015.pdf:pdf},
isbn = {0138-9130 (Print)0138-9130 (Linking)},
issn = {15882861},
journal = {Scientometrics},
keywords = {Innovation,R{\&}D,Web mining,Web scraping},
number = {1},
pages = {653--671},
pmid = {26696691},
title = {{Use of web mining in studying innovation}},
volume = {102},
year = {2015}
}
@article{Powell2016,
abstract = {The webpages of organizations are both a form of representation and a type of narrative. They entertain, persuade, express a point of view, and provide a means to organize collective action and economic exchange. Increasingly, webpages are the primary point of access between an organization and its environment. An organization's online presence offers a major new source of rich information about organizations. In this paper, we develop three perspectives on websites from an organizational point of view: as identity projects, tools, and relational maps. We draw on data from the online and offline presences of “brick and mortar” nonprofit organizations in the San Francisco Bay Area to both illustrate how a digital transformation shaped these organizations and identify a host of new methods that can be used to study organizations using webpages. Finally, we reflect on both the strengths of these new sources of data as well as possible limitations and conclude with theoretical implications for organizational scholars.},
author = {Powell, Walter W. and Horvath, Aaron and Brandtner, Christof},
doi = {10.1016/j.riob.2016.07.001},
file = {:S$\backslash$:/SciencePhdSorted/Research in Organizational Behavior/Click and mortar Organizations on the web{\_}Powell, Horvath, Brandtner{\_}2016.pdf:pdf},
isbn = {0191-3085},
issn = {01913085},
journal = {Research in Organizational Behavior},
keywords = {Content analysis,Identity,Nonprofits,Organizational mission,Relational maps,Semantic data,Tools,Webpages},
pages = {101--120},
publisher = {Elsevier Ltd},
title = {{Click and mortar: Organizations on the web}},
url = {http://dx.doi.org/10.1016/j.riob.2016.07.001},
volume = {36},
year = {2016}
}
@book{KOLOG,
author = {KOLOG, EMMANUEL AWUNI},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Dissertations in Forestry and Natural Sciences{\_}KOLOG{\_}Unknown.pdf:pdf},
isbn = {9789526121581},
title = {{Dissertations in Forestry and Natural Sciences}}
}
@article{Kumar2015,
abstract = {In this digital era, marketing theory and practice are being transformed by increasing complexity due to information availability, higher reach and interactions, and faster speeds of transactions. These have led to the adoption of intelligent agent technologies (IATs) by many companies. As IATs are relatively new and technologically complex, several definitions are evolving, and the theory in this area is not yet fully developed. There is a need to provide structure and guidance to marketers to further this emerging stream of research. As a first step, this paper proposes a marketing-centric definition and a systematic taxonomy and framework. The authors, using a grounded theory approach, conduct an extensive literature review and a qualitative study in which interviews with managers from 50 companies in 22 industries reveal the importance of understanding IAT applications and adopting them. Further, the authors propose an integrated conceptual framework with several propositions regarding IAT adoption. This research identifies the gaps in the literature and the need for adoption of IATs in the future of marketing given changing consumer behavior and product and industry characteristics.},
author = {Kumar, V. and Dixit, Ashutosh and Javalgi, Rajshekar (Raj) G and Dass, Mayukh},
doi = {10.1007/s11747-015-0426-9},
file = {:S$\backslash$:/SciencePhdSorted/Journal of the Academy of Marketing Science/Research framework, strategies, and applications of intelligent agent technologies (IATs) in marketing{\_}Kumar et al.{\_}2015.pdf:pdf},
isbn = {0092-0703},
issn = {00920703},
journal = {Journal of the Academy of Marketing Science},
keywords = {Grounded theory,Intelligent agent technologies,Marketing strategy},
pages = {24--45},
title = {{Research framework, strategies, and applications of intelligent agent technologies (IATs) in marketing}},
year = {2015}
}
@article{Palomino2012,
abstract = {Purpose: In this review, the aim is first to define horizon scanning and then outline the general approach currently employed by many organisations using web-based resources. It then aims to discuss the benefits and drivers of horizon scanning, to identify some organisations currently undertaking activities in the field, and explain in detail how the web-based horizon scanning approach is implemented. The aim is then to conclude with a discussion of good practice and areas for further research. Design/methodology/approach: The basis for this review is a symposium held at the UK Defence Science and Technology Laboratory in March 2010, where groups undertaking horizon scanning activities shared practices and reviewed the state of the art. Practitioners from both public sector and private organisations attending this symposium, as well as others, were invited to contribute to the manuscript, developing this as an iterative exercise over the last year. Findings: Structured processes of web-based horizon scanning, underpinned by strong technical understanding and principles of good practice described in the review, can add significant value to organisational decision making. Originality/value: While a growing number of private and public sector organisations have already embarked on the use of the web as a key information resource, no detailed explanation of the web-based horizon scanning approach has been published. The review therefore makes an original contribution to this field, with collaborations by horizon scanning practitioners, discussing what constitutes good practice and highlighting areas where future research is needed. {\textcopyright} Emerald Group Publishing Limited.},
author = {Palomino, Marco A. and Bardsley, Sarah and Bown, Kevin and {De Lurio}, Jennifer and Ellwood, Peter and Holland‐Smith, David and Huggins, Bob and Vincenti, Alexandra and Woodroof, Harry and Owen, Richard},
doi = {10.1108/14636681211269851},
file = {:S$\backslash$:/SciencePhdSorted/Foresight/Web‐based horizon scanning concepts and practice{\_}Palomino et al.{\_}2012.pdf:pdf},
isbn = {1463-6689},
issn = {1463-6689},
journal = {Foresight},
number = {5},
pages = {355--373},
title = {{Web‐based horizon scanning: concepts and practice}},
url = {http://www.emeraldinsight.com/doi/10.1108/14636681211269851},
volume = {14},
year = {2012}
}
@article{Davis2012,
author = {Davis, Judith R},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/O Perational B Usiness I Ntelligence {\_}Davis{\_}2012.pdf:pdf},
keywords = {blog mining,business intelligence,design science,social networks,web mining},
number = {4},
pages = {1189--1216},
title = {{O Perational B Usiness I Ntelligence :}},
volume = {36},
year = {2012}
}
@article{Brin2012,
abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from 3 years ago. This paper provides an in-depth description of our large-scale web search engine - the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections, where anyone can publish anything they want. ?? 2012 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Brin, Sergey and Page, Lawrence},
doi = {10.1016/j.comnet.2012.10.007},
eprint = {1111.6189v1},
file = {:S$\backslash$:/SciencePhdSorted/Computer Networks/Reprint of The anatomy of a large-scale hypertextual web search engine{\_}Brin, Page{\_}2012.pdf:pdf},
isbn = {9781424453658},
issn = {13891286},
journal = {Computer Networks},
keywords = {Google,Information retrieval,Pagerank,Search engines,World Wide Web},
number = {18},
pages = {3825--3833},
pmid = {17520979},
publisher = {Elsevier B.V.},
title = {{Reprint of: The anatomy of a large-scale hypertextual web search engine}},
url = {http://dx.doi.org/10.1016/j.comnet.2012.10.007},
volume = {56},
year = {2012}
}
@article{Sauer2014,
abstract = {Web communities and the Web 2.0 provide a huge amount of experiences and there has been a growing availability of Linked (Open) Data. Making experiences and data available as knowledge to be used in case-based reasoning (CBR) systems is a current research effort. The process of extracting such knowledge from the diverse data types used in web communities, to transform data obtained from Linked Data sources, and then formalising it for CBR, is not an easy task. In this paper, we present a prototype, the Knowledge Extraction Workbench (KEWo), which supports the knowledge engineer in this task. We integrated the KEWo into the open-source case-based reasoning tool myCBR Workbench. We provide details on the abilities of the KEWo to extract vocabularies from Linked Data sources and generate taxonomies from Linked Data as well as from web community data in the form of semi-structured texts.},
author = {Sauer, Christian Severin and Roth-Berghofer, Thomas},
doi = {10.1111/exsy.12034},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems/Extracting knowledge from web communities and linked data for case-based reasoning systems{\_}Sauer, Roth-Berghofer{\_}2014.pdf:pdf},
issn = {14680394},
journal = {Expert Systems},
keywords = {Case-based reasoning,Experience web,Information extraction,Linked data},
number = {5},
pages = {448--456},
title = {{Extracting knowledge from web communities and linked data for case-based reasoning systems}},
volume = {31},
year = {2014}
}
@article{Ruotsalo2013,
abstract = {Semantic and context knowledge has been envisioned as an appropriate solution for addressing the content heterogeneity and information overload in mobile Web information access, but few have explored their full potential in mobile scenarios, where information objects refer to their physical counterparts, and retrieval is context-aware and personalized for users. We present SMARTMUSEUM, a mobile ubiquitous recommender system for the Web of Data, and its application to information needs of tourists in context-aware on-site access to cultural heritage. The SMARTMUSEUM system utilizes Semantic Web languages as the form of data representation. Ontologies are used to bridge the semantic gap between heterogeneous content descriptions, sensor inputs, and user profiles. The system makes use of an information retrieval framework wherein context data and search result clustering are used in recommendation of suitable content for mobile users. Results from laboratory experiments demonstrate that ontology-based reasoning, query expansion, search result clustering, and context knowledge lead to significant improvement in recommendation performance. The results from field trials show that the usability of the system meets users' expectations in real-world use. The results indicate that semantic content representation and retrieval can significantly improve the performance of mobile recommender systems in knowledge-rich domains. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Ruotsalo, Tuukka and Haav, Krister and Stoyanov, Antony and Roche, Sylvain and Fani, Elena and Deliai, Romina and M{\"{a}}kel{\"{a}}, Eetu and Kauppinen, Tomi and Hyv{\"{o}}nen, Eero},
doi = {10.1016/j.websem.2013.03.001},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Web Semantics/SMARTMUSEUM A mobile recommender system for the Web of Data{\_}Ruotsalo et al.{\_}2013.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Cultural heritage,Information retrieval,Mobile systems,Recommender systems,Semantic Web,Web of Data},
pages = {50--67},
publisher = {Elsevier B.V.},
title = {{SMARTMUSEUM: A mobile recommender system for the Web of Data}},
url = {http://dx.doi.org/10.1016/j.websem.2013.03.001},
volume = {20},
year = {2013}
}
@article{Grubic2012,
author = {Grubi{\'{c}}, Goran and Ratkovi{\'{c}}, Milijanka and Paunovi{\'{c}}, Lidija and Bara{\'{c}}, Du{\v{s}}an and Radenkovi{\'{c}}, Bo{\v{z}}idar},
file = {:S$\backslash$:/SciencePhdSorted/Metalurgia International/E-Commerce Development, Structure and Infrastructure in the Republic of Serbia With Focus on E-Retail{\_}Grubi{\'{c}} et al.{\_}2012.pdf:pdf;:S$\backslash$:/SciencePhdSorted/Metalurgia International/E-Commerce Development, Structure and Infrastructure in the Republic of Serbia With Focus on E-Retail{\_}Grubi{\'{c}} et al.{\_}2012(2).pdf:pdf},
issn = {1582-2214},
journal = {Metalurgia International},
keywords = {business environment,consumer behavior,electronic commerce,online retail,serbia},
number = {10},
pages = {207--213},
title = {{E-Commerce Development, Structure and Infrastructure in the Republic of Serbia With Focus on E-Retail}},
year = {2012}
}
@article{Ali2015,
author = {Ali, Rashid and Naim, Iram},
doi = {10.1007/s13042-013-0212-2},
file = {:S$\backslash$:/SciencePhdSorted/International Journal of Machine Learning and Cybernetics/User feedback based metasearching using neural network{\_}Ali, Naim{\_}2015.pdf:pdf},
issn = {1868808X},
journal = {International Journal of Machine Learning and Cybernetics},
keywords = {Correlation coefficient,Metasearching,Neural network,Performance evaluation,User feedback},
number = {2},
pages = {265--275},
title = {{User feedback based metasearching using neural network}},
volume = {6},
year = {2015}
}
@article{Varelas2005,
abstract = {Semantic Similarity relates to computing the similarity between concepts which are not lexicographically similar. We investigate approaches to computing semantic similarity by mapping terms (concepts) to an ontology and by examining their relationships in that ontology. Some of the most popular semantic similarity methods are implemented and evaluated using WordNet as the underlying reference ontology. Building upon the idea of semantic similarity, a novel information retrieval method is also proposed. This method is capable of detecting similarities between documents containing semantically similar but not necessarily lexicographically similar terms. The proposed method has been evaluated in retrieval of images and documents on the Web. The experimental results demonstrated very promising performance improvements over state-of-the-art information retrieval methods.},
author = {Varelas, Giannis and Voutsakis, Epimenidis and Raftopoulou, Paraskevi and Petrakis, Euripides G.M. and Milios, Evangelos E.},
doi = {10.1145/1097047.1097051},
file = {:S$\backslash$:/SciencePhdSorted/Proceedings of the seventh ACM international workshop on Web information and data management - WIDM '05/Semantic similarity methods in wordNet and their application to information retrieval on the web{\_}Varelas et al.{\_}2005.pdf:pdf},
isbn = {1595931945},
journal = {Proceedings of the seventh ACM international workshop on Web information and data management - WIDM '05},
keywords = {information retrieval,semantic similarity,wordnet,world},
pages = {10},
title = {{Semantic similarity methods in wordNet and their application to information retrieval on the web}},
url = {http://portal.acm.org/citation.cfm?doid=1097047.1097051},
year = {2005}
}
@article{Stoffel2009,
author = {Stoffel, Kilian},
file = {:S$\backslash$:/SciencePhdSorted/HMD – Praxis der Wirtschaftsinformatik/Web Data Mining = Web Mining Nutzbarmachung des Web{\_}Stoffel{\_}2009.pdf:pdf},
journal = {HMD – Praxis der Wirtschaftsinformatik},
number = {HMD 268},
pages = {6--20},
title = {{Web + Data Mining = Web Mining Nutzbarmachung des Web}},
volume = {46. Jahrga},
year = {2009}
}
@article{Papadimitriou2010,
abstract = {Web graphs are approximate snapshots of the web, created by search engines. They are essential to monitor the evolution of the web and to compute global properties like PageRank values of web pages. Their continuous monitoring requires a notion of graph similarity to help measure the amount and significance of changes in the evolving web. As a result, these measurements provide means to validate how well search engines acquire content from the web. In this paper, we propose five similarity schemes: three of them we adapted from existing graph similarity measures, and two we adapted from well-known document and vector similarity methods (namely, the shingling method and random projection based method). We empirically evaluate and compare all five schemes using a sequence of web graphs from Yahoo!, and study if the schemes can identify anomalies that may occur due to hardware or other problems.},
author = {Papadimitriou, Panagiotis and Dasdan, Ali and Garcia-Molina, Hector},
doi = {10.1007/s13174-010-0003-x},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Internet Services and Applications/Web graph similarity for anomaly detection{\_}Papadimitriou, Dasdan, Garcia-Molina{\_}2010.pdf:pdf},
isbn = {1867-4828 (Print) 1869-0238 (Online)},
issn = {18674828},
journal = {Journal of Internet Services and Applications},
keywords = {Anomaly detection,Graph similarity,Locality sensitive hashing},
number = {1},
pages = {19--30},
title = {{Web graph similarity for anomaly detection}},
volume = {1},
year = {2010}
}
@article{Ertekin,
author = {Ertekin, Seyda and Giles, C Lee},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/A Comparative Study on Representation of Web Pages in Automatic Text Categorization{\_}Ertekin, Giles{\_}Unknown.pdf:pdf},
keywords = {Document model},
mendeley-tags = {Document model},
title = {{A Comparative Study on Representation of Web Pages in Automatic Text Categorization}},
volume = {2}
}
@article{Klingler2009,
author = {Klingler, H. C. and Dietersdorfer, F. and Fink, K. G. and Fischer, M. and Heidler, H. and Huber, J. and L{\"{u}}ftenegger, W. and Madersbacher, H. and Petrovic, Z. and Primus, G. and Schrey, A. and W{\"{a}}chter, J.},
doi = {10.1108/10662240910927830},
file = {:S$\backslash$:/SciencePhdSorted/Journal fur Urologie und Urogynakologie/Leitlinie Der intermittierende Katheterismus{\_}Klingler et al.{\_}2009.pdf:pdf},
issn = {10236090},
journal = {Journal fur Urologie und Urogynakologie},
keywords = {disabilities,paper type research paper,user studies,worldwide web},
number = {2},
pages = {5--7},
title = {{Leitlinie: Der intermittierende Katheterismus}},
volume = {16},
year = {2009}
}
@article{Huang2011d,
abstract = {This research pertains to the design and development of a shopbot called WebShopper+. This shopbot is intended to help customers find and compare e-tailers that market their wares using different languages. WebShopper+ is built with a multilingual ontology to overcome the language barriers that arise with global e-commerce. This research proposes a semi-automatic method of constructing a multilingual ontology by using the formal concept analysis and association analysis. It also proposes an automatic method for the categorization of product data into predefined classes, with the aim of alleviating administrators' task load. Additionally, a semantic search mechanism based on concept similarity is designed to assist customers in finding more desirable products. The experimental results show that these methods perform well and the shopbot can help customers find real bargains on the Web and to find products that cannot be bought locally. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Huang, Shiu Li and Tsai, Yu Hsiang},
doi = {10.1016/j.dss.2010.10.004},
file = {:S$\backslash$:/SciencePhdSorted/Decision Support Systems/Designing a cross-language comparison-shopping agent{\_}Huang, Tsai{\_}2011.pdf:pdf},
isbn = {0167-9236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Comparison-shopping,Formal concept analysis,Ontology,Semantic similarity,Shopbot},
number = {2},
pages = {428--438},
publisher = {Elsevier B.V.},
title = {{Designing a cross-language comparison-shopping agent}},
url = {http://dx.doi.org/10.1016/j.dss.2010.10.004},
volume = {50},
year = {2011}
}
@article{Hern??ndez2016,
abstract = {Web page classification refers to the problem of automatically assigning a web page to one or more classes after analysing its features. Automated web page classifiers have many applications, and many researchers have proposed techniques and tools to perform web page classification. Unfortunately, the existing tools have a number of drawbacks that makes them unappealing for real-world scenarios, namely: they require a previous extensive crawling, they are supervised, they need to download a page before classifying it, or they are site-, language-, or domain-dependent. In this article, we propose CALA, a tool for URL-based web page classification. The strongest features of our tool are that it does not require a previous extensive crawling to achieve good classification results, it is unsupervised, it is based exclusively on URL features, which means that pages can be classified without downloading them, and it is site-, language-, and domain-independent, which makes it generally applicable. We have validated our tool with 22 real-world web sites from multiple domains and languages, and our conclusion is that CALA is very effective and efficient in practice.},
author = {Hern??ndez, Inma and Rivero, Carlos R. and Ruiz, David and Corchuelo, Rafael},
doi = {10.1016/j.jss.2016.02.006},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Systems and Software/CALA ClAssifying Links Automatically based on their URL{\_}Hernndez et al.{\_}2016.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {URL patterns,Web page classification},
pages = {130--143},
title = {{CALA: ClAssifying Links Automatically based on their URL}},
volume = {115},
year = {2016}
}
@article{Du2015a,
author = {Du, Yajun and Tian, Xiuxia and Liu, Wenjun and Wang, Min and Song, Wen and Fan, Yongquan},
doi = {10.3233/IDA-150762},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/A novel page ranking algorithm based on triadic closure and hyperlink-induced topic search{\_}Du et al.{\_}2015.pdf:pdf},
keywords = {common reference,hits algorithm,random walks,topic similarity,triadic closure,trust-degree},
pages = {1131--1149},
title = {{A novel page ranking algorithm based on triadic closure and hyperlink-induced topic search}},
volume = {19},
year = {2015}
}
@article{Chevalier2008,
author = {Chevalier, Max},
doi = {10.1007/s10791-008-9057-9},
file = {:S$\backslash$:/SciencePhdSorted/Information Retrieval/Zdravko Markov and Daniel T. Larose, Data Mining the Web Uncovering Patterns in Web Content, Structure, and Usage{\_}Chevalier{\_}2008.pdf:pdf},
isbn = {9780471666554},
issn = {1386-4564},
journal = {Information Retrieval},
number = {2},
pages = {169--174},
title = {{Zdravko Markov and Daniel T. Larose, Data Mining the Web: Uncovering Patterns in Web Content, Structure, and Usage}},
url = {http://link.springer.com/article/10.1007/s10791-008-9057-9/fulltext.html},
volume = {11},
year = {2008}
}
@article{Maree2013,
abstract = {With the development of the Semantic Web technology, the use of ontologies to store and retrieve information covering several domains has increased. However, very few ontologies are able to cope with the ever-growing need of frequently updated semantic information or specific user requirements in specialized domains. As a result, a critical issue is related to the unavailability of relational information between concepts, also coined missing background knowledge. One solution to address this issue relies on the manual enrichment of ontologies by domain experts which is however a time consuming and costly process, hence the need for dynamic ontology enrichment. In this paper we present an automatic coupled statistical/semantic framework for dynamically enriching large-scale generic ontologies from the World Wide Web. Using the massive amount of information encoded in texts on the Web as a corpus, missing background knowledge can therefore be discovered through a combination of semantic relatedness measures and pattern acquisition techniques and subsequently exploited. The benefits of our approach are: (i) proposing the dynamic enrichment of large-scale generic ontologies with missing background knowledge, and thus, enabling the reuse of such knowledge, (ii) dealing with the issue of costly ontological manual enrichment by domain experts. Experimental results in a precision-based evaluation setting demonstrate the effectiveness of the proposed techniques.},
author = {Maree, Mohammed and Belkhatir, Mohammed},
doi = {10.1007/s10844-012-0233-4},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Intelligent Information Systems/Coupling semantic and statistical techniques for dynamically enriching web ontologies{\_}Maree, Belkhatir{\_}2013.pdf:pdf},
isbn = {0925-9902},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Dynamic ontology enrichment,Large-scale ontologies,Missing background knowledge,Precision-based evaluation,Semantic relatedness},
number = {3},
pages = {455--478},
title = {{Coupling semantic and statistical techniques for dynamically enriching web ontologies}},
volume = {40},
year = {2013}
}
@article{Prakash2015,
author = {Prakash, Jay and Kumar, Rakesh},
doi = {10.1016/j.procs.2015.04.172},
file = {:S$\backslash$:/SciencePhdSorted/Procedia - Procedia Computer Science/Web Crawling through Shark-Search using PageRank{\_}Prakash, Kumar{\_}2015.pdf:pdf},
issn = {1877-0509},
journal = {Procedia - Procedia Computer Science},
keywords = {pagerank,shark-search,web crawling},
number = {Iccc},
pages = {210--216},
publisher = {Elsevier Masson SAS},
title = {{Web Crawling through Shark-Search using PageRank}},
url = {http://dx.doi.org/10.1016/j.procs.2015.04.172},
volume = {48},
year = {2015}
}
@article{Saad2012,
abstract = {A pattern is a model or a template used to summarize and describe the behavior (or the trend) of data having generally some recurrent events. Patterns have received a considerable attention in recent years and were widely studied in the data mining field. Various pattern mining approaches have been proposed and used for different applications such as network monitoring, moving object tracking, financial or medical data analysis, scientific data processing, etc. In these different contexts, discovered patterns were useful to detect anomalies, to predict data behavior (or trend) or, more generally, to simplify data processing or to improve system performance. However, to the best of our knowledge, patterns have never been used in the context of Web archiving. Web archiving is the process of continuously collecting and preserving portions of the World Wide Web for future generations. In this paper, we show how patterns of page changes can be useful tools to efficiently archive Websites. We first define our pattern model that describes the importance of page changes. Then, we present the strategy used to (i) extract the temporal evolution of page changes, (ii) discover patterns, to (iii) exploit them to improve Web archives. The archive of French public TV channels France T{\'{e}}l{\'{e}}visions is chosen as a case study to validate our approach. Our experimental evaluation based on real Web pages shows the utility of patterns to improve archive quality and to optimize indexing or storing. {\textcopyright} 2012 Springer-Verlag.},
author = {Saad, Myriam Ben and Gan??arski, St??phane},
doi = {10.1007/s00799-012-0094-z},
file = {:S$\backslash$:/SciencePhdSorted/International Journal on Digital Libraries/Archiving the web using page changes patterns A case study{\_}Saad, Ganarski{\_}2012.pdf:pdf},
isbn = {14325012 (ISSN)},
issn = {14325012},
journal = {International Journal on Digital Libraries},
keywords = {Importance of page changes,Pattern,Temporal completeness,Web archiving},
number = {1},
pages = {33--49},
pmid = {1197168439},
title = {{Archiving the web using page changes patterns: A case study}},
volume = {13},
year = {2012}
}
@article{Thangaraj2014,
abstract = {The current web IR system retrieves relevant information only based on the keywords which is inadequate for that vast amount of data. It provides limited capabilities to capture the concepts of the user needs and the relation between the keywords. These limitations lead to the idea of the user conceptual search which includes concepts and meanings. This study deals with the Semantic Based Information Retrieval System for a semantic web search and presented with an improved algorithm to retrieve the information in a more efficient way. This architecture takes as input a list of plain keywords provided by the user and the query is converted into semantic query. This conversion is carried out with the help of the domain concepts of the pre-existing domain ontologies and a third party thesaurus and discover semantic relationship between them in runtime. The relevant information for the semantic query is retrieved and ranked according to the relevancy with the help of an improved algorithm. The performance analysis shows that the proposed system can improve the accuracy and effectiveness for retrieving relevant web documents compared to the existing systems.?? 2014 Elsevier Ltd. All rights reserved.},
author = {Thangaraj, M. and Sujatha, G.},
doi = {10.1016/j.eswa.2014.07.017},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/An architectural design for effective information retrieval in semantic web{\_}Thangaraj, Sujatha{\_}2014.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Information retrieval,Ontology,Semantic query,Semantic search,Semantic web},
number = {18},
pages = {8225--8233},
publisher = {Elsevier Ltd},
title = {{An architectural design for effective information retrieval in semantic web}},
url = {http://dx.doi.org/10.1016/j.eswa.2014.07.017},
volume = {41},
year = {2014}
}
@article{Choi2012,
abstract = {Function-Oriented Search (FOS) has been proposed as a tool for use in searching patent databases to find existing solutions to new problems. To implement FOS effectively, a well-structured Function-based Technology Database (FTDB) is required. An FTDB is a data repository of technology information represented as "function". To implement an FTDB, four features should be addressed: continual data updating, limited area searching, function generalization, and semantics handling. In this paper, we consider these features to suggest a fact-oriented ontological approach to implementing an FTDB by Subject-Action-Object (SAO)-based function modeling of patents. The proposed approach uses fact-oriented ontology modeling of SAO structures extracted from patent documents, and implements an FTDB, which is an SAO-based patent retrieval system to support FOS. We also verify the feasibility of the proposed approach to by using it to conduct case studies of patent retrieval. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Choi, Sungchul and Kang, Dongwoo and Lim, Joohyung and Kim, Kwangsoo},
doi = {10.1016/j.eswa.2012.02.041},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/A fact-oriented ontological approach to SAO-based function modeling of patents for implementing Function-based Technology Database{\_}Choi.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Fact-oriented ontology,Function Oriented Search,Function based Technology Database,Patent analysis,TRIZ},
number = {10},
pages = {9129--9140},
publisher = {Elsevier Ltd},
title = {{A fact-oriented ontological approach to SAO-based function modeling of patents for implementing Function-based Technology Database}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.02.041},
volume = {39},
year = {2012}
}
@article{Chau2005,
author = {Chau, Rowena and Yeh, Chung-hsing and Smith, Kate A and Smith, Kate},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/A P ersonalized M ultilingual W eb Content M iner PMW eb M iner{\_}Chau et al.{\_}2005.pdf:pdf},
pages = {956--965},
title = {{A P ersonalized M ultilingual W eb Content M iner : PMW eb M iner}},
year = {2005}
}
@article{Lazarinis2009,
abstract = {With increasingly higher numbers of non-English language web searchers the problems of efficient handling of non-English Web documents and user queries are becoming major issues for search engines. The main aim of this review paper is to make researchers aware of the existing problems in monolingual non-English Web retrieval by providing an overview of open issues. A significant number of papers are reviewed and the research issues investigated in these studies are categorized in order to identify the research questions and solutions proposed in these papers. Further research is proposed at the end of each section. [ABSTRACT FROM AUTHOR]$\backslash$nCopyright of Information Retrieval is the property of Springer Science {\&} Business Media B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Lazarinis, Fotis and Vilares, Jes{\'{u}}s and Tait, John and Efthimiadis, Efthimis N.},
doi = {10.1007/s10791-009-9093-0},
file = {:S$\backslash$:/SciencePhdSorted/Information Retrieval/Current research issues and trends in non-English Web searching{\_}Lazarinis et al.{\_}2009.pdf:pdf},
isbn = {1079100990930},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Encoding handling,Indexing,Language identification,Lemmatization,Non-English retrieval,Query log analysis,Segmentation,Stemming,Stopwords,Web searching},
number = {3},
pages = {230--250},
title = {{Current research issues and trends in non-English Web searching}},
volume = {12},
year = {2009}
}
@article{Saha2009,
author = {Saha, Suman and Murthy, C A and Pal, Sankar K},
doi = {10.3233/FI-2009-198},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Tensor Framework and Combined Symmetry for Hypertext Mining{\_}Saha, Murthy, Pal{\_}2009.pdf:pdf},
keywords = {hypertext,internal structure,similarity measure,tensor space},
pages = {215--234},
title = {{Tensor Framework and Combined Symmetry for Hypertext Mining}},
volume = {97},
year = {2009}
}
@article{Zhang2005,
author = {Zhang, Huaxiang and Huang, Shangteng},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/An Incremental Approach to Link Evaluation in Topic-Driven Web Resource Discovery{\_}Zhang, Huang{\_}2005.pdf:pdf},
isbn = {978-3-540-26224-4, 978-3-540-32440-9},
issn = {03029743},
pages = {301--310},
title = {{An Incremental Approach to Link Evaluation in Topic-Driven Web Resource Discovery}},
url = {http://www.springerlink.com/content/m3uhj0x396w36wg6/{\%}5Cnhttp://www.springerlink.com/content/m3uhj0x396w36wg6/fulltext.pdf},
volume = {3521},
year = {2005}
}
@article{McKerlich2013d,
abstract = {(Open Nottingham OER report; survey of 51 undergraduates' use of OER.)},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {McKerlich, Ross and Ives, Cindy and McGreal, Rory},
doi = {10.1002/asi},
eprint = {0803.1716},
file = {:S$\backslash$:/SciencePhdSorted/International Review of Research in Open and Distance Learning/Measuring use and creation of open educational resources in higher education{\_}McKerlich, Ives, McGreal{\_}2013(3).pdf:pdf;:S$\backslash$:/SciencePhdSorted/International Review of Research in Open and Distance Learning/Measuring use and creation of open educational resources in higher education{\_}McKerlich, Ives, McGreal{\_}2013(4).pdf:pdf;:S$\backslash$:/SciencePhdSorted/International Review of Research in Open and Distance Learning/Measuring use and creation of open educational resources in higher education{\_}McKerlich, Ives, McGreal{\_}2013(2).pdf:pdf;:S$\backslash$:/SciencePhdSorted/International Review of Research in Open and Distance Learning/Measuring use and creation of open educational resources in higher education{\_}McKerlich, Ives, McGreal{\_}2013.pdf:pdf},
isbn = {9783848215430},
issn = {14923831},
journal = {International Review of Research in Open and Distance Learning},
keywords = {OER,Open educational resources,Open learning,Open textbooks},
number = {4},
pages = {90--103},
pmid = {502955140},
title = {{Measuring use and creation of open educational resources in higher education}},
volume = {14},
year = {2013}
}
@article{Vidya2016,
author = {Vidya, P V and C, Reghu Raj P and Jayan, V},
doi = {10.1016/j.protcy.2016.05.102},
file = {:S$\backslash$:/SciencePhdSorted/Procedia Technology/Web Page Ranking Using Multilingual Information Search Algorithm - A Novel Approach{\_}Vidya, C, Jayan{\_}2016.pdf:pdf},
issn = {2212-0173},
journal = {Procedia Technology},
keywords = {google search,google translate api,inverted index,multilingual information retrieval,word frequency},
pages = {1240--1247},
publisher = {The Author(s)},
title = {{Web Page Ranking Using Multilingual Information Search Algorithm - A Novel Approach}},
url = {http://dx.doi.org/10.1016/j.protcy.2016.05.102},
volume = {24},
year = {2016}
}
@article{Iliou2017,
author = {Iliou, Christos and Kalpakis, George and Tsikrika, Theodora and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
doi = {10.1186/s13635-017-0064-5},
file = {:S$\backslash$:/SciencePhdSorted/EURASIP Journal on Information Security/Hybrid focused crawling on the Surface and the Dark Web{\_}Iliou et al.{\_}2017.pdf:pdf},
issn = {2510-523X},
journal = {EURASIP Journal on Information Security},
keywords = {Focused crawling,Dark web,Darknets,Tor,I2P,Freenet,dark web,darknets,dynamic linear combination,focused crawling,freenet,i2p,tor},
number = {1},
pages = {11},
publisher = {EURASIP Journal on Information Security},
title = {{Hybrid focused crawling on the Surface and the Dark Web}},
url = {https://jis-eurasipjournals.springeropen.com/articles/10.1186/s13635-017-0064-5},
volume = {2017},
year = {2017}
}
@article{Qi2009,
abstract = {Classification of Web page content is essential to many tasks inWeb information retrieval such as maintaining Web directories and focused crawling. The uncontrolled nature of Web content presents additional challenges to Web page classification as compared to traditional text classification, but the interconnected nature of hypertext also provides features that can assist the process. As we review work in Web page classification, we note the importance of these Web-specific features and algorithms, describe state-of-the-art practices, and track the underlying assumptions behind the use of information from neighboring pages.},
author = {Qi, Xiaoguang and Davison, Brian D.},
doi = {10.1145/1459352.1459357},
file = {:S$\backslash$:/SciencePhdSorted/ACM Computing Surveys/Web page classification{\_}Qi, Davison{\_}2009.pdf:pdf},
isbn = {1581139128},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {2},
pages = {1--31},
pmid = {11717453},
title = {{Web page classification}},
volume = {41},
year = {2009}
}
@article{St2003,
author = {St, Townsend and Francisco, San},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Web Business Intelligence Mining the Web for Actionable Knowledge{\_}St, Francisco{\_}2003.pdf:pdf},
keywords = {and,com,going from page to,ii,iii,ing or,like yahoo,page,randomly brows-,surfing,the use of a,the web,to find pages rele-,vant to certain concepts,web index or portal,www,yahoo},
number = {2},
pages = {191--207},
title = {{Web Business Intelligence : Mining the Web for Actionable Knowledge}},
volume = {15},
year = {2003}
}
@article{Gali2017,
author = {Gali, Najlah and Mariescu-istodor, Radu and Fr{\"{a}}nti, Pasi},
doi = {10.1016/j.eswa.2017.02.045},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Using linguistic features to automatically extract web page title{\_}Gali, Mariescu-istodor, Fr{\"{a}}nti{\_}2017.pdf:pdf},
keywords = {Web content mining,Information extraction,Title ex},
pages = {296--312},
publisher = {Elsevier Ltd},
title = {{Using linguistic features to automatically extract web page title}},
volume = {79},
year = {2017}
}
@article{Fazzinga2011,
abstract = {Many experts predict that the next huge step forward in Web information technology will be achieved by adding semantics to Web data, and will possibly consist of (some form of) the Semantic Web. In this paper, we present a novel approach to Semantic Web search, called Serene, which allows for a semantic processing of Web search queries, and for evaluating complex Web search queries that involve reasoning over the Web. More specifically, we first add ontological structure and semantics to Web pages, which then allows for both attaching a meaning to Web search queries and Web pages, and for formulating and processing ontology-based complex Web search queries (i.e., conjunctive queries) that involve reasoning over the Web. Here, we assume the existence of an underlying ontology (in a lightweight ontology language) relative to which Web pages are annotated and Web search queries are formulated. Depending on whether we use a general or a specialized ontology, we thus obtain a general or a vertical Semantic Web search interface, respectively. That is, we are actually mapping the Web into an ontological knowledge base, which then allows for Semantic Web search relative to the underlying ontology. The latter is then realized by reduction to standard Web search on standard Web pages and logically completed ontological annotations. That is, standard Web search engines are used as the main inference motor for ontology-based Semantic Web search. We develop the formal model behind this approach and also provide an implementation in desktop search. Furthermore, we report on extensive experiments, including an implemented Semantic Web search on the Internet Movie Database. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Fazzinga, Bettina and Gianforme, Giorgio and Gottlob, Georg and Lukasiewicz, Thomas},
doi = {10.1016/j.websem.2011.08.003},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Web Semantics/Semantic Web search based on ontological conjunctive queries{\_}Fazzinga et al.{\_}2011.pdf:pdf},
isbn = {3642118283},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Conjunctive queries,Description logics,Ontologies,Semantic Web,Semantic Web search,Web search},
number = {4},
pages = {453--473},
publisher = {Elsevier B.V.},
title = {{Semantic Web search based on ontological conjunctive queries}},
url = {http://dx.doi.org/10.1016/j.websem.2011.08.003},
volume = {9},
year = {2011}
}
@article{Armentano2014,
abstract = {An appropriate promotion, distribution and dissemination of scientific, artistic and technology developments can foster the collaboration between a country's productive and academic sectors. The purpose of this paper is to present a novel search engine aiming at helping people to access science and technology advances, researchers and institutions working in specific areas of research. Our search engine first collects information disseminated on the Web in academic institution sites and in researchers personal homepages. Then, after intensive text processing, it summarizes the information in an enriched and user-friendly presentation oriented to non-expert users. Stable performance and an acceptable level of effectiveness for automatic named entities recognition indicate the potential of our approach for bridging the gap between the heterogeneous and unstructured information available on the Web about the research and development advances in a country and the innovation required by the productive sectors. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Armentano, Marcelo G. and Godoy, Daniela and Campo, Marcelo and Amandi, Analia},
doi = {10.1016/j.eswa.2013.10.023},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/NLP-based faceted search Experience in the development of a science and technology search engine{\_}Armentano et al.{\_}2014.pdf:pdf},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Faceted search,Named entities recognition,Natural language processing,Vertical search engines},
number = {6},
pages = {2886--2896},
publisher = {Elsevier Ltd},
title = {{NLP-based faceted search: Experience in the development of a science and technology search engine}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.10.023},
volume = {41},
year = {2014}
}
@article{Ai2006,
abstract = {Market Intelligence (MI) is information and knowledge relevant to an enterprise market decision-making process. MI acquiring is a key activity for enterprises to keep predominance in furious market competition. The quick-developed Internet provides abundant information resources, but there is a lack of effective new approaches and models for MI acquiring. In this paper, we concentrate on MI mining based on B2C websites. We develop a specialized B2C websites mining model by syncretizing technology of web mining, knowledge representation, data warehouse and metadata. We design a web content mining algorithm integrating several web mining methods, and perform the digital camera sales experiments to validate it.},
author = {Ai, Danxiang and Zhang, Yufeng and Zuo, Hui and Wang, Quan},
doi = {10.1007/11906070_16},
file = {:S$\backslash$:/SciencePhdSorted/Web Information Systems - Wise 2006 Workshops, Proceedings/Web Content Mining for Market Intelligence Acquiring from B2C Websites{\_}Ai et al.{\_}2006.pdf:pdf},
isbn = {3540476636},
issn = {0302-9743},
journal = {Web Information Systems - Wise 2006 Workshops, Proceedings},
keywords = {Market Intelligence acquiring,web content mining},
number = {70573082},
pages = {159--170},
title = {{Web Content Mining for Market Intelligence Acquiring from B2C Websites}},
url = {http://apps.webofknowledge.com.ezproxy.unal.edu.co/full{\_}record.do?product=WOS{\&}search{\_}mode=GeneralSearch{\&}qid=1{\&}SID=4CK6BMec94VDv4wUVgW{\&}page=3{\&}doc=23{\&}cacheurlFromRightClick=no{\%}5Cnhttp://link.springer.com/10.1007/11906070{\_}16},
volume = {4256},
year = {2006}
}
@book{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar and Schutze, Hinrich},
booktitle = {Online},
doi = {10.1109/LPT.2009.2020494},
eprint = {0521865719 9780521865715},
file = {:S$\backslash$:/SciencePhdSorted/Online/An Introduction to Information Retrieval{\_}Manning, Raghavan, Schutze{\_}2009.pdf:pdf},
isbn = {0521865719},
issn = {13864564},
keywords = {keyword},
number = {c},
pages = {569},
pmid = {10575050},
title = {{An Introduction to Information Retrieval}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538},
year = {2009}
}
@article{Harper2012,
abstract = {The World Wide Web (Web) is in constant evolutionary change. This evolution occurs along many fronts and is led by infrastructure developers, Web designers, technologists, and users. These multiple stake–holders ensure that the Web is a heterogeneous entity, not just in the nature of the content, but in the technology and agents used to deliver and render that content. It is precisely this heterogeneity which gives the Web its strength and its weakness. A weakness in technology adoption leading to an increasing disconnect between the actual user experience and the expected experience of the technology stakeholders. We are interested in the human factors surrounding the evolution of the Web interface; and believe that the wait is always too long for new accessibility recommendations, guidelines, and technology to be adopted. In this case, we describe a ten-year longitudinal study comprising approximately 6,000 home pages. From this study we conclude that as a ‘rule-of-thumb' mainstream technology is adopted at about 15{\%} within the first three years, incremental version releases are adopted at about 10{\%} within the first three years. However, sites which are most popular often exhibit enhanced adoption rates of between 10 and 15{\%} over the same period. In addition, we see that accessibility guidelines are mostly ignored with only a 10{\%} adoption rate after more than ten years. From this we infer that, for maximum accessibility adoption, guidelines might be supported and reflected in mainstream specifications instead of remaining only as a separate document.},
author = {Harper, Simon and Chen, Alex Q.},
doi = {10.1007/s11280-011-0130-8},
file = {:S$\backslash$:/SciencePhdSorted/World Wide Web/Web accessibility guidelines A lesson from the evolving Web{\_}Harper, Chen{\_}2012.pdf:pdf},
isbn = {1128001101},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Web accessibility,evolution,measurement,visual impairment},
number = {1},
pages = {61--88},
title = {{Web accessibility guidelines: A lesson from the evolving Web}},
volume = {15},
year = {2012}
}
@article{Deursen2015,
author = {Deursen, Arie Van and Mesbah, Ali and Nederlof, Alex},
doi = {10.1016/j.scico.2014.09.005},
file = {:S$\backslash$:/SciencePhdSorted/Science of Computer Programming/Science of Computer Programming Crawl-based analysis of web applications Prospects and challenges{\_}Deursen, Mesbah, Nederlof{\_}2015.pdf:pdf},
issn = {0167-6423},
journal = {Science of Computer Programming},
pages = {173--180},
publisher = {Elsevier B.V.},
title = {{Science of Computer Programming Crawl-based analysis of web applications : Prospects and challenges}},
url = {http://dx.doi.org/10.1016/j.scico.2014.09.005},
volume = {97},
year = {2015}
}
@article{Evrim2014,
abstract = {Finding the relevant set of information that satisfies an information$\backslash$nrequest of a Web user in the availability of today's vast amount of$\backslash$ndigital data is becoming a challenging problem. Currently, available$\backslash$nInformation Retrieval (IR) Systems are designed to return long lists of$\backslash$nresults, only a few of which are relevant for a specific user. In this$\backslash$npaper, an IR method called Context-Based Information Analysis (CONIA)$\backslash$nthat investigates the context information of the user and user's$\backslash$ninformation request to provide relevant results for the given domain$\backslash$nusers is introduced. In this paper, relevance is measured by the$\backslash$nsemantics of the information provided in the documents. The information$\backslash$nextracted from lexical and domain ontologies is integrated by the user's$\backslash$ninterest information to expand the terms entered in the request. The$\backslash$nobtained set of terms is categorized by a novel approach, and the$\backslash$nrelations between the categories are obtained from the ontologies. This$\backslash$ncategorization is used to improve the quality of the document selection$\backslash$nby going beyond checking the availability of the words in the document$\backslash$nby analyzing the semantic composition of the mapped terms.},
author = {Evrim, Vesile and McLeod, Dennis},
doi = {10.1007/s10115-012-0493-x},
file = {:S$\backslash$:/SciencePhdSorted/Knowledge and Information Systems/Context-based information analysis for the Web environment{\_}Evrim, McLeod{\_}2014.pdf:pdf},
isbn = {0219-1377},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Context-based search,Information retrieval,Ontology,Query,Relevance},
number = {1},
pages = {109--140},
title = {{Context-based information analysis for the Web environment}},
volume = {38},
year = {2014}
}
@article{BaazaouiZghal2013,
abstract = {Ontologies have proven to be useful in the area of Information Retrieval and the biomedical informatics community has acknowledged, in recent years, their utility. However, building and updating manually ontologies is a long and tedious task. This paper proposes a system that allows any search engine to develop its semantic layer by applying ontology learning techniques on Web snippets and applies it to a well-known medical digital library, PubMed. The new system (SemPubMed) automatically builds new ontology fragments related to the user's query and then it reformulates queries using the new concepts in order to improve information retrieval. Our system has endured a twofold evaluations. On the one hand, we have evaluated the quality of the modular ontologies built by the system. On the other hand, we have studied how the semantic reformulation of the queries has led to an improvement of the quality of the results given by PubMed, both in terms of precision and recall. Obtained results show that adding semantic layer to PubMed enables an improvement of query reformulation and predicted ranking score.},
author = {{Baazaoui Zghal}, Hajer and Moreno, Antonio},
doi = {10.1007/s11042-013-1527-4},
file = {:S$\backslash$:/SciencePhdSorted/Multimedia Tools and Applications/A system for information retrieval in a medical digital library based on modular ontologies and query reformulation{\_}Baazaoui Zghal, More.pdf:pdf},
isbn = {1380-7501},
issn = {13807501},
journal = {Multimedia Tools and Applications},
keywords = {Medical digital library,Ontology,Query reformulation,Semantic information retrieval},
pages = {1--20},
title = {{A system for information retrieval in a medical digital library based on modular ontologies and query reformulation}},
year = {2013}
}
@article{Theocharopoulou2012,
abstract = {In this study, the goal is multifold. At first we present a summarized review of terms and facts regarding the branch of ecoinformatics, web mining and the semantic web. In Section 2 we provide some related work derived from the current literature upon the web mining and the production of semantic content. The main part of our work follows presenting a notional model for building semantic content through 2-level web mining. This is achieved in web sites containing environmental data. We conclude mentioning the importance of this contribution from different points of view. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
author = {Theocharopoulou, Georgia and Giannakis, Konstantinos},
doi = {10.1007/978-3-642-33412-2_42},
file = {:S$\backslash$:/SciencePhdSorted/IFIP Advances in Information and Communication Technology/Web mining to create semantic content A case study for the environment{\_}Theocharopoulou, Giannakis{\_}2012.pdf:pdf},
isbn = {9783642334115},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {ecoinformatics,semantic web,web mining},
number = {PART 2},
pages = {411--420},
title = {{Web mining to create semantic content: A case study for the environment}},
volume = {382 AICT},
year = {2012}
}
@article{Song2015,
author = {Song, Dandan and Sun, Fei and Liao, Lejian},
doi = {10.1007/s10115-013-0687-x},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/A hybrid approach for content extraction with text density and visual importance of DOM nodes{\_}Song, Sun, Liao{\_}2015.pdf:pdf},
keywords = {content extraction,text density,visual importance},
pages = {75--96},
title = {{A hybrid approach for content extraction with text density and visual importance of DOM nodes}},
year = {2015}
}
@article{Xu2016a,
abstract = {With the rapid proliferation of online information, how to find useful information, such as suitable jobs, appropriate experts, and proper projects, is really an important problem. Recommendation technique, as one of emerging tools to deal with information overload and information asymmetry, is critically important for providing personalized online information services. With the increase of R{\&}D investment in government and industry, such as high-tech companies and advanced manufacturing enterprises, more and more R{\&}D project information are launched in public websites for cooperation. When the number of online information and users is extremely huge, how to effectively recommend R{\&}D project opportunities to related researchers and practitioners is a challenging and complex task. In this paper, a novel two-stage method is proposed for R{\&}D project opportunity recommendation. An information filtering method is first offered to identity proper R{\&}D projects as a candidate set. Then, an information aggregation model with various constraints is suggested to recommend appropriate R{\&}D projects for applicants. The proposed method has been implemented in an online research community - ScholarMate (www.scholarmate.com). An online user study has been conducted and the evaluation results exhibit that the proposed method is more effective than existing ones.},
archivePrefix = {arXiv},
arxivId = {1612.00733},
author = {Xu, Wei and Sun, Jianshan and Ma, Jian and Du, Wei},
doi = {10.1016/j.jnca.2015.01.003},
eprint = {1612.00733},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Network and Computer Applications/A personalized information recommendation system for R{\&}D project opportunity finding in big data contexts{\_}Xu et al.{\_}2016.pdf:pdf},
isbn = {9781450337397},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Big data analytics,Online information services,R{\&}D projects,Recommendation,Research social network},
pages = {362--369},
title = {{A personalized information recommendation system for R{\&}D project opportunity finding in big data contexts}},
volume = {59},
year = {2016}
}
@article{Khalil2017,
author = {Khalil, Salim and Fakir, Mohamed},
doi = {10.1016/j.softx.2017.04.004},
file = {:S$\backslash$:/SciencePhdSorted/SoftwareX/SoftwareX RCrawler An R package for parallel web crawling and scraping{\_}Khalil, Fakir{\_}2017.pdf:pdf},
issn = {23527110},
journal = {SoftwareX},
pages = {98--106},
publisher = {Elsevier B.V.},
title = {{SoftwareX RCrawler : An R package for parallel web crawling and scraping}},
url = {http://dx.doi.org/10.1016/j.softx.2017.04.004},
volume = {6},
year = {2017}
}
@article{Hsinchun1998,
abstract = {As Internet services based on the World-Wide Web become more popular, information overload has become a pressing research problem. Difficulties with search on Internet will worsen as the amount of on-line information increases. A scalable approach to Internet search is critical to the success of Internet services and other current and future National Information Infrastructure (Nil) applications. As part of the ongoing Illinois Digital Library Initiative project, this research proposes an intelligent personal spider (agent) approach to Internet searching. The approach, which is grounded on automatic textual analysis and general-purpose search algorithms, is expected to be an improvement over the current static and inefficient Internet searches. In this experiment, we implemented Internet personal spiders based on best first search and genetic algorithm techniques. These personal spiders can dynamically take a user's selected starting homepages and search for the most closely related homepages in the web, based on the links and keyword indexing. A plain, static CGI/HTML-based interface was developed earlier, followed by a recent enhancement of a graphical, dynamic Java-based interface. Preliminary evaluation results and two working prototypes (available for Web access) are presented. Although the examples and evaluations presented are mainly based on Internet applications, the applicability of the proposed techniques to the potentially more rewarding Intranet applications should be obvious. In particular, we believe the proposed agent design can be used to locate organization-wide information, to gather new, time-critical organizational information, and to support team-building and communication in Intranets. {\textcopyright} 1998 Elsevier Science B.V. All rights reserved.},
author = {Hsinchun, Chen and Yi-Ming, Chung and Ramsey, Marshall and Yang, Christopher C.},
doi = {10.1016/S0167-9236(98)00035-9},
file = {:S$\backslash$:/SciencePhdSorted/Decision Support Systems/An intelligent personal spider (agent) for dynamic InternetIntranet searching{\_}Hsinchun et al.{\_}1998.pdf:pdf},
isbn = {0167-9236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Agents,Evolutionary programming,Information retrieval,Internet,Intranet,Java,Machine learning,Semantic retrieval,Spider,World-Wide Web},
number = {1},
pages = {41--58},
title = {{An intelligent personal spider (agent) for dynamic Internet/Intranet searching}},
volume = {23},
year = {1998}
}
@article{Vaughan2007,
abstract = {The study examined search engine coverage of websites across countries and domains. Websites in four domains (commercial, educational, governmental, and organizational) from four countries (U.S., China, Singapore, and Taiwan) were randomly sampled by custom-built computer programs and then manually filtered for their suitability for the study. Representation of the 1,664 sampled sites in four major search engines (Google, Yahoo!, MSN, and Yahoo! China) was examined in terms of whether the site was cov- ered and the number of pages indexed by the search engines. The study found that U.S. sites received higher coverage rates than their counterparts in other countries. The language of a site did not affect the site's chance of being indexed by search engines. Sites that were more visible had a higher chance of being indexed, but this factor did not seem to explain the differentiated coverage across countries. Yahoo! China provided better coverage of sites from China and surrounding regions than its global counterpart, Yahoo!. The poor coverage of Chinese commercial and governmental sites is noted and the implications are discussed in light of the tremendous development of the Web in China.},
author = {Vaughan, Liwen and Zhang, Yanjun},
doi = {10.1111/j.1083-6101.2007.00355.x},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Computer-Mediated Communication/Equal representation by search engines A comparison of websites across countries and domains{\_}Vaughan, Zhang{\_}2007.pdf:pdf},
isbn = {10836101},
issn = {10836101},
journal = {Journal of Computer-Mediated Communication},
number = {3},
pages = {888--909},
pmid = {25317335},
title = {{Equal representation by search engines? A comparison of websites across countries and domains}},
volume = {12},
year = {2007}
}
@article{Thorleuchter2015,
author = {Thorleuchter, D and Poel, D Van Den},
doi = {10.1016/j.futures.2014.12.007},
file = {:S$\backslash$:/SciencePhdSorted/Futures/Idea mining for web-based weak signal detection{\_}Thorleuchter, Poel{\_}2015.pdf:pdf},
issn = {0016-3287},
journal = {Futures},
pages = {25--34},
publisher = {Elsevier Ltd},
title = {{Idea mining for web-based weak signal detection}},
url = {http://dx.doi.org/10.1016/j.futures.2014.12.007},
volume = {66},
year = {2015}
}
@article{Atkinson2014,
abstract = {Secondary-school teachers are in constant need of finding relevant digital resources to support specific didactic goals. Unfortunately, generic search engines do not allow them to identify learning objects among semi-structured candidate educational resources, much less retrieve them by teaching goals. This article describes a multi-strategy approach for semantically guided extraction, indexing and search of educational metadata; it combines machine learning, concept analysis, and corpus-based natural language processing techniques. The overall model was validated by comparing extracted metadata against standard search methods and heuristic-based techniques for Classification Accuracy and Metadata Quality (as evaluated by actual teachers), yielding promising results and showing that this semantically guided metadata extraction can effectively enhance access and use of educational digital material.},
author = {Atkinson, John and Gonzalez, Andrea and Munoz, Mauricio and Astudillo, Hernan},
doi = {10.1007/s10489-014-0557-6},
file = {:S$\backslash$:/SciencePhdSorted/Applied Intelligence/Web metadata extraction and semantic indexing for learning objects extraction{\_}Atkinson et al.{\_}2014.pdf:pdf},
isbn = {9783642385766},
issn = {0924669X},
journal = {Applied Intelligence},
keywords = {Learning objects,Machine learning,Metadata extraction,Semantic analysis,Text mining},
number = {2},
pages = {649--664},
title = {{Web metadata extraction and semantic indexing for learning objects extraction}},
volume = {41},
year = {2014}
}
@article{Fiirnkranz2005,
abstract = {The World-Wide Web provides every internet citizen with access to an abundance of information, but it becomes increasingly difficult to identify the relevant pieces of information. Research in web mining tries to address this problem by applying techniques from data mining and machine learning Web data and documents. This chapter provides a brief overview of web mining techniques and research areas, most notably hypertext classification, wrapper induction, recommender systems and web usage mining.},
author = {Fiirnkranz, Johannes},
file = {:S$\backslash$:/SciencePhdSorted/Data Mining and Knowledge Discovery Handbook/Web Mining{\_}Fiirnkranz{\_}2005.pdf:pdf},
isbn = {978-0-387-25465-4},
journal = {Data Mining and Knowledge Discovery Handbook},
pages = {899--920},
title = {{Web Mining}},
year = {2005}
}
@article{Mavridis2015,
author = {Mavridis, Themistoklis and Symeonidis, Andreas L},
doi = {10.1016/j.engappai.2015.02.002},
file = {:S$\backslash$:/SciencePhdSorted/Engineering Applications of Artificial Intelligence/Engineering Applications of Arti fi cial Intelligence Identifying valid search engine ranking factors in a Web 2 . 0 and Web 3 . 0 conte.pdf:pdf},
issn = {0952-1976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Content quality,Search engine optimization,Search engine ranking factors analysis,Semantic web,Social web,analysis,search engine optimization,search engine ranking factors},
pages = {75--91},
publisher = {Elsevier},
title = {{Engineering Applications of Arti fi cial Intelligence Identifying valid search engine ranking factors in a Web 2 . 0 and Web 3 . 0 context for building ef fi cient SEO mechanisms}},
url = {http://dx.doi.org/10.1016/j.engappai.2015.02.002},
volume = {41},
year = {2015}
}
@article{Calaresu2015,
abstract = {Purpose – The purpose of this article is to explore and conceptualize the Semantic Web as a term that has been widely mentioned in the literature of library and information science. More specifically, its aim is to shed light on the evolution of the Web and to highlight a previously proposed means of attempting to improve automated manipulation of Web-based data in the context of a rapidly expanding base of both users and digital content. Design/methodology/approach – The conceptual analysis presented in this paper adopts a three-dimensional model for the discussion of Semantic Web. The first dimension focuses on Semantic Web's basic nature, purpose and history, as well as the current state and limitations of modern search systems and related software agents. The second dimension focuses on critical knowledge structures such as taxonomies, thesauri and ontologies which are understood as fundamental elements in the creation of a Semantic Web architecture. In the third dimension, an alternative conceptual model is proposed, one, which unlike more commonly prevalent Semantic Web models, offers a greater emphasis on describing the proposed structure from an interpretive viewpoint, rather than a technical one. This paper adopts an interpretive, historical and conceptual approach to the notion of the Semantic Web by reviewing the literature and by analyzing the developments associated with the Web over the past three decades. It proposes a simplified conceptual model for easy understanding. Findings – The paper provides a conceptual model of the Semantic Web that encompasses four key strata, namely, the body of human users, the body of software applications facilitating creation and consumption of documents, the body of documents themselves and a proposed layer that would improve automated manipulation of Web-based data by the software applications. Research limitations/implications – This paper will facilitate a better conceptual understanding of the Semantic Web, and thereby contribute, in a small way, to the larger body of discourse surrounding it. The conceptual model will provide a reference point for education and research purposes. Originality/value – This paper provides an original analysis of both conceptual and technical aspects of Semantic Web. The proposed conceptual model provides a new perspective on this subject.},
author = {Calaresu, Michael and Shiri, Ali},
doi = {10.1108/LR-09-2014-0097},
file = {:S$\backslash$:/SciencePhdSorted/Library Review/Understanding Semantic Web a conceptual model{\_}Calaresu, Shiri{\_}2015.pdf:pdf},
isbn = {00242535},
issn = {0024-2535},
journal = {Library Review},
number = {1/2},
pages = {82--100},
pmid = {100835725},
title = {{Understanding Semantic Web: a conceptual model}},
url = {http://www.emeraldinsight.com/doi/10.1108/LR-09-2014-0097},
volume = {64},
year = {2015}
}
@article{Jin2014,
abstract = {In market-driven product design, customer requirements (CRs) are usually obtained from consumer sur- veys. However, valuable CRs can also be found in a large number of online reviews. Largely due to their free text nature and the quantity, these reviews are often neglected and are seldom utilised directly by designers. In this work, one important question in quality function deployment on how to prioritise engineering characteristics (ECs) is investigated. Customer opinions concerning ECs are extracted from online reviews. By taking advantage of such opinion information, an ordinal classification approach is proposed to prioritise ECs. In a pairwise manner, in which customer opinions are deemed as features and the overall customer satisfactions are regarded as the target values, the weights of ECs are derived. Furthermore, an integer linear programming model is implemented to convert the pairwise results into the original customer satisfaction ratings. Finally, an exploratory case study is presented using reviews of four branded printers collected from Amazon and their analysis was conducted by two experienced design engineers. The experimental study reveals the merits of the proposed approach.},
author = {Jin, Jian and Ji, Ping and Liu, Ying},
doi = {10.1080/09544828.2014.984665},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Engineering Design/Prioritising engineering characteristics based on customer online reviews for quality function deployment{\_}Jin, Ji, Liu{\_}2014.pdf:pdf},
issn = {14661837},
journal = {Journal of Engineering Design},
keywords = {Customer reviews,Engineering characteristics,Product design,QFD,User review analysis},
pages = {303--324},
title = {{Prioritising engineering characteristics based on customer online reviews for quality function deployment}},
volume = {25},
year = {2014}
}
@article{Nesi2016,
author = {Nesi, Paolo and Pantaleo, Gianni and Tenti, Marco},
doi = {10.1016/j.engappai.2016.01.011},
file = {:S$\backslash$:/SciencePhdSorted/Engineering Applications of Artificial Intelligence/Engineering Applications of Arti fi cial Intelligence Geographical localization of web domains and organization addresses recognition by.pdf:pdf},
issn = {0952-1976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Data mining,Geocoding,Geographic Information Retrieval,Geoparsing,Hierarchical Clustering,Natural language processing,geographic information retrieval},
pages = {202--211},
publisher = {Elsevier},
title = {{Engineering Applications of Arti fi cial Intelligence Geographical localization of web domains and organization addresses recognition by employing natural language processing , Pattern Matching and clustering}},
url = {http://dx.doi.org/10.1016/j.engappai.2016.01.011},
volume = {51},
year = {2016}
}
@article{Velasquez2012,
author = {Velasquez, Juan D},
doi = {10.3233/IDA-2012-0526},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Web site keywords A methodology for improving gradually the web site text content{\_}Velasquez{\_}2012.pdf:pdf},
keywords = {web content mining,web site keywords,web site text content,web usage mining},
pages = {327--348},
title = {{Web site keywords : A methodology for improving gradually the web site text content}},
volume = {16},
year = {2012}
}
@article{Lee2017,
author = {Lee, Jong Jae and Lee, Jong Jae and Street, North Charles},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Submission Files Included in this PDF Submission Files Not Included in this PDF department of economics{\_}Lee, Lee, Street{\_}2017.pdf:pdf},
isbn = {1114427918},
keywords = {corresponding author},
title = {{Submission Files Included in this PDF Submission Files Not Included in this PDF department of economics}},
year = {2017}
}
@article{Kumar2016,
abstract = {In this digital era, marketing theory and practice are being transformed by increasing complexity due to information availability, higher reach and interactions, and faster speeds of transactions. These have led to the adoption of intelligent agent technologies (IATs) by many companies. As IATs are relatively new and technologically complex, several definitions are evolving, and the theory in this area is not yet fully developed. There is a need to provide structure and guidance to marketers to further this emerging stream of research. As a first step, this paper proposes a marketing-centric definition and a systematic taxonomy and framework. The authors, using a grounded theory approach, conduct an extensive literature review and a qualitative study in which interviews with managers from 50 companies in 22 industries reveal the importance of understanding IAT applications and adopting them. Further, the authors propose an integrated conceptual framework with several propositions regarding IAT adoption. This research identifies the gaps in the literature and the need for adoption of IATs in the future of marketing given changing consumer behavior and product and industry characteristics.},
author = {Kumar, V. and Dixit, Ashutosh and Javalgi, Rajshekar (Raj) G. and Dass, Mayukh},
doi = {10.1007/s11747-015-0426-9},
file = {:S$\backslash$:/SciencePhdSorted/Journal of the Academy of Marketing Science/Research framework, strategies, and applications of intelligent agent technologies (IATs) in marketing{\_}Kumar et al.{\_}2016.pdf:pdf},
isbn = {0092-0703},
issn = {00920703},
journal = {Journal of the Academy of Marketing Science},
keywords = {Grounded theory,Intelligent agent technologies,Marketing strategy},
number = {1},
pages = {24--45},
title = {{Research framework, strategies, and applications of intelligent agent technologies (IATs) in marketing}},
volume = {44},
year = {2016}
}
@article{Kaur2017,
author = {Kaur, Loveleen and Mishra, Ashutosh},
doi = {10.1016/j.jss.2016.11.028},
file = {:S$\backslash$:/SciencePhdSorted/The Journal of Systems {\&} Software/The Journal of Systems and Software Software component and the semantic Web An in-depth content analysis and integration history{\_}Kaur,.pdf:pdf},
issn = {0164-1212},
journal = {The Journal of Systems {\&} Software},
keywords = {Component-based software engineering,Semantic Web,,component-based software engineering},
pages = {152--169},
publisher = {Elsevier Inc.},
title = {{The Journal of Systems and Software Software component and the semantic Web : An in-depth content analysis and integration history}},
url = {http://dx.doi.org/10.1016/j.jss.2016.11.028},
volume = {125},
year = {2017}
}
@article{Kornai2004,
author = {Kornai, Andras and Thompson, Bradley},
file = {:S$\backslash$:/SciencePhdSorted/Proc. LREC'2004/Creating open language resources for {\{}Hungarian{\}}{\_}Kornai, Thompson{\_}2004.pdf:pdf},
journal = {Proc. LREC'2004},
pages = {203--210},
title = {{Creating open language resources for {\{}Hungarian{\}}}},
year = {2004}
}
@article{,
file = {:S$\backslash$:/SciencePhdSorted/Unknown/No Title{\_}Unknown{\_}Unknown(4).pdf:pdf},
title = {{No Title}}
}
@article{Urh,
author = {Urh, Marko},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/SEARCH ENGINE OPTIMIZATION UNDERSTANDING KEY ELEMENTS OF HIGH PAGE RANKING OPTIMIZACIJA ZA PRETRA{\v{Z}}IVA{\v{C}} E RAZUMEVANJE KLJU{\v{C}}NIH{\_}Urh{\_}Unkn.pdf:pdf},
keywords = {internet marketing,optimization,page ranking,search engine optimization},
title = {{SEARCH ENGINE OPTIMIZATION : UNDERSTANDING KEY ELEMENTS OF HIGH PAGE RANKING OPTIMIZACIJA ZA PRETRA{\v{Z}}IVA{\v{C}} E : RAZUMEVANJE KLJU{\v{C}}NIH}}
}
@article{Wu2017a,
author = {Wu, Yu-chieh},
doi = {10.1007/s10844-016-0395-6},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Intelligent Information Systems/Multilingual news extraction via stopword language model scoring{\_}Wu{\_}2017.pdf:pdf},
issn = {0925-9902},
journal = {Journal of Intelligent Information Systems},
keywords = {Information extraction,Content identification,Text,construction,content identification,corpus,information extraction,knowledge acquisition,text mining},
pages = {191--213},
publisher = {Journal of Intelligent Information Systems},
title = {{Multilingual news extraction via stopword language model scoring}},
url = {http://dx.doi.org/10.1007/s10844-016-0395-6},
year = {2017}
}
@article{Hersovici1998,
abstract = {This paper introduces the “shark search” algorithm, a refined version of one of the first dynamic Web search algorithms, experiments show significant improvements over the original fish-search algorithm. by Elsevier Science B.V. All rights reserved. Kqwords: Dynamic search; Site mapping; Resource discovery the “fish search”. The shark-search has been embodied into a dynamic Web site mapping that enables users to tailor Web},
author = {Hersovici, Michael and Jacovi, Michal and Maarek, Yoelle S. and Pelleg, Dan and Shtalhaim, Menachem and Ur, Sigalit},
doi = {10.1016/S0169-7552(98)00038-5},
file = {:S$\backslash$:/SciencePhdSorted/Computer Networks and ISDN Systems/The shark-search algorithm. An application tailored Web site mapping{\_}Hersovici et al.{\_}1998.pdf:pdf},
issn = {01697552},
journal = {Computer Networks and ISDN Systems},
keywords = {1,but it is basically,dynamic search,efficient for searching large,index that is actually,introduction and motivation,parts of the web,resource discovery,s,server,service,site mapping,static,stored on the search,the,this approach is pretty,use a previously built,web search services typically},
month = {apr},
number = {1-7},
pages = {317--326},
title = {{The shark-search algorithm. An application: tailored Web site mapping}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169755298000385},
volume = {30},
year = {1998}
}
@article{Matsudaira2012,
author = {Matsudaira, Kate},
doi = {10.1145/2567664},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Capturing and Structuring Data Mined from the Web{\_}Matsudaira{\_}2012.pdf:pdf},
number = {3},
pages = {5--7},
title = {{Capturing and Structuring Data Mined from the Web}},
year = {2012}
}
@article{Zhang2007,
abstract = {Although considerable research has been conducted in the field of hierarchical text categorization, little has been done on automatically collecting labeled corpus for building hierarchical taxonomies. In this paper, we propose an automatic method of collecting training samples to build hierarchical taxonomies. In our method, the category node is initially defined by some keywords, the web search engine is then used to construct a small set of labeled documents, and a topic tracking algorithm with keyword-based content normalization is applied to enlarge the training corpus on the basis of the seed documents. We also design a method to check the consistency of the collected corpus. The above steps produce a flat category structure which contains all the categories for building the hierarchical taxonomy. Next, linear discriminant projection approach is utilized to construct more meaningful intermediate levels of hierarchies in the generated flat set of categories. Experimental results show that the training corpus is good enough for statistical classification methods.},
author = {Zhang, Li and Li, Tao and Liu, Shixia and Pan, Yue},
doi = {10.1007/s10791-007-9028-6},
file = {:S$\backslash$:/SciencePhdSorted/Information Retrieval/An integrated system for building enterprise taxonomies{\_}Zhang et al.{\_}2007.pdf:pdf},
isbn = {1386-4564},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Consistency,Discriminant projection,Taxonomy},
number = {4-5},
pages = {365--391},
title = {{An integrated system for building enterprise taxonomies}},
volume = {10},
year = {2007}
}
@article{Keikha2017,
author = {Keikha, Andisheh and Ensan, Faezeh and Bagheri, Ebrahim},
doi = {10.1007/s10844-017-0466-3},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Query expansion using pseudo relevance feedback on wikipedia{\_}Keikha, Ensan, Bagheri{\_}2017.pdf:pdf},
issn = {0925-9902},
keywords = {Query suggestion,Query expansion,Wikipedia,Web sea,query expansion,query suggestion,web search,wikipedia},
publisher = {Journal of Intelligent Information Systems},
title = {{Query expansion using pseudo relevance feedback on wikipedia}},
year = {2017}
}
@article{Wu2012,
abstract = {The IR-style Web services discovery represents an important approach that applies proven techniques developed in the field of Information Retrieval (IR). Many studies exploited the Web Services Description Language (WSDL) syntax to extract useful service metadata for building indexes. However, a fundamental issue associated with this approach is the WSDL term tokenization. This paper proposes the application of three statistical methods for WSDL term tokenizationMDL, TP, and PPM. With the increasing need for effective IR-style Web services discovery facilities, term tokenization is of fundamental importance for properly indexing WSDL documents. We compare our applied methods with two baseline methods. The experiment suggests the superiority of MDL and PPM methods based on IR evaluation metrics. To the best of our knowledge, our work is the first to systematically investigate the issue of WSDL term tokenization for Web services discovery. Our solution can benefit source coding mining, in which a key step is to tokenize names (i.e. terms) of variables, functions, classes, modules, etc. for semantic analysis. Our methods could also be used for solving Web-related string tokenization problems such as URL analysis and Web scripts comprehension. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Wu, Chen},
doi = {10.1016/j.scico.2011.08.001},
file = {:S$\backslash$:/SciencePhdSorted/Science of Computer Programming/WSDL term tokenization methods for IR-style Web services discovery{\_}Wu{\_}2012.pdf:pdf},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {Data engineering,Information Retrieval,Source code mining,String tokenization,Web services discovery},
number = {3},
pages = {355--374},
publisher = {Elsevier B.V.},
title = {{WSDL term tokenization methods for IR-style Web services discovery}},
url = {http://dx.doi.org/10.1016/j.scico.2011.08.001},
volume = {77},
year = {2012}
}
@article{Yanes2017,
abstract = {Commercial Off-The-Shelf (COTS) components are coarse-grained software components that satisfy high-level requirements by integrating several services and offering several interfaces. They are usually used to build larger systems. The paper proposes an ontology-based recommender system for COTS components, that contributes to COTS-based development by improving COTS components identification. It combines into a single framework information retrieval technologies and knowledge about COTS components and users in order to provide the most relevant COTS components meeting users needs. The recommender system is based on (1) an ontology of COTS components, named ONTOCOTS, that describes COTS components and unifies their heterogeneous descriptions available on the Web, and (2) a user model that represents user preferences and interest domains. The proposed recommender system is broken down on two main processes. The first one is responsible for extracting information about COTS components from COTS repositories and representing it as ONTOCOTS instances. The second one is the recommendation process during which the user query is expanded using the linguistic ontology WordNet, and is used along with the user profile and the domain ontology ODP (Open Directory Project) to generate a formal query. Results list is ranked according to the satisfaction degree of user requirements and preferences. Experimentations show an amelioration in recommendations relevance by placing the relevant COTS components at the top of the recommendation list.},
author = {Yanes, Nacim and {Ben Sassi}, Sihem and {Hajjami Ben Ghezala}, Henda},
doi = {10.1016/j.jss.2017.07.031},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Systems and Software/Ontology-based recommender system for COTS components{\_}Yanes, Ben Sassi, Hajjami Ben Ghezala{\_}2017.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {COTS component,Identification,Information extraction,Ontology,Recommender system,User model},
pages = {283--297},
publisher = {Elsevier Inc.},
title = {{Ontology-based recommender system for COTS components}},
url = {http://dx.doi.org/10.1016/j.jss.2017.07.031},
volume = {132},
year = {2017}
}
@article{Nekrestyanov2002,
author = {Nekrestyanov, I S and Panteleeva, N V},
file = {:S$\backslash$:/SciencePhdSorted/Text/Text Retrieval Systems for the Web{\_}Nekrestyanov, Panteleeva{\_}2002.pdf:pdf},
journal = {Text},
number = {4},
pages = {207--225},
title = {{Text Retrieval Systems for the Web}},
volume = {28},
year = {2002}
}
@article{Ferrara2014a,
abstract = {Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of applications. Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. This survey aims at providing a structured and comprehensive overview of the literature in the field of Web Data Extraction. We provided a simple classification framework in which existing Web Data Extraction applications are grouped into two main classes, namely applications at the Enterprise level and at the Social Web level. At the Enterprise level, Web Data Extraction techniques emerge as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. At the Social Web level, Web Data Extraction techniques allow to gather a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities to analyze human behavior at a very large scale. We discuss also the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains.},
archivePrefix = {arXiv},
arxivId = {1207.0246},
author = {Ferrara, Emilio and {De Meo}, Pasquale and Fiumara, Giacomo and Baumgartner, Robert},
doi = {10.1016/j.knosys.2014.07.007},
eprint = {1207.0246},
file = {:S$\backslash$:/SciencePhdSorted/Knowledge-Based Systems/Web data extraction, applications and techniques A survey{\_}Ferrara et al.{\_}2014(2).pdf:pdf},
isbn = {978-1-4244-5678-9},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Business intelligence,Information retrieval,Knowledge engineering,Knowledge-based systems,Web data mining,Web information extraction},
pages = {301--323},
pmid = {18244404},
publisher = {Elsevier B.V.},
title = {{Web data extraction, applications and techniques: A survey}},
url = {http://dx.doi.org/10.1016/j.knosys.2014.07.007},
volume = {70},
year = {2014}
}
@article{Lee2012,
abstract = {As the information on the Internet dramatically increases, more and more limitations in information searching are revealed, because web pages are designed for human use by mixing content with presentation. In order to overcome these limitations, the Semantic Web, based on ontology, was introduced by W3C to bring about significant advancement in web searching. To accomplish this, the Semantic Web must provide search methods based on the different relationships between resources. In this paper, we propose a semantic association search methodology that consists of the evaluation of resources and relationships between resources, as well as the identification of relevant information based on ontology, a semantic network of resources and properties. The proposed semantic search method is based on an extended spreading activation technique. In order to evaluate the importance of a query result, we propose weighting methods for measuring properties and resources based on their specificity and generality. From this work, users can search semantically associated resources for their query, confident that the information is valuable and important. The experimental results show that our method is valid and efficient for searching and ranking semantic search results. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Lee, Myungjin and Kim, Wooju and Park, Sangun},
doi = {10.1016/j.eswa.2011.09.127},
file = {:S$\backslash$:/SciencePhdSorted/Expert Systems with Applications/Searching and ranking method of relevant resources by user intention on the Semantic Web{\_}Lee, Kim, Park{\_}2012.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Ontology retrieval,Semantic Web,Semantic associations,Semantic search,Spreading activation},
number = {4},
pages = {4111--4121},
publisher = {Elsevier Ltd},
title = {{Searching and ranking method of relevant resources by user intention on the Semantic Web}},
url = {http://dx.doi.org/10.1016/j.eswa.2011.09.127},
volume = {39},
year = {2012}
}
@article{Sud2014,
abstract = {Many webometric studies have used hyperlinks to investigate links to or$\backslash$nbetween specific collections of websites to estimate their impact or$\backslash$nidentify connectivity patterns. Whilst major commercial search engines$\backslash$nhave previously been used to identify hyperlinks for these purposes,$\backslash$ntheir hyperlink search facilities have now been shut down. In response,$\backslash$na range of alternative sources of link data have been suggested, but all$\backslash$nhave limitations. This article introduces a new type of link that can be$\backslash$nidentified from commercial search engines, linked title mentions. These$\backslash$ncan be found by querying title mentions in a search engine and then$\backslash$nremoving those not associated with a relevant hyperlink. Results of a$\backslash$nproof of concept test on 51 U.S. library and information science schools$\backslash$nand four other sets of schools suggest that linked title mentions may$\backslash$ntend to give better results than title mentions in some cases when used$\backslash$nfor site inlinks but may not always be an improvement on URL citations.$\backslash$nFor links between or co-inlinks to specified pairs of academic websites,$\backslash$nlinked title mentions do not generally provide an improvement over title$\backslash$nmentions, but they do over URL citations in some cases. Linked title$\backslash$nmentions may also be useful for sets of non-academic websites when the$\backslash$nalternatives give too few or misleading results.},
author = {Sud, Pardeep and Thelwall, Mike},
doi = {10.1007/s11192-014-1374-8},
file = {:S$\backslash$:/SciencePhdSorted/Scientometrics/Linked title mentions a new automated link search candidate{\_}Sud, Thelwall{\_}2014.pdf:pdf},
issn = {15882861},
journal = {Scientometrics},
keywords = {Link analysis,Web impact,Web network,Webometrics},
number = {3},
pages = {1831--1849},
title = {{Linked title mentions: a new automated link search candidate}},
volume = {101},
year = {2014}
}
@article{Sheng2018,
author = {Sheng, Jie and Amankwah-amoah, Joseph and Wang, Xiaojun},
doi = {10.1016/j.techfore.2018.06.009},
file = {:S$\backslash$:/SciencePhdSorted/Unknown/Technological Forecasting {\&} Social Change Technology in the 21st century New challenges and opportunities{\_}Sheng, Amankwah-amoah, Wang{\_}2.pdf:pdf},
keywords = {business intelligence},
number = {September 2017},
title = {{Technological Forecasting {\&} Social Change Technology in the 21st century : New challenges and opportunities}},
year = {2018}
}
@article{Ahamed2016,
author = {Ahamed, B Bazeer and Ramkumar, T},
doi = {10.1016/j.compeleceng.2016.09.033},
file = {:S$\backslash$:/SciencePhdSorted/Computers and Electrical Engineering/An intelligent web search framework for performing efficient retrieval of data{\_}Ahamed, Ramkumar{\_}2016.pdf:pdf},
issn = {0045-7906},
journal = {Computers and Electrical Engineering},
keywords = {Web content,Search engines,Web data,Multiple data},
pages = {289--299},
publisher = {Elsevier Ltd},
title = {{An intelligent web search framework for performing efficient retrieval of data}},
url = {http://dx.doi.org/10.1016/j.compeleceng.2016.09.033},
volume = {56},
year = {2016}
}
@article{Thada2013,
abstract = {A similarity coefficient represents the similarity between two documents, two queries, or one document and one query. The retrieved documents can also be ranked in the order of presumed importance. A similarity coefficient is a function which computes the degree of similarity between a pair of text objects. There are a large number of similarity coefficients proposed in the literature, because the best similarity measure doesn't exist (yet !). In this paper we do a comparative analysis for finding out the most relevant document for the given set of keyword by using three similarity coefficients viz Jaccard, Dice and Cosine coefficients. This we perform using genetic algorithm approach. Due to the randomized nature of genetic algorithm the best fitness value is the average of 10 runs of the same code for a fixed number of iterations.The similarity coefficient for a set of documents retrieved for a given query from Google are find out then average relevancy in terms of fitness values using similarity coefficients is calculated. In this paper we have averaged 10 different generations for each query by running the program 10 times for the fixed value of Probability of Crossover Pc=0.7 and Probability of Mutation Pm=0.01. The same experiment was conducted for 10 queries.},
author = {Thada, Vikas and Jaglan, Vivek},
file = {:S$\backslash$:/SciencePhdSorted/International Journal of Innovations in Engineering and Technology/Comparison of Jaccard, Dice, Cosine Similarity Coefficient To Find Best Fitness Value for Web Retrieved Documents Using Genetic Algorith.pdf:pdf},
journal = {International Journal of Innovations in Engineering and Technology},
keywords = {algorithm,coefficient,cosine,dice,genetic,jaccard,relevancy},
number = {4},
pages = {202--205},
title = {{Comparison of Jaccard, Dice, Cosine Similarity Coefficient To Find Best Fitness Value for Web Retrieved Documents Using Genetic Algorithm}},
url = {http://www.dknmu.org/uploads/file/6842.pdf},
volume = {2},
year = {2013}
}
@article{Li2018,
abstract = {To compete in the fierce market, a key issue for shipbuilding companies is to effectively integrate outer resources with the inner business. However, multi-agent systems (MAS) developed so far have proven to be inapt to support the whole process of making a coordinated supply plan. Meanwhile, huge amount of data emerging from various kinds of enterprise infrastructures have not been semantically consolidated for smooth and efficient workflows. To overcome these barriers, this paper proposes a hybrid semantic-augmented MAS (OMAS), consisting of a Service Management Agent, a Configuration Agent, a Member Management Agent, a Process-Driven Module and a Semantic-Enhanced Module. These components collaborate to integrate heterogeneous data devoid of semantic ambiguities, thereby improving workflows' coherence and efficiency. A scalable ontology grid is initialized as the operational foundation after analyzing the related works. In addition, OMAS innovatively adds semantic assistance into the supplier selection process wherein a two-phase negotiation algorithm is suggested. Finally, an illustrative example from a marine company is conducted to demonstrate the benefits of OMAS, through which the effectiveness of the proposed algorithm is proved to be much better than that of a contemporary contribution.},
author = {Li, Jinghua and Sun, Miaomiao and Han, Duanfeng and Wu, Xiaoyuan and Yang, Boxin and Mao, Xuezhang and Zhou, Qinghua},
doi = {10.1016/j.compind.2018.01.001},
file = {:S$\backslash$:/SciencePhdSorted/Computers in Industry/Semantic multi-agent system to assist business integration An application on supplier selection for shipbuilding yards{\_}Li et al.{\_}2018.pdf:pdf},
issn = {01663615},
journal = {Computers in Industry},
keywords = {Domain ontology,Multi-agent system,Service-oriented architecture,Shipbuilding industry,Supplier selection},
pages = {10--26},
publisher = {Elsevier B.V.},
title = {{Semantic multi-agent system to assist business integration: An application on supplier selection for shipbuilding yards}},
url = {http://dx.doi.org/10.1016/j.compind.2018.01.001},
volume = {96},
year = {2018}
}
@article{Li2016a,
abstract = {In the era of big data, facing vast amounts of network data, only identifying the reliable data source can the researchers extract the original data that can be used in scientific research. Building reliable network data mining model based on the improvement of PageRank algorithm with applying each improved algorithm. Then the model is divided into three modules: the first, use PageRank and TrustRank to eliminate cheating webpages; then, refine webpages which related to research topic highly by TC-PageRank which combined with the topic relevancy between webpages and weight of time difference; finally, determine the authoritative webpages of the original data source by the improved HITS which considered the influence of the similarity between webpage and research topic and the amplification of webpage links to the authoritative webpages. Meanwhile, the partitioning of matrix operation based on MapReduce reduces the time and space complexity of the algorithms. And the feasibility and accuracy of the method are verified by comparative analysis of the algorithms. {\&}copy; 2016 The Natural Computing Applications Forum},
author = {Li, Jinhai and He, Youshi and Ma, Yunlei},
doi = {10.1007/s00521-016-2349-x},
file = {:S$\backslash$:/SciencePhdSorted/Neural Computing and Applications/Research of network data mining based on reliability source under big data environment{\_}Li, He, Ma{\_}2016.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Big data,HITS,MapReduce,PageRank,TC-PageRank,The original data},
pages = {1--9},
publisher = {Springer London},
title = {{Research of network data mining based on reliability source under big data environment}},
year = {2016}
}
@article{Fensel2014,
abstract = {Over the last few years, we have encountered an exponential growth in online communication opportunities. Organizations have more and more ways to connect and engage with their current or future customers. The existence of more opportunities in connecting to people can be both an enabler and a burden. Being present at a multitude of different channels requires the effective management of a very large number of adapted contents, formats, and interaction patterns fulfilling the communication and cooperation needs of distributed target groups. In this respect, we integrate existing fragmented communication and monitoring approaches into a full-fledged communication model as a basis for an adequate engagement approach. We describe applications of our approach in both the eTourism and manufacturing domain. In this paper, we introduce an approach that will enable communication, collaboration and value exchange of users through a multitude of online interaction possibilities based on the use of semantic technology. Finally, we also compare our approach with existing solutions with respect to the identified challenges in this subject. {\textcopyright} 2014 Elsevier B.V.},
author = {Fensel, Anna and Toma, Ioan and Garc{\'{i}}a, Jos{\'{e}} Mar{\'{i}}a and Stavrakantonakis, Ioannis and Fensel, Dieter},
doi = {10.1016/j.compind.2014.02.001},
file = {:S$\backslash$:/SciencePhdSorted/Computers in Industry/Enabling customers engagement and collaboration for small and medium-sized enterprises in ubiquitous multi-channel ecosystems{\_}Fensel et.pdf:pdf},
isbn = {0166-3615},
issn = {01663615},
journal = {Computers in Industry},
keywords = {Collaboration,Communication,Engagement,Multi-channel ecosystems,Semantic technology,Small and medium enterprises},
number = {5},
pages = {891--904},
title = {{Enabling customers engagement and collaboration for small and medium-sized enterprises in ubiquitous multi-channel ecosystems}},
volume = {65},
year = {2014}
}
@article{Corsini2006,
abstract = {Determining profiles of web portal typical users can be extremely useful, for instance, to personalize the web portal, to provide customized guide and to send tailored advertisements. In this work, we present a system to produce a small number of user profiles from the web access log and to associate each user with one of these profiles. The system is based on a version of the fuzzy C-means (FCM) algorithm which uses the cosine distance rather than the classical Euclidean distance. After filtering the access log, for instance, by removing occasional and undecided users, the FCM algorithm clusters the users into groups characterized by a set of common interests and represented by a prototype, which defines the profile of the group typical member. To attest the validity of these profiles, we extract a set of association rules from the raw access log data by applying the well-known A-priori algorithm and show how the profiles are a concise representation of the association rules. Finally, to test the effectiveness of the overall fuzzy system, we illustrate how the profiles determined by the FCM algorithm from access log data collected along a period of 30 days allow classifying approximately 93{\%} of the users defined by access log data collected during subsequent 30 days. [ABSTRACT FROM AUTHOR]},
author = {Corsini, Paolo and Marcelloni, Francesco},
file = {:S$\backslash$:/SciencePhdSorted/Journal of Intelligent {\&} Fuzzy Systems/A fuzzy system for profiling web portal users from web access log.{\_}Corsini, Marcelloni{\_}2006.pdf:pdf},
issn = {10641246},
journal = {Journal of Intelligent {\&} Fuzzy Systems},
keywords = {ALGORITHMS,FUZZY logic,FUZZY systems,INTERNET users,WEB portals,Web mining,association rules,fuzzy c-means,user profile},
number = {5},
pages = {503--516},
title = {{A fuzzy system for profiling web portal users from web access log.}},
volume = {17},
year = {2006}
}
@article{Kenekayoro2014,
abstract = {Counts of hyperlinks between websites can be unreliable for webometrics$\backslash$nstudies so researchers have attempted to find alternate counting methods$\backslash$nor have tried to identify the reasons why links in websites are created.$\backslash$nManual classification of individual links in websites is infeasible for$\backslash$nlarge webometrics studies, so a more efficient approach to identifying$\backslash$nthe reasons for link creation is needed to fully harness the potential$\backslash$nof hyperlinks for webometrics research. This paper describes a machine$\backslash$nlearning method to automatically classify hyperlink source and target$\backslash$npage types in university websites. 78 {\%} accuracy was achieved for$\backslash$nautomatically classifying web page types and up to 74 {\%} accuracy for$\backslash$npredicting link target page types from link source page characteristics.},
author = {Kenekayoro, Patrick and Buckley, Kevan and Thelwall, Mike},
doi = {10.1007/s11192-014-1292-9},
file = {:S$\backslash$:/SciencePhdSorted/Scientometrics/Automatic classification of academic web page types{\_}Kenekayoro, Buckley, Thelwall{\_}2014.pdf:pdf},
isbn = {01389130 (ISSN)},
issn = {01389130},
journal = {Scientometrics},
keywords = {Decision tree induction,Link classification,Supervised learning,Support vector machines,Webometrics},
number = {2},
pages = {1015--1026},
title = {{Automatic classification of academic web page types}},
volume = {101},
year = {2014}
}
@article{Brin1998,
abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from 3 years ago. This paper provides an in-depth description of our large-scale web search engine - the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections, where anyone can publish anything they want. ?? 2012 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Brin, S and Page, L},
doi = {10.1.1.109.4049},
eprint = {1111.6189v1},
file = {:S$\backslash$:/SciencePhdSorted/Computer Networks and ISDN Systems/The anatomy of a large scale hypertextual Web search engine{\_}Brin, Page{\_}1998.pdf:pdf},
isbn = {0169-7552},
issn = {01697552},
journal = {Computer Networks and ISDN Systems},
keywords = {information retrieval,pagerank,search engines,world wide web},
number = {1/7},
pages = {107--17},
pmid = {726238533241861127},
title = {{The anatomy of a large scale hypertextual Web search engine}},
volume = {30},
year = {1998}
}
@article{Dobre2014,
abstract = {With Cloud Computing emerging as a promising new approach for ad-hoc parallel data processing, major companies have started to integrate frameworks for parallel data processing in their product portfolio, making it easy for customers to access these services and to deploy their programs. We have entered the Era of Big Data. The explosion and profusion of available data in a wide range of application domains rise up new challenges and opportunities in a plethora of disciplines—ranging from science and engineering to biology and business. One major challenge is how to take advantage of the unprecedented scale of data—typically of heterogeneous nature—in order to acquire further insights and knowledge for improving the quality of the offered services. To exploit this new resource, we need to scale up and scale out both our infrastructures and standard techniques. Our society is already data-rich, but the question remains whether or not we have the conceptual tools to handle it. In this paper we discuss and analyze opportunities and challenges for efficient parallel data processing. Big Data is the next frontier for innovation, competition, and productivity, and many solutions continue to appear, partly supported by the considerable enthusiasm around the MapReduce paradigm for large-scale data analysis. We review various parallel and distributed programming paradigms, analyzing how they fit into the Big Data era, and present modern emerging paradigms and frameworks. To better support practitioners interesting in this domain, we end with an analysis of on-going research challenges towards the truly fourth generation data-intensive science.},
author = {Dobre, Ciprian and Xhafa, Fatos},
doi = {10.1007/s10766-013-0272-7},
file = {:S$\backslash$:/SciencePhdSorted/International Journal of Parallel Programming/Parallel programming paradigms and frameworks in Big Data Era{\_}Dobre, Xhafa{\_}2014.pdf:pdf},
isbn = {08857458},
issn = {08857458},
journal = {International Journal of Parallel Programming},
keywords = {Big Data,Challenges,MapReduce,Parallel programming,Programming models},
number = {5},
pages = {710--738},
pmid = {1531888121},
title = {{Parallel programming paradigms and frameworks in Big Data Era}},
volume = {42},
year = {2014}
}
@article{Toma??ev2017,
author = {Toma??ev, Nenad},
doi = {10.1007/s10796-015-9596-8},
file = {:S$\backslash$:/SciencePhdSorted/Information Systems Frontiers/Extracting the patterns of truthfulness from political information systems in Serbia{\_}Tomaev{\_}2017.pdf:pdf},
issn = {15729419},
journal = {Information Systems Frontiers},
keywords = {Association rules,Data mining,Information systems,Politics,Text mining,Truthfulness},
number = {1},
pages = {109--127},
title = {{Extracting the patterns of truthfulness from political information systems in Serbia}},
volume = {19},
year = {2017}
}
@article{Ma2011,
abstract = {Identifying competitors is important for businesses. We present an approach that uses graph-theoretic measures and machine learning techniques to infer competitor relationships on the basis of structure of an intercompany network derived from company citations (cooccurrence) in online news articles. We also estimate to what extent our approach complements the commercial company profile data sources, such as Hoover's and Mergent. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Ma, Zhongming and Pant, Gautam and Sheng, Olivia R L},
doi = {10.1016/j.elerap.2010.11.006},
file = {:S$\backslash$:/SciencePhdSorted/Electronic Commerce Research and Applications/Mining competitor relationships from online news A network-based approach{\_}Ma, Pant, Sheng{\_}2011.pdf:pdf},
isbn = {9781634823074},
issn = {15674223},
journal = {Electronic Commerce Research and Applications},
keywords = {Business news,Classification in networked data,Competitor discovery,Web mining},
number = {4},
pages = {418--427},
publisher = {Elsevier B.V.},
title = {{Mining competitor relationships from online news: A network-based approach}},
url = {http://dx.doi.org/10.1016/j.elerap.2010.11.006},
volume = {10},
year = {2011}
}
@article{Blum1998,
abstract = {We consider the problem of using a large unla- beled sample to boost performance of a learn- ing algorit,hrn when only a small set of labeled examples is available. In particular, we con- sider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the de- scription of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled ex- amples. Specifically, the presence of two dis- tinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algo- rithm's predictions on new unlabeled exam- ples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and un- labeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Blum, Avrim and Mitchell, Tom},
doi = {10.1145/279943.279962},
eprint = {arXiv:1011.1669v3},
file = {:S$\backslash$:/SciencePhdSorted/Proceedings of the eleventh annual conference on Computational learning theory - COLT' 98/Combining labeled and unlabeled data with co-training{\_}Blum, Mitchell{\_}1998.pdf:pdf},
isbn = {1581130570},
issn = {1098-6596},
journal = {Proceedings of the eleventh annual conference on Computational learning theory  - COLT' 98},
pages = {92--100},
pmid = {25246403},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://portal.acm.org/citation.cfm?doid=279943.279962},
year = {1998}
}
